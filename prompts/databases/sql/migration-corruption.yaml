id: "db-sql-migration-corruption-001"
version: "2.0.0"
category: "databases"
subcategory: "sql"

# =============================================================================
# LLM GENERATION FRAMEWORK
# =============================================================================
# This configuration enables generation of 10,000+ unique, complex tasks
# related to database migration issues, data corruption, and schema changes.
# =============================================================================

generation_framework:
  # Difficulty parameters - SIGNIFICANTLY increased for database architect-level challenges
  time_range: [5400, 14400]  # 90-240 minutes for database architects with internals expertise
  command_steps: [80, 200]   # Requires extensive multi-layer investigation and debugging
  
  generation_targets:
    minimum_difficulty: "90+ minutes, requires deep database kernel and DDL internals expertise"
    target_audience: "Principal Database Architects and database migration engineers with 10+ years experience"
    complexity_level: "Nightmare - requires synthesizing knowledge across DDL mechanics, storage engine internals, and replication protocols"
  
  multi_agent_orchestration:
    description: "Coordinate 5-8 specialized database agents for complex migration debugging"
    required_agents:
      - name: "ddl_lock_analyzer"
        role: "Analyze DDL locking behavior, metadata locks, and schema modification conflicts"
        expertise: ["ACCESS EXCLUSIVE locks", "metadata locks", "online DDL algorithms", "lock escalation"]
      - name: "data_type_conversion_expert"
        role: "Debug data type conversions, precision loss, and implicit coercion issues"
        expertise: ["numeric precision", "character encoding", "temporal conversions", "binary transformations"]
      - name: "constraint_validator"
        role: "Analyze constraint validation timing, deferred checks, and FK cascades"
        expertise: ["FK validation", "check constraints", "unique enforcement", "deferred constraints"]
      - name: "online_ddl_debugger"
        role: "Debug online schema change tools and algorithms"
        expertise: ["gh-ost", "pt-online-schema-change", "pg_repack", "DBMS_REDEFINITION"]
      - name: "replication_migration_coordinator"
        role: "Coordinate migrations with replication lag and consistency requirements"
        expertise: ["logical replication", "CDC integration", "binlog parsing", "WAL decoding"]
      - name: "storage_engine_migration_expert"
        role: "Debug storage-level operations during table rebuilds and reorganization"
        expertise: ["table rebuild", "index reorganization", "tablespace operations", "data file management"]
      - name: "rollback_recovery_specialist"
        role: "Design and test rollback strategies and point-in-time recovery"
        expertise: ["backup strategies", "PITR", "undo management", "transaction log recovery"]
      - name: "cross_database_migration_analyst"
        role: "Handle cross-database migrations and compatibility issues"
        expertise: ["schema translation", "data type mapping", "feature parity", "migration tools"]
    
    cross_engine_attack_chains:
      - chain: "ddl_lock_acquisition -> replication_lag -> application_timeout -> connection_exhaustion -> cascade_failure"
        description: "DDL lock causes replication lag that cascades into application-level failures"
      - chain: "type_conversion -> precision_loss -> constraint_violation -> rollback_failure -> data_corruption"
        description: "Type conversion loses precision causing constraint failures with unrecoverable state"
      - chain: "online_ddl -> trigger_conflict -> data_divergence -> replication_break -> split_brain"
        description: "Online DDL conflicts with triggers causing data divergence across replicas"
      - chain: "fk_validation -> table_lock -> deadlock -> migration_abort -> partial_state"
        description: "FK validation causes table locks leading to deadlocks and partial migration state"
    
    parallel_analysis_requirements:
      - "Concurrent DDL impact analysis across all replica sets"
      - "Parallel data validation between source and target schemas"
      - "Simultaneous lock monitoring across all database sessions"
      - "Coordinated rollback testing with point-in-time recovery"
    
    agent_handoff_scenarios:
      - from: "ddl_lock_analyzer"
        to: "replication_migration_coordinator"
        trigger: "DDL locks impacting replication consistency"
      - from: "data_type_conversion_expert"
        to: "constraint_validator"
        trigger: "Type conversion causing constraint validation failures"
      - from: "online_ddl_debugger"
        to: "storage_engine_migration_expert"
        trigger: "Online DDL revealing storage-level issues"
  
  multi_conversation_workflow:
    phase_1_research: |
      Research obscure database migration behaviors and failure modes:
      - Database-specific DDL locking mechanisms
      - Online vs offline schema change techniques
      - Data type conversion edge cases
      - Constraint validation timing
      - Rollback and recovery limitations
    
    phase_2_creation: |
      Create task with hidden complexity and traps including:
      - Subtle data precision loss during conversion
      - Implicit type coercion issues
      - Constraint violation cascades
      - Backup/restore timing windows
      - Concurrent DML during migration
    
    phase_3_amplification: |
      Add difficulty multipliers and edge cases:
      - Large table scenarios (billions of rows)
      - Zero-downtime requirements
      - Cross-database migrations
      - Regulatory compliance constraints
      - Data validation requirements
    
    phase_4_verification: |
      Validate task requires deep database expertise:
      - Must understand DDL internals at storage engine level
      - Must know database-specific locking, logging, and recovery behaviors
      - Cannot be solved by simple migration tools or cookbook approaches
      - Requires careful planning, testing, and multi-layer validation
      - Tests understanding of data integrity across schema evolution
      - Has at least 10+ deeply interacting traps across DDL/locking/storage/replication layers
      - Has cascading failure modes that span metadata, storage, replication, and application layers
      - Requires knowledge of database kernel internals, online DDL algorithms, and recovery protocols
      - Would take experienced database architects 90+ minutes
      - Involves cross-database migrations with complex data type mappings
  
  quality_requirements:
    - "Must require understanding of database DDL mechanics at implementation level"
    - "Must have at least 8 non-obvious failure modes with complex interactions"
    - "Must not be solvable by simple migration tool usage or standard procedures"
    - "Must require careful data validation with precision and encoding verification"
    - "Must involve database-specific quirks, version differences, and edge cases"
    - "90+ minutes for experienced database architects, 180+ for senior engineers"
    - "Requires synthesizing DDL internals, data integrity, recovery protocols, and replication coordination"
    - "Must involve zero-downtime requirements with production traffic"
    - "Requires understanding of online schema change algorithms and their limitations"

# =============================================================================
# TOPIC UNIVERSE - 150+ Migration-Related Topics
# =============================================================================

topic_universe:
  # ---------------------------------------------------------------------------
  # DDL Operations and Locking
  # ---------------------------------------------------------------------------
  ddl_locking:
    alter_table_operations:
      add_column:
        behavior_by_db:
          postgresql:
            no_default: "Instant, no rewrite"
            with_default: "Requires table rewrite (pre-11) or instant (11+)"
            with_not_null: "Requires rewrite to validate"
          mysql_innodb:
            instant: "8.0.12+ for some operations"
            inplace: "ALGORITHM=INPLACE for many"
            copy: "Full table copy as fallback"
          sql_server:
            online: "Enterprise edition feature"
            offline: "Full table lock"
          oracle:
            instant: "12c+ for nullable columns"
        lock_types:
          - "ACCESS EXCLUSIVE (PostgreSQL)"
          - "METADATA LOCK (MySQL)"
          - "Sch-M (SQL Server)"
          - "DDL Lock (Oracle)"
      
      drop_column:
        behavior_by_db:
          postgresql: "Marks column as dropped, no physical removal"
          mysql: "Requires table rebuild"
          sql_server: "Online with restrictions"
          oracle: "SET UNUSED for fast, DROP UNUSED COLUMNS later"
        traps:
          - "Dependent objects (views, triggers)"
          - "Stored procedures referencing column"
          - "Foreign key constraints"
      
      modify_column:
        type_changes:
          safe: "VARCHAR(50) -> VARCHAR(100)"
          risky: "VARCHAR -> INT"
          dangerous: "DECIMAL(10,2) -> DECIMAL(8,2)"
        behavior_by_db:
          postgresql: "Often requires rewrite"
          mysql: "Depends on change type"
          sql_server: "May require rebuild"
          oracle: "Restrictions on populated columns"
      
      add_constraint:
        types:
          primary_key: "Requires unique data, creates index"
          foreign_key: "Validates all existing data"
          unique: "Scans for duplicates"
          check: "Validates all rows"
          not_null: "Validates no NULLs exist"
        validation_modes:
          postgresql: "NOT VALID option defers validation"
          mysql: "No deferral option"
          sql_server: "WITH NOCHECK option"
          oracle: "ENABLE NOVALIDATE option"
    
    online_ddl:
      postgresql:
        create_index_concurrently:
          description: "Build index without blocking writes"
          limitations:
            - "Takes longer than regular CREATE INDEX"
            - "Requires two table scans"
            - "Cannot be run in transaction"
            - "May fail if concurrent updates conflict"
        
      mysql_online_ddl:
        algorithms:
          instant: "Metadata only change"
          inplace: "Modifies in place"
          copy: "Full table copy"
        lock_options:
          none: "Allow DML during operation"
          shared: "Allow reads, block writes"
          exclusive: "Block all access"
        
      sql_server_online:
        requirements:
          - "Enterprise edition"
          - "Sufficient tempdb space"
          - "No LOB columns for some operations"
        
      oracle_online:
        operations:
          - "ONLINE index rebuild"
          - "ONLINE table redefinition"
          - "DBMS_REDEFINITION package"

  # ---------------------------------------------------------------------------
  # Data Type Conversions
  # ---------------------------------------------------------------------------
  data_type_conversions:
    numeric_conversions:
      integer_types:
        tinyint: "1 byte, 0-255 or -128 to 127"
        smallint: "2 bytes"
        int: "4 bytes"
        bigint: "8 bytes"
        overflow_risks:
          - "INT to SMALLINT truncation"
          - "BIGINT to INT overflow"
          - "Unsigned to signed issues"
      
      decimal_types:
        precision_loss:
          - "DECIMAL(10,4) to DECIMAL(10,2) rounds"
          - "FLOAT to DECIMAL precision issues"
          - "DECIMAL to FLOAT representation errors"
        database_differences:
          postgresql: "NUMERIC exact arithmetic"
          mysql: "DECIMAL stored as string internally"
          sql_server: "DECIMAL and NUMERIC synonymous"
          oracle: "NUMBER with variable precision"
      
      money_types:
        traps:
          - "MONEY type has fixed precision"
          - "Currency conversion during migration"
          - "Floating point comparison issues"
    
    string_conversions:
      length_changes:
        truncation_risk: "VARCHAR(100) to VARCHAR(50)"
        charset_impact: "UTF-8 uses 1-4 bytes per char"
        collation_changes: "May affect sorting/comparison"
      
      char_vs_varchar:
        char_padding: "Fixed-length pads with spaces"
        comparison_issues: "Trailing space handling varies"
      
      text_types:
        postgresql: "TEXT has no length limit"
        mysql: "TEXT/MEDIUMTEXT/LONGTEXT different sizes"
        sql_server: "VARCHAR(MAX) vs TEXT (deprecated)"
    
    temporal_conversions:
      date_time_types:
        precision_changes:
          - "DATETIME to DATE loses time"
          - "TIMESTAMP(6) to TIMESTAMP(3) loses microseconds"
        timezone_handling:
          postgresql: "TIMESTAMP WITH/WITHOUT TIME ZONE"
          mysql: "TIMESTAMP auto-converts to UTC"
          sql_server: "DATETIMEOFFSET for timezone"
          oracle: "TIMESTAMP WITH TIME ZONE"
      
      date_format_issues:
        - "Two-digit year interpretation"
        - "Month/day order ambiguity"
        - "Leap second handling"
        - "Historical calendar changes"
    
    binary_conversions:
      blob_types:
        size_limits_by_db:
          postgresql: "BYTEA up to 1GB"
          mysql: "BLOB types vary: 64KB to 4GB"
          sql_server: "VARBINARY(MAX) up to 2GB"
          oracle: "BLOB up to 4GB"
      
      encoding_issues:
        - "Base64 encoding overhead"
        - "Binary vs hex representation"
        - "Compression during migration"

  # ---------------------------------------------------------------------------
  # Constraint Handling
  # ---------------------------------------------------------------------------
  constraint_handling:
    foreign_keys:
      validation_timing:
        immediate: "Check on each statement"
        deferred: "Check at transaction commit"
        disabled: "No validation (dangerous)"
      
      cascade_operations:
        on_delete: ["CASCADE", "SET NULL", "SET DEFAULT", "RESTRICT", "NO ACTION"]
        on_update: ["CASCADE", "SET NULL", "SET DEFAULT", "RESTRICT", "NO ACTION"]
      
      migration_challenges:
        - "Circular FK references"
        - "Self-referencing tables"
        - "Cross-schema references"
        - "Orphaned records during migration"
    
    check_constraints:
      validation_approaches:
        full_scan: "Validate all existing rows"
        partial: "Only validate new/modified rows"
        skip: "Trust data, validate later"
      
      expression_limitations:
        postgresql: "Full SQL expressions allowed"
        mysql: "Limited until 8.0.16"
        sql_server: "No subqueries"
        oracle: "No subqueries, some function limits"
    
    unique_constraints:
      duplicate_handling:
        - "Find and resolve duplicates first"
        - "Create unique index CONCURRENTLY"
        - "Add constraint with NOVALIDATE"
      
      null_handling:
        postgresql: "Multiple NULLs allowed"
        mysql: "Multiple NULLs allowed"
        sql_server: "Filtered index needed for multiple NULLs"
        oracle: "NULLs not indexed, multiple allowed"

  # ---------------------------------------------------------------------------
  # Large Table Migrations
  # ---------------------------------------------------------------------------
  large_table_strategies:
    batch_processing:
      description: "Process data in chunks"
      techniques:
        - "Primary key ranges"
        - "Modulo-based batching"
        - "Cursor-based iteration"
      considerations:
        - "Batch size tuning"
        - "Progress tracking"
        - "Error handling per batch"
        - "Gap handling in sequences"
    
    parallel_migration:
      description: "Multiple processes working simultaneously"
      approaches:
        - "Range-based partitioning"
        - "Hash-based work distribution"
        - "Queue-based processing"
      challenges:
        - "Resource contention"
        - "Progress coordination"
        - "Error recovery"
    
    online_schema_change_tools:
      gh_ost:
        description: "GitHub online schema change for MySQL"
        mechanism: "Binary log parsing, shadow table"
        limitations:
          - "Foreign key constraints"
          - "Triggers"
          - "Unique key requirements"
      
      pt_online_schema_change:
        description: "Percona toolkit for MySQL"
        mechanism: "Triggers, shadow table"
        limitations:
          - "Trigger overhead"
          - "Chunk size tuning"
      
      pg_repack:
        description: "PostgreSQL table reorganization"
        mechanism: "Creates new table, swaps"
        limitations:
          - "Needs temporary space"
          - "Brief exclusive lock at end"
      
      lhm:
        description: "Large Hadron Migrator (Ruby)"
        mechanism: "Chunked migration"

  # ---------------------------------------------------------------------------
  # Rollback and Recovery
  # ---------------------------------------------------------------------------
  rollback_strategies:
    reversible_migrations:
      techniques:
        - "Store old column before drop"
        - "Create backup table"
        - "Use database point-in-time recovery"
      limitations:
        - "Space requirements"
        - "Data changes during rollback window"
    
    irreversible_operations:
      examples:
        - "DROP COLUMN (data loss)"
        - "TRUNCATE TABLE (data loss)"
        - "Data type narrowing (precision loss)"
      mitigation:
        - "Backup before operation"
        - "Two-phase deployment"
        - "Blue-green deployments"
    
    point_in_time_recovery:
      postgresql: "pg_basebackup + WAL replay"
      mysql: "mysqlbinlog for point-in-time"
      sql_server: "Backup/restore to point"
      oracle: "Flashback or RMAN"

  # ---------------------------------------------------------------------------
  # Cross-Database Migrations
  # ---------------------------------------------------------------------------
  cross_database_migration:
    schema_translation:
      data_types:
        - "PostgreSQL SERIAL -> MySQL AUTO_INCREMENT"
        - "SQL Server IDENTITY -> PostgreSQL SERIAL"
        - "Oracle NUMBER -> PostgreSQL NUMERIC"
      
      syntax_differences:
        - "LIMIT vs TOP vs FETCH FIRST"
        - "String concatenation operators"
        - "NULL handling differences"
        - "Boolean representation"
    
    feature_mapping:
      sequences: "Not all databases have standalone sequences"
      materialized_views: "Syntax varies significantly"
      partitioning: "Different partitioning strategies"
      stored_procedures: "Language differences (PL/pgSQL, T-SQL, PL/SQL)"
    
    tools:
      - "pgloader (PostgreSQL)"
      - "ora2pg (Oracle to PostgreSQL)"
      - "AWS DMS (Database Migration Service)"
      - "Azure Database Migration Service"

  # ---------------------------------------------------------------------------
  # Data Validation
  # ---------------------------------------------------------------------------
  data_validation:
    pre_migration:
      checks:
        - "Row counts"
        - "Checksum/hash of data"
        - "Constraint validity"
        - "Data distribution statistics"
    
    during_migration:
      monitoring:
        - "Progress tracking"
        - "Error logging"
        - "Performance metrics"
    
    post_migration:
      verification:
        - "Row count comparison"
        - "Sample data verification"
        - "Constraint validation"
        - "Application testing"
      reconciliation:
        - "Identify discrepancies"
        - "Determine root cause"
        - "Remediate issues"

  # ---------------------------------------------------------------------------
  # Zero-Downtime Migrations
  # ---------------------------------------------------------------------------
  zero_downtime:
    expand_contract_pattern:
      expand_phase:
        - "Add new column"
        - "Backfill data"
        - "Update application to write both"
      transition_phase:
        - "Switch reads to new column"
        - "Verify data consistency"
      contract_phase:
        - "Remove old column writes"
        - "Drop old column"
    
    dual_write_pattern:
      description: "Write to both old and new during transition"
      challenges:
        - "Maintaining consistency"
        - "Performance overhead"
        - "Error handling"
    
    feature_flags:
      usage:
        - "Control migration rollout"
        - "Enable quick rollback"
        - "A/B testing of changes"

# =============================================================================
# TRAP TYPES - 75+ Migration Traps
# =============================================================================

trap_types:
  # ---------------------------------------------------------------------------
  # Table Locking Traps
  # ---------------------------------------------------------------------------
  table_locking:
    - trap_id: "TL-001"
      name: "ALTER TABLE Full Lock"
      description: "ALTER TABLE locks entire table for duration"
      trigger: "Running ALTER on large table during business hours"
      difficulty: "medium"
      detection: "Queries timing out, connection pool exhaustion"
      databases: ["All"]
    
    - trap_id: "TL-002"
      name: "Metadata Lock Blocking"
      description: "Long-running query prevents DDL"
      trigger: "Open transaction blocking ALTER TABLE"
      difficulty: "medium"
      detection: "DDL waiting indefinitely"
      databases: ["MySQL", "PostgreSQL"]
    
    - trap_id: "TL-003"
      name: "CASCADE Lock Propagation"
      description: "FK CASCADE causes locks on related tables"
      trigger: "ALTER on parent table with many children"
      difficulty: "hard"
      detection: "Unexpected locks on child tables"
    
    - trap_id: "TL-004"
      name: "Index Rebuild Lock"
      description: "Index rebuild takes exclusive lock"
      trigger: "REINDEX or ALTER INDEX REBUILD without ONLINE"
      difficulty: "medium"
      detection: "Blocked queries during index operation"

  # ---------------------------------------------------------------------------
  # Data Precision Traps
  # ---------------------------------------------------------------------------
  precision_loss:
    - trap_id: "PL-001"
      name: "Decimal Truncation"
      description: "Reducing decimal precision loses data"
      trigger: "ALTER COLUMN DECIMAL(10,4) to DECIMAL(10,2)"
      difficulty: "medium"
      detection: "Post-migration data validation failure"
    
    - trap_id: "PL-002"
      name: "Float to Decimal Rounding"
      description: "Float representation causes rounding"
      trigger: "Converting FLOAT/DOUBLE to DECIMAL"
      difficulty: "hard"
      detection: "Subtle value differences"
    
    - trap_id: "PL-003"
      name: "Integer Overflow"
      description: "Value exceeds target type range"
      trigger: "BIGINT to INT conversion"
      difficulty: "easy"
      detection: "Conversion errors or silent truncation"
    
    - trap_id: "PL-004"
      name: "Timestamp Precision Loss"
      description: "Microseconds lost in conversion"
      trigger: "TIMESTAMP(6) to TIMESTAMP(3)"
      difficulty: "medium"
      detection: "Time-based queries returning different results"
    
    - trap_id: "PL-005"
      name: "Currency Rounding Issues"
      description: "Financial calculations affected by type change"
      trigger: "Changing money storage format"
      difficulty: "hard"
      detection: "Financial reconciliation failures"

  # ---------------------------------------------------------------------------
  # Foreign Key Traps
  # ---------------------------------------------------------------------------
  foreign_key_issues:
    - trap_id: "FK-001"
      name: "Orphaned Records"
      description: "Disabling FK creates orphaned child records"
      trigger: "SET FOREIGN_KEY_CHECKS=0 without cleanup"
      difficulty: "medium"
      detection: "Referential integrity violations"
    
    - trap_id: "FK-002"
      name: "Circular FK Deadlock"
      description: "Circular FKs cause migration deadlock"
      trigger: "Tables with mutual FK references"
      difficulty: "hard"
      detection: "Migration hangs or fails"
    
    - trap_id: "FK-003"
      name: "FK Validation Full Scan"
      description: "Adding FK requires full table scan"
      trigger: "ADD FOREIGN KEY on large table"
      difficulty: "medium"
      detection: "Long-running DDL"
    
    - trap_id: "FK-004"
      name: "Cross-Schema FK Issues"
      description: "FK across schemas causes permission issues"
      trigger: "ALTER TABLE with cross-schema FK"
      difficulty: "medium"
      detection: "Permission denied errors"

  # ---------------------------------------------------------------------------
  # Constraint Violation Traps
  # ---------------------------------------------------------------------------
  constraint_violations:
    - trap_id: "CV-001"
      name: "NOT NULL on Existing NULLs"
      description: "Adding NOT NULL when NULLs exist"
      trigger: "ALTER COLUMN SET NOT NULL"
      difficulty: "easy"
      detection: "Constraint violation error"
    
    - trap_id: "CV-002"
      name: "Unique on Duplicates"
      description: "Adding UNIQUE when duplicates exist"
      trigger: "ADD UNIQUE CONSTRAINT"
      difficulty: "easy"
      detection: "Duplicate key error"
    
    - trap_id: "CV-003"
      name: "Check Constraint Violation"
      description: "Existing data violates new CHECK"
      trigger: "ADD CHECK CONSTRAINT"
      difficulty: "medium"
      detection: "Check constraint failure"
    
    - trap_id: "CV-004"
      name: "Deferred Constraint Surprise"
      description: "Deferred constraint fails at commit"
      trigger: "Using DEFERRABLE constraints"
      difficulty: "hard"
      detection: "Failure at transaction commit"

  # ---------------------------------------------------------------------------
  # Character Set/Encoding Traps
  # ---------------------------------------------------------------------------
  encoding_issues:
    - trap_id: "EN-001"
      name: "UTF-8 Length Overflow"
      description: "Multi-byte chars exceed column length"
      trigger: "VARCHAR(100) with UTF-8 data"
      difficulty: "medium"
      detection: "String truncation errors"
    
    - trap_id: "EN-002"
      name: "Collation Change Data Loss"
      description: "Changing collation affects data ordering"
      trigger: "ALTER TABLE COLLATE"
      difficulty: "hard"
      detection: "Unexpected sort order"
    
    - trap_id: "EN-003"
      name: "Emoji Storage Failure"
      description: "UTF-8 column can't store 4-byte chars"
      trigger: "utf8 vs utf8mb4 in MySQL"
      difficulty: "medium"
      detection: "Data insertion failures"
      databases: ["MySQL"]

  # ---------------------------------------------------------------------------
  # Rollback Traps
  # ---------------------------------------------------------------------------
  rollback_issues:
    - trap_id: "RB-001"
      name: "Non-Transactional DDL"
      description: "DDL auto-commits, can't rollback"
      trigger: "ALTER TABLE in transaction"
      difficulty: "medium"
      detection: "Partial migration state"
      databases: ["MySQL", "Oracle"]
    
    - trap_id: "RB-002"
      name: "Destructive Operation No Backup"
      description: "DROP COLUMN without backup"
      trigger: "Dropping column with needed data"
      difficulty: "easy"
      detection: "Data loss discovered post-migration"
    
    - trap_id: "RB-003"
      name: "Backup Too Old"
      description: "Backup stale when rollback needed"
      trigger: "Long migration with outdated backup"
      difficulty: "medium"
      detection: "Data divergence during restore"

  # ---------------------------------------------------------------------------
  # Concurrent Access Traps
  # ---------------------------------------------------------------------------
  concurrent_access:
    - trap_id: "CA-001"
      name: "Data Modified During Migration"
      description: "Rows changed during backfill"
      trigger: "Long-running UPDATE for migration"
      difficulty: "hard"
      detection: "Inconsistent migrated data"
    
    - trap_id: "CA-002"
      name: "Sequence Gap During Migration"
      description: "Sequence values skipped during migration"
      trigger: "INSERT during sequence-based migration"
      difficulty: "medium"
      detection: "Missing or duplicate sequence values"
    
    - trap_id: "CA-003"
      name: "Trigger Interference"
      description: "Triggers fire during migration"
      trigger: "Existing triggers on migrated table"
      difficulty: "hard"
      detection: "Unexpected side effects"

  # ---------------------------------------------------------------------------
  # Tool/Version Traps
  # ---------------------------------------------------------------------------
  tooling_issues:
    - trap_id: "TO-001"
      name: "Migration Tool Version Mismatch"
      description: "Tool not compatible with database version"
      trigger: "Using outdated migration tool"
      difficulty: "easy"
      detection: "Syntax or feature errors"
    
    - trap_id: "TO-002"
      name: "ORM Migration Generator Bug"
      description: "ORM generates incorrect migration"
      trigger: "Complex schema change with ORM"
      difficulty: "medium"
      detection: "Runtime errors or data issues"
    
    - trap_id: "TO-003"
      name: "Migration Ordering Issue"
      description: "Migrations run out of order"
      trigger: "Parallel development with migrations"
      difficulty: "medium"
      detection: "Missing dependency errors"

  # ---------------------------------------------------------------------------
  # Resource Exhaustion Traps
  # ---------------------------------------------------------------------------
  resource_issues:
    - trap_id: "RE-001"
      name: "Disk Space Exhaustion"
      description: "Migration consumes all disk space"
      trigger: "Table rebuild without space check"
      difficulty: "medium"
      detection: "Out of space errors"
    
    - trap_id: "RE-002"
      name: "Transaction Log Explosion"
      description: "Large migration fills transaction log"
      trigger: "Single large transaction"
      difficulty: "medium"
      detection: "Log file full errors"
    
    - trap_id: "RE-003"
      name: "Memory Exhaustion"
      description: "Migration consumes all memory"
      trigger: "Large batch size or index build"
      difficulty: "medium"
      detection: "OOM errors"
    
    - trap_id: "RE-004"
      name: "CPU Saturation"
      description: "Migration saturates CPU"
      trigger: "Unthrottled migration"
      difficulty: "easy"
      detection: "System-wide slowdown"

# =============================================================================
# EDGE CASES - 100+ Edge Cases
# =============================================================================

edge_cases:
  # ---------------------------------------------------------------------------
  # Data Type Edge Cases
  # ---------------------------------------------------------------------------
  data_types:
    - case_id: "DT-001"
      name: "Zero-Width Characters"
      description: "Zero-width Unicode characters in strings"
      migration_impact: "Length calculations incorrect"
    
    - case_id: "DT-002"
      name: "NaN and Infinity in Floats"
      description: "Special float values during conversion"
      migration_impact: "Conversion to DECIMAL fails"
    
    - case_id: "DT-003"
      name: "Leading Zeros in Strings"
      description: "Numeric strings with leading zeros"
      migration_impact: "Lost when converted to numbers"
    
    - case_id: "DT-004"
      name: "Boolean String Representations"
      description: "Various boolean representations (yes/no, 1/0, true/false)"
      migration_impact: "Inconsistent conversion"
    
    - case_id: "DT-005"
      name: "Empty String vs NULL"
      description: "Some databases treat empty string as NULL"
      migration_impact: "Data meaning changes"
      databases: ["Oracle"]

  # ---------------------------------------------------------------------------
  # Temporal Edge Cases
  # ---------------------------------------------------------------------------
  temporal:
    - case_id: "TM-001"
      name: "Daylight Saving Time Transition"
      description: "Times during DST change"
      migration_impact: "Non-existent or duplicate times"
    
    - case_id: "TM-002"
      name: "Year 2038 Problem"
      description: "32-bit timestamp overflow"
      migration_impact: "Future dates wrap to past"
    
    - case_id: "TM-003"
      name: "Year 10000 Problem"
      description: "5-digit years in some systems"
      migration_impact: "Date parsing failures"
    
    - case_id: "TM-004"
      name: "Historical Timezone Changes"
      description: "Timezones changed historically"
      migration_impact: "Historical dates convert incorrectly"
    
    - case_id: "TM-005"
      name: "Leap Year February 29"
      description: "Dates that don't exist every year"
      migration_impact: "Date math errors"

  # ---------------------------------------------------------------------------
  # Constraint Edge Cases
  # ---------------------------------------------------------------------------
  constraints:
    - case_id: "CN-001"
      name: "Self-Referencing FK with Delete"
      description: "Deleting row that references itself"
      migration_impact: "Unexpected cascade behavior"
    
    - case_id: "CN-002"
      name: "Multiple Unique Constraints"
      description: "Table with many unique constraints"
      migration_impact: "Complex duplicate resolution"
    
    - case_id: "CN-003"
      name: "Partial Index Uniqueness"
      description: "Unique constraint via partial index"
      migration_impact: "Constraint lost in migration"
    
    - case_id: "CN-004"
      name: "Expression-Based Constraint"
      description: "Constraint using expression/function"
      migration_impact: "Cross-database compatibility"

  # ---------------------------------------------------------------------------
  # Large Data Edge Cases
  # ---------------------------------------------------------------------------
  large_data:
    - case_id: "LD-001"
      name: "Multi-GB Row"
      description: "Single row with huge LOB data"
      migration_impact: "Memory exhaustion"
    
    - case_id: "LD-002"
      name: "Billion Row Table"
      description: "Table with billions of rows"
      migration_impact: "Extended migration time"
    
    - case_id: "LD-003"
      name: "Wide Table"
      description: "Table with hundreds of columns"
      migration_impact: "Row size limits"
    
    - case_id: "LD-004"
      name: "Deeply Nested FK Chain"
      description: "Long chain of FK relationships"
      migration_impact: "Complex ordering requirements"

  # ---------------------------------------------------------------------------
  # Sequence/Identity Edge Cases
  # ---------------------------------------------------------------------------
  sequences:
    - case_id: "SQ-001"
      name: "Sequence Gaps"
      description: "Gaps in sequence values"
      migration_impact: "Gap handling logic needed"
    
    - case_id: "SQ-002"
      name: "Sequence Overflow"
      description: "Sequence reaching max value"
      migration_impact: "Need to handle wrap or extend"
    
    - case_id: "SQ-003"
      name: "Negative Sequence Values"
      description: "Sequences with negative values"
      migration_impact: "Target system may not support"
    
    - case_id: "SQ-004"
      name: "Sequence Cache Issues"
      description: "Cached sequence values during migration"
      migration_impact: "Duplicate values possible"

# =============================================================================
# DIFFICULTY MULTIPLIERS
# =============================================================================

difficulty_multipliers:
  difficulty_amplifiers:
    nightmare:
      multiplier: 3.0
      description: "Extreme difficulty requiring expert DBA knowledge across multiple systems"
      requirements:
        - "7+ interacting traps across storage engine, query optimizer, and lock manager"
        - "Requires understanding of database internals and source code behavior"
        - "Time estimate: 90+ minutes for senior database engineers"
        - "Cross-database compatibility issues that manifest differently"
        - "Requires synthesizing SQL, transactions, and distributed systems knowledge"
    
    nightmare_plus:
      multiplier: 5.0
      estimated_time: [28800, 172800]  # 8-48 hours
      command_steps: [400, 1500]
      techniques_required: 12
      description: "Database kernel development difficulty requiring DDL internals and storage engine expertise"
      requirements:
        - "12+ deeply interacting traps across DDL, locking, storage, and replication layers"
        - "Requires understanding of online DDL algorithm implementations and edge cases"
        - "Time estimate: 8-48 hours for principal database architects"
        - "Cross-database migration requiring deep type system and feature mapping knowledge"
        - "Requires synthesizing schema evolution, data integrity, and distributed consistency"
        - "Must handle migrations across multiple regions with regulatory constraints"
        - "Requires understanding of CDC integration and logical replication edge cases"
        - "Must design rollback strategies for billion-row table migrations"
  
  storage_engine_internals:
    btree_ddl_interaction:
      description: "B-tree modifications during DDL operations"
      factors:
        - "Index rebuild page allocation patterns"
        - "Concurrent DML during index creation"
        - "Split/merge operations during ALTER TABLE"
        - "Clustered index rebuild data movement"
      multiplier: 1.8
    
    lsm_ddl_compaction:
      description: "LSM-tree interactions during schema changes"
      factors:
        - "Compaction interference with DDL operations"
        - "Memtable flush during schema modification"
        - "SSTable format changes during migration"
        - "Tombstone handling across schema versions"
      multiplier: 2.0
    
    wal_ddl_logging:
      description: "Write-ahead log handling for DDL operations"
      factors:
        - "DDL logging for crash recovery"
        - "WAL size explosion during large migrations"
        - "Checkpoint timing during DDL"
        - "WAL archiving for PITR during migration"
      multiplier: 1.7
    
    buffer_pool_ddl:
      description: "Buffer pool behavior during schema changes"
      factors:
        - "Buffer pool pressure during table rebuild"
        - "Double-write buffer for DDL safety"
        - "Page format changes and buffer invalidation"
        - "Working set displacement during migration"
      multiplier: 1.5
    
    lock_manager_ddl:
      description: "Lock management for DDL operations"
      factors:
        - "Metadata lock acquisition ordering"
        - "Schema lock vs table lock interactions"
        - "Online DDL lock downgrade strategies"
        - "Deadlock detection during schema changes"
      multiplier: 1.9
    
    mvcc_schema_evolution:
      description: "MVCC handling during schema changes"
      factors:
        - "Version visibility during DDL"
        - "Old version cleanup after schema change"
        - "Snapshot isolation during migration"
        - "Read view construction with schema changes"
      multiplier: 2.1
  
  distributed_database_complexity:
    ddl_replication:
      logical_replication_ddl:
        - "DDL not replicated in logical replication"
        - "Schema drift between publisher and subscriber"
        - "Replication slot growth during long DDL"
        - "Replica identity changes during migration"
      statement_vs_row_replication:
        - "DDL statement replication timing"
        - "Row-based replication during schema change"
        - "Mixed format edge cases"
        - "Binlog position during DDL"
      multiplier: 2.3
    
    distributed_ddl_coordination:
      cross_shard_ddl:
        - "Schema consistency across shards"
        - "Atomic DDL across distributed databases"
        - "Rolling schema upgrades"
        - "Schema version mismatch handling"
      metadata_synchronization:
        - "Metadata server DDL propagation"
        - "Routing table updates for schema changes"
        - "Cache invalidation for schema changes"
      multiplier: 2.4
    
    online_ddl_replication:
      ghost_table_replication:
        - "Shadow table replication during gh-ost"
        - "Cutover timing with replication lag"
        - "Binlog consumption during long migrations"
        - "Heartbeat tracking during schema changes"
      multiplier: 2.0
    
    partition_handling_ddl:
      scenarios:
        - "DDL during network partition"
        - "Schema divergence after partition healing"
        - "Quorum requirements for DDL"
        - "Metadata consistency during partition"
      multiplier: 2.2
  
  scale_factors:
    - factor: "billion_row_table"
      multiplier: 2.0
      description: "Table has 1B+ rows"
    
    - factor: "terabyte_database"
      multiplier: 1.8
      description: "Database is 1TB+"
    
    - factor: "thousands_of_tables"
      multiplier: 1.5
      description: "Schema has 1000+ tables"
  
  availability_factors:
    - factor: "zero_downtime"
      multiplier: 2.0
      description: "Absolutely no downtime allowed"
    
    - factor: "limited_maintenance_window"
      multiplier: 1.6
      description: "Only 1-hour maintenance window"
    
    - factor: "global_deployment"
      multiplier: 1.7
      description: "Must migrate across regions"
  
  complexity_factors:
    - factor: "cross_database_migration"
      multiplier: 1.8
      description: "Migrating between different database systems"
    
    - factor: "schema_redesign"
      multiplier: 1.7
      description: "Significant schema restructuring"
    
    - factor: "data_transformation"
      multiplier: 1.5
      description: "Complex data transformation required"
  
  regulatory_factors:
    - factor: "audit_requirements"
      multiplier: 1.4
      description: "Must maintain full audit trail"
    
    - factor: "data_residency"
      multiplier: 1.5
      description: "Data must stay in specific regions"
    
    - factor: "compliance_validation"
      multiplier: 1.3
      description: "Must pass compliance checks"

# =============================================================================
# DATABASE SYSTEM SPECIFICS
# =============================================================================

database_specifics:
  postgresql:
    versions: ["12", "13", "14", "15", "16"]
    online_operations:
      - "CREATE INDEX CONCURRENTLY"
      - "ALTER TABLE ... ADD COLUMN (no default)"
      - "DROP INDEX CONCURRENTLY"
    limitations:
      - "ALTER TABLE with NOT NULL requires rewrite"
      - "RENAME COLUMN requires ACCESS EXCLUSIVE lock"
      - "No transactional DDL for all operations"
    tools:
      - "pg_dump/pg_restore"
      - "pg_repack"
      - "pgloader"
    
  mysql:
    versions: ["5.7", "8.0", "8.1"]
    online_operations:
      - "ALGORITHM=INSTANT (8.0.12+)"
      - "ALGORITHM=INPLACE"
      - "pt-online-schema-change"
      - "gh-ost"
    limitations:
      - "DDL causes implicit commit"
      - "Some changes require table copy"
      - "Foreign key limitations with online DDL"
    tools:
      - "mysqldump"
      - "mysqlpump"
      - "pt-online-schema-change"
      - "gh-ost"
    
  sql_server:
    versions: ["2016", "2017", "2019", "2022"]
    online_operations:
      - "ONLINE = ON (Enterprise)"
      - "RESUMABLE index operations"
    limitations:
      - "Online DDL requires Enterprise"
      - "LOB columns restrict some online ops"
    tools:
      - "BACPAC"
      - "DACFx"
      - "SSIS"
    
  oracle:
    versions: ["19c", "21c", "23c"]
    online_operations:
      - "ONLINE DDL"
      - "DBMS_REDEFINITION"
      - "Edition-Based Redefinition"
    limitations:
      - "Complex edition management"
      - "Space requirements for online ops"
    tools:
      - "Data Pump"
      - "GoldenGate"
      - "DBMS_REDEFINITION"

# =============================================================================
# PROBLEM STATEMENT TEMPLATES
# =============================================================================

problem_statement: |
  A {{ scenario_type }} database requires {{ migration_type }} migration.
  The system uses {{ database_system }} {{ database_version }} with {{ table_count }} tables.
  
  The migration involves:
  {{ migration_operations }}
  
  Current environment:
  - Largest table size: {{ largest_table_rows }} rows
  - Database size: {{ database_size }}
  - Maintenance window: {{ maintenance_window }}
  - Replication: {{ replication_type }}
  
  Critical constraints:
  {{ constraints }}

requirements: |
  - Complete migration within {{ time_constraint }}
  - Achieve {{ data_integrity_goal }} data integrity
  - Maintain {{ availability_requirement }} availability
  - Handle {{ edge_cases }} edge cases
  - Provide {{ rollback_capability }} rollback capability
  - Document {{ documentation_requirements }}

interface: |
  Input: {{ input_description }}
  Output: {{ output_description }}
  Validation: {{ validation_criteria }}

# =============================================================================
# REFERENCE SOLUTION PATTERNS
# =============================================================================

reference_solution: |
  -- Comprehensive Database Migration Patterns
  
  -- ============================================================
  -- PATTERN 1: Non-Blocking Column Addition
  -- ============================================================
  
  /*
  DANGEROUS: Direct column add with default on large table
  
  ALTER TABLE users ADD COLUMN status VARCHAR(20) DEFAULT 'active';
  -- Locks entire table while setting default on all rows!
  -- On 100M rows, this can take hours with table locked.
  */
  
  -- SAFE APPROACH: Three-phase migration
  
  -- Phase 1: Add nullable column (instant in modern PostgreSQL)
  ALTER TABLE users ADD COLUMN status VARCHAR(20);
  
  -- Phase 2: Set default for new rows (instant)
  ALTER TABLE users ALTER COLUMN status SET DEFAULT 'active';
  
  -- Phase 3: Backfill existing rows in batches
  DO $$
  DECLARE
    batch_size INT := 10000;
    rows_updated INT;
    total_updated INT := 0;
  BEGIN
    LOOP
      -- Update batch with row-level locking
      WITH batch AS (
        SELECT id FROM users
        WHERE status IS NULL
        LIMIT batch_size
        FOR UPDATE SKIP LOCKED
      )
      UPDATE users u
      SET status = 'active'
      FROM batch b
      WHERE u.id = b.id;
      
      GET DIAGNOSTICS rows_updated = ROW_COUNT;
      total_updated := total_updated + rows_updated;
      
      RAISE NOTICE 'Updated % rows (total: %)', rows_updated, total_updated;
      
      EXIT WHEN rows_updated = 0;
      
      -- Commit between batches
      COMMIT;
      
      -- Brief pause to reduce load
      PERFORM pg_sleep(0.1);
    END LOOP;
  END $$;
  
  -- Phase 4: Add NOT NULL constraint after all rows filled
  ALTER TABLE users ALTER COLUMN status SET NOT NULL;
  
  -- ============================================================
  -- PATTERN 2: Safe Data Type Change
  -- ============================================================
  
  /*
  DANGEROUS: Direct type change may lose precision
  
  ALTER TABLE products ALTER COLUMN price TYPE INTEGER;
  -- $19.99 becomes $19!
  */
  
  -- SAFE APPROACH: New column + migration + swap
  
  -- Step 1: Add new column with target type
  ALTER TABLE products ADD COLUMN price_cents INTEGER;
  
  -- Step 2: Create trigger for concurrent changes during migration
  CREATE OR REPLACE FUNCTION sync_price_to_cents()
  RETURNS TRIGGER AS $$
  BEGIN
    NEW.price_cents := ROUND(NEW.price * 100)::INTEGER;
    RETURN NEW;
  END;
  $$ LANGUAGE plpgsql;
  
  CREATE TRIGGER trg_sync_price
  BEFORE INSERT OR UPDATE ON products
  FOR EACH ROW
  EXECUTE FUNCTION sync_price_to_cents();
  
  -- Step 3: Backfill existing data
  UPDATE products 
  SET price_cents = ROUND(price * 100)::INTEGER
  WHERE price_cents IS NULL;
  
  -- Step 4: Validate no precision loss
  SELECT COUNT(*) FROM products 
  WHERE ABS(price * 100 - price_cents) > 0.5;
  -- Should return 0
  
  -- Step 5: Update application to use new column
  -- (Deploy application change)
  
  -- Step 6: Drop trigger and old column
  DROP TRIGGER trg_sync_price ON products;
  ALTER TABLE products DROP COLUMN price;
  ALTER TABLE products RENAME COLUMN price_cents TO price;
  
  -- ============================================================
  -- PATTERN 3: Foreign Key Constraint Addition
  -- ============================================================
  
  /*
  DANGEROUS: Adding FK validates all rows with full scan
  
  ALTER TABLE orders ADD FOREIGN KEY (customer_id) REFERENCES customers(id);
  -- Scans entire orders table, locks both tables
  */
  
  -- SAFE APPROACH: Create NOT VALID, then validate separately
  
  -- Step 1: Clean up any orphaned records first
  SELECT o.id, o.customer_id 
  FROM orders o
  LEFT JOIN customers c ON o.customer_id = c.id
  WHERE c.id IS NULL;
  
  -- Handle orphans (delete, set null, or fix)
  DELETE FROM orders 
  WHERE customer_id NOT IN (SELECT id FROM customers);
  
  -- Step 2: Add constraint as NOT VALID (no full scan)
  ALTER TABLE orders 
  ADD CONSTRAINT fk_orders_customer
  FOREIGN KEY (customer_id) REFERENCES customers(id)
  NOT VALID;
  
  -- Step 3: Validate in background (doesn't block writes)
  ALTER TABLE orders VALIDATE CONSTRAINT fk_orders_customer;
  
  -- ============================================================
  -- PATTERN 4: Unique Index Creation
  -- ============================================================
  
  /*
  DANGEROUS: CREATE INDEX locks table
  
  CREATE UNIQUE INDEX idx_users_email ON users(email);
  -- Blocks writes during entire build
  */
  
  -- SAFE APPROACH: Create CONCURRENTLY
  
  -- Step 1: Find and resolve duplicates first
  SELECT email, COUNT(*) as cnt
  FROM users
  GROUP BY email
  HAVING COUNT(*) > 1;
  
  -- Resolve duplicates (strategy depends on business logic)
  -- Example: Keep newest, mark others as duplicate
  WITH duplicates AS (
    SELECT id, email,
           ROW_NUMBER() OVER (PARTITION BY email ORDER BY created_at DESC) as rn
    FROM users
  )
  UPDATE users u
  SET email = u.email || '.duplicate.' || u.id
  FROM duplicates d
  WHERE u.id = d.id AND d.rn > 1;
  
  -- Step 2: Create index concurrently
  CREATE UNIQUE INDEX CONCURRENTLY idx_users_email ON users(email);
  
  -- Note: If concurrent creation fails, clean up invalid index
  -- DROP INDEX CONCURRENTLY idx_users_email;
  
  -- ============================================================
  -- PATTERN 5: Large Table Batch Migration
  -- ============================================================
  
  CREATE OR REPLACE FUNCTION migrate_data_batched(
    p_source_table TEXT,
    p_target_table TEXT,
    p_batch_size INT DEFAULT 10000,
    p_sleep_ms INT DEFAULT 100
  ) RETURNS TABLE (
    batches_processed INT,
    rows_migrated BIGINT,
    errors_encountered INT
  ) AS $$
  DECLARE
    v_last_id BIGINT := 0;
    v_batch_count INT := 0;
    v_total_rows BIGINT := 0;
    v_errors INT := 0;
    v_rows INT;
  BEGIN
    LOOP
      BEGIN
        EXECUTE format(
          'INSERT INTO %I 
           SELECT * FROM %I 
           WHERE id > $1 
           ORDER BY id 
           LIMIT $2',
          p_target_table, p_source_table
        ) USING v_last_id, p_batch_size;
        
        GET DIAGNOSTICS v_rows = ROW_COUNT;
        
        IF v_rows = 0 THEN
          EXIT;
        END IF;
        
        -- Get last processed ID
        EXECUTE format(
          'SELECT MAX(id) FROM %I WHERE id > $1',
          p_source_table
        ) INTO v_last_id USING v_last_id;
        
        v_batch_count := v_batch_count + 1;
        v_total_rows := v_total_rows + v_rows;
        
        COMMIT;
        
        PERFORM pg_sleep(p_sleep_ms / 1000.0);
        
      EXCEPTION WHEN OTHERS THEN
        v_errors := v_errors + 1;
        RAISE WARNING 'Batch error at id %: %', v_last_id, SQLERRM;
        COMMIT;
      END;
    END LOOP;
    
    RETURN QUERY SELECT v_batch_count, v_total_rows, v_errors;
  END;
  $$ LANGUAGE plpgsql;
  
  -- ============================================================
  -- PATTERN 6: Zero-Downtime Column Rename
  -- ============================================================
  
  -- Direct RENAME requires exclusive lock, but we can use views
  
  -- Step 1: Add new column
  ALTER TABLE users ADD COLUMN full_name VARCHAR(200);
  
  -- Step 2: Copy data
  UPDATE users SET full_name = name WHERE full_name IS NULL;
  
  -- Step 3: Create trigger to sync
  CREATE OR REPLACE FUNCTION sync_name_columns()
  RETURNS TRIGGER AS $$
  BEGIN
    IF TG_OP = 'UPDATE' THEN
      IF NEW.name IS DISTINCT FROM OLD.name THEN
        NEW.full_name := NEW.name;
      ELSIF NEW.full_name IS DISTINCT FROM OLD.full_name THEN
        NEW.name := NEW.full_name;
      END IF;
    ELSIF TG_OP = 'INSERT' THEN
      IF NEW.full_name IS NULL THEN
        NEW.full_name := NEW.name;
      END IF;
    END IF;
    RETURN NEW;
  END;
  $$ LANGUAGE plpgsql;
  
  CREATE TRIGGER trg_sync_name
  BEFORE INSERT OR UPDATE ON users
  FOR EACH ROW
  EXECUTE FUNCTION sync_name_columns();
  
  -- Step 4: Migrate application to use full_name
  -- Step 5: Drop old column and trigger
  DROP TRIGGER trg_sync_name ON users;
  ALTER TABLE users DROP COLUMN name;
  
  -- ============================================================
  -- PATTERN 7: Migration Validation
  -- ============================================================
  
  CREATE OR REPLACE FUNCTION validate_migration(
    p_source TEXT,
    p_target TEXT
  ) RETURNS TABLE (
    check_name TEXT,
    status TEXT,
    details TEXT
  ) AS $$
  DECLARE
    v_source_count BIGINT;
    v_target_count BIGINT;
    v_checksum_source TEXT;
    v_checksum_target TEXT;
  BEGIN
    -- Row count check
    EXECUTE format('SELECT COUNT(*) FROM %I', p_source) INTO v_source_count;
    EXECUTE format('SELECT COUNT(*) FROM %I', p_target) INTO v_target_count;
    
    RETURN QUERY SELECT 
      'Row Count'::TEXT,
      CASE WHEN v_source_count = v_target_count THEN 'PASS' ELSE 'FAIL' END,
      format('Source: %s, Target: %s', v_source_count, v_target_count);
    
    -- Additional validation checks...
    
  END;
  $$ LANGUAGE plpgsql;

# =============================================================================
# TEST CASES
# =============================================================================

fail_to_pass:
  - "test_no_table_lock_timeout"
  - "test_no_precision_loss"
  - "test_no_orphaned_records"
  - "test_rollback_possible"
  - "test_zero_downtime_achieved"
  - "test_data_integrity_preserved"
  - "test_constraints_valid_post_migration"
  - "test_all_batches_completed"

pass_to_pass:
  - "test_basic_migration_completes"
  - "test_small_table_migration"
  - "test_empty_table_migration"

# =============================================================================
# VARIABLES FOR TASK GENERATION
# =============================================================================

variables:
  - name: scenario_type
    type: string
    options:
      - "e-commerce platform"
      - "financial trading system"
      - "healthcare records"
      - "social media platform"
      - "IoT data warehouse"
      - "multi-tenant SaaS"
  
  - name: migration_type
    type: string
    options:
      - "schema evolution"
      - "data type change"
      - "cross-database"
      - "sharding migration"
      - "version upgrade"
  
  - name: database_system
    type: string
    options: ["PostgreSQL", "MySQL", "SQL Server", "Oracle"]
  
  - name: database_version
    type: string
    generator: "version_by_system"
  
  - name: table_count
    type: int
    min: 50
    max: 5000
  
  - name: largest_table_rows
    type: string
    options:
      - "10 million"
      - "100 million"
      - "1 billion"
      - "10 billion"
  
  - name: database_size
    type: string
    options:
      - "100 GB"
      - "1 TB"
      - "10 TB"
      - "100 TB"
  
  - name: maintenance_window
    type: string
    options:
      - "none (zero downtime)"
      - "1 hour"
      - "4 hours"
      - "weekend"

# =============================================================================
# ANTI-PATTERNS - LLM Failure Modes
# =============================================================================

anti_patterns:
  llm_failure_modes:
    - "Applying generic SQL patterns without considering database-specific behavior"
    - "Missing isolation level interactions between concurrent transactions"
    - "Ignoring lock manager implementation details"
    - "Not considering query optimizer behavior changes across versions"
    - "Missing hidden full table scans in seemingly optimized queries"
    - "Overlooking index maintenance overhead during writes"
    - "Assuming ORM generates efficient queries"
    - "Missing deadlock potential in cross-schema operations"
    - "Ignoring transaction log and recovery implications"
    - "Assuming ALTER TABLE with DEFAULT is instant without checking database version"
    - "Ignoring metadata lock blocking chain from long-running queries"
    - "Missing precision loss during DECIMAL to FLOAT conversions"
    - "Assuming online DDL algorithms are available for all operations"
    - "Overlooking FK validation full table scan during constraint addition"
    - "Missing implicit commit behavior of DDL in MySQL/Oracle"
    - "Assuming CONCURRENTLY index creation cannot fail"
    - "Ignoring tablespace requirements during large table rebuilds"
    - "Missing encoding conversion data corruption for multi-byte characters"
    - "Assuming NOT NULL addition with DEFAULT is atomic"
    - "Overlooking trigger execution during migration data movement"
    - "Missing replication lag amplification during large DDL operations"
    - "Assuming gh-ost/pt-osc work with all table structures"
    - "Ignoring foreign key constraint ordering during schema migrations"
    - "Missing transaction log explosion during large UPDATE migrations"
    - "Assuming TRUNCATE is always faster than DELETE for migration cleanup"
    - "Overlooking statistics invalidation after major schema changes"
    - "Missing auto-vacuum blocking during long-running DDL in PostgreSQL"
  
  query_optimizer_internals:
    plan_invalidation_during_ddl:
      - "Plan cache flush timing after schema change"
      - "Statistics staleness after data migration"
      - "Index availability during online rebuild"
      - "Histogram accuracy after data transformation"
    migration_query_optimization:
      - "Batch size optimization for data migration"
      - "Parallel DML limitations during migration"
      - "Memory grant requirements for large transforms"
      - "Sort spill during migration operations"
    cross_database_optimizer_differences:
      - "Implicit type conversion differences"
      - "NULL handling variations"
      - "Expression evaluation order differences"
      - "Collation and comparison semantics"

# =============================================================================
# ANTI-HARDCODING MEASURES
# =============================================================================

anti_hardcoding:
  canary_tokens: true
  randomize_paths: true
  dynamic_content: true
  randomize_identifiers: true
  randomize_values: true
  
  migration_risks:
    - "table_lock_timeout"
    - "precision_loss"
    - "fk_orphans"
    - "no_rollback_plan"
    - "concurrent_modification"
    - "resource_exhaustion"
    - "constraint_violation"
  
  trap_injection:
    - inject_precision_trap: true
    - inject_locking_trap: true
    - inject_constraint_trap: true
    - inject_version_quirk: true
