id: "db-opt-query-plan-trap-001"
version: "2.0.0"
category: "databases"
subcategory: "optimization"

# =============================================================================
# LLM GENERATION FRAMEWORK
# =============================================================================
# This configuration enables generation of 10,000+ unique, complex tasks
# related to query execution plans, optimizer behavior, and plan analysis.
# =============================================================================

generation_framework:
  # Difficulty parameters - SIGNIFICANTLY increased for database architect-level challenges
  time_range: [5400, 14400]  # 90-240 minutes for database architects with internals expertise
  command_steps: [80, 200]   # Requires extensive multi-layer investigation and debugging
  
  generation_targets:
    minimum_difficulty: "90+ minutes, requires deep query optimizer and execution engine internals expertise"
    target_audience: "Principal Database Architects and query optimization engineers with 10+ years experience"
    complexity_level: "Nightmare - requires synthesizing knowledge across optimizer algorithms, cost models, and execution engine internals"
  
  multi_agent_orchestration:
    description: "Coordinate 5-8 specialized database agents for complex query plan debugging"
    required_agents:
      - name: "cardinality_estimator_analyst"
        role: "Debug cardinality estimation errors and statistics accuracy"
        expertise: ["histogram analysis", "selectivity estimation", "correlation detection", "join cardinality"]
      - name: "cost_model_debugger"
        role: "Analyze cost model accuracy and parameter tuning"
        expertise: ["I/O costing", "CPU costing", "memory costing", "network costing"]
      - name: "join_order_optimizer"
        role: "Optimize join ordering and analyze search space exploration"
        expertise: ["dynamic programming", "GEQO", "heuristic pruning", "bushy joins"]
      - name: "parallel_execution_expert"
        role: "Debug parallel query execution and worker coordination"
        expertise: ["parallel scan", "parallel hash", "gather nodes", "parallel aggregation"]
      - name: "plan_cache_analyst"
        role: "Debug plan caching, parameter sniffing, and plan stability"
        expertise: ["plan cache pollution", "adaptive plans", "plan forcing", "cache eviction"]
      - name: "subquery_optimizer"
        role: "Analyze subquery flattening, unnesting, and correlation"
        expertise: ["scalar subquery", "EXISTS/IN optimization", "lateral joins", "CTE materialization"]
      - name: "aggregation_optimizer"
        role: "Optimize grouping, sorting, and aggregation operations"
        expertise: ["hash vs sort aggregation", "partial aggregation", "distinct optimization", "window functions"]
      - name: "execution_engine_profiler"
        role: "Profile execution engine behavior and resource consumption"
        expertise: ["spill to disk", "memory grants", "batch processing", "vectorized execution"]
    
    cross_engine_attack_chains:
      - chain: "stale_statistics -> cardinality_error -> wrong_join_order -> nested_loop_explosion -> memory_exhaustion"
        description: "Stale statistics cause cascading optimizer failures leading to resource exhaustion"
      - chain: "parameter_sniffing -> suboptimal_plan -> plan_cache_lock_in -> performance_regression -> application_timeout"
        description: "Parameter sniffing causes plan instability affecting application performance"
      - chain: "correlated_columns -> selectivity_underestimate -> hash_join_on_huge -> spill_to_disk -> I/O_saturation"
        description: "Column correlation causes underestimate leading to I/O-bound execution"
      - chain: "CTE_materialization -> intermediate_explosion -> temp_space_exhaustion -> query_failure -> batch_abort"
        description: "Unnecessary CTE materialization causes temp space issues"
    
    parallel_analysis_requirements:
      - "Concurrent plan comparison across parameter value ranges"
      - "Parallel cardinality estimation verification"
      - "Simultaneous cost model calibration testing"
      - "Coordinated plan stability analysis over time"
    
    agent_handoff_scenarios:
      - from: "cardinality_estimator_analyst"
        to: "join_order_optimizer"
        trigger: "Cardinality error causing join order issues"
      - from: "plan_cache_analyst"
        to: "cost_model_debugger"
        trigger: "Plan caching revealing cost model inaccuracies"
      - from: "parallel_execution_expert"
        to: "execution_engine_profiler"
        trigger: "Parallel execution issues need deep engine analysis"
  
  multi_conversation_workflow:
    phase_1_research: |
      Research advanced query optimizer concepts:
      - Cost-based optimization internals
      - Cardinality estimation algorithms
      - Join order optimization
      - Plan caching and reuse
      - Adaptive query execution
    
    phase_2_creation: |
      Create task with hidden complexity and traps including:
      - Parameter sniffing issues
      - Statistics staleness problems
      - Join order suboptimality
      - Plan cache pollution
      - Cardinality misestimation
    
    phase_3_amplification: |
      Add difficulty multipliers and edge cases:
      - Complex multi-table joins
      - Correlated subqueries
      - Window functions
      - CTEs and recursive queries
      - Parallel query issues
    
    phase_4_verification: |
      Validate task requires deep optimizer expertise:
      - Must understand cost model at implementation level
      - Must know statistics generation, histogram usage, and correlation handling
      - Cannot be solved by adding indexes alone or simple hints
      - Requires deep plan analysis with estimated vs actual comparison
      - Tests understanding of optimizer decisions and search space exploration
      - Has at least 10+ deeply interacting traps across optimizer/statistics/execution/memory layers
      - Has cascading failure modes that span cardinality estimation, join selection, and resource management
      - Requires knowledge of optimizer algorithms, cost model calibration, and execution engine internals
      - Would take experienced database architects 90+ minutes
      - Involves complex queries with multiple join orderings and subquery transformations
  
  quality_requirements:
    - "Must require understanding of optimizer internals at algorithm level"
    - "Must have at least 8 non-obvious plan issues with complex interactions"
    - "Must not be solvable by simple index creation or straightforward hints"
    - "Must require deep execution plan analysis with resource consumption correlation"
    - "Must involve database-specific optimizer behaviors and version differences"
    - "90+ minutes for experienced database architects, 180+ for senior engineers"
    - "Requires synthesizing cost models, cardinality estimation, join optimization, and execution engine behavior"
    - "Must involve plan stability analysis across parameter ranges"
    - "Requires understanding of adaptive query execution and runtime feedback"

# =============================================================================
# TOPIC UNIVERSE - 150+ Query Plan Topics
# =============================================================================

topic_universe:
  # ---------------------------------------------------------------------------
  # Query Optimizer Architecture
  # ---------------------------------------------------------------------------
  optimizer_architecture:
    cost_based_optimization:
      description: "Estimate cost of different plans, choose lowest"
      components:
        parser: "SQL to parse tree"
        rewriter: "View expansion, rule application"
        planner: "Generate candidate plans"
        optimizer: "Choose optimal plan"
        executor: "Execute chosen plan"
      cost_model:
        factors:
          - "I/O cost (disk reads)"
          - "CPU cost (comparisons, calculations)"
          - "Memory cost (sorts, hash tables)"
          - "Network cost (distributed queries)"
        estimation:
          - "Based on statistics"
          - "Cardinality estimates"
          - "Selectivity estimates"
    
    rule_based_optimization:
      description: "Apply transformation rules"
      transformations:
        - "Predicate pushdown"
        - "Projection pushdown"
        - "Join reordering"
        - "Subquery unnesting"
        - "Common subexpression elimination"
      databases: ["Oracle (historical)", "Some embedded DBs"]
    
    adaptive_optimization:
      description: "Adjust plan based on runtime feedback"
      implementations:
        sql_server: "Adaptive joins (2017+)"
        oracle: "Adaptive plans (12c+)"
        postgresql: "JIT compilation, adaptive execution"
      features:
        - "Runtime cardinality feedback"
        - "Plan switching mid-execution"
        - "Memory grant feedback"

  # ---------------------------------------------------------------------------
  # Cardinality Estimation
  # ---------------------------------------------------------------------------
  cardinality_estimation:
    statistics:
      types:
        histograms:
          equi_width: "Equal-size buckets"
          equi_depth: "Equal-frequency buckets"
          top_n: "Frequent values tracked separately"
          hybrid: "Combination approaches"
        single_column:
          - "NDV (number of distinct values)"
          - "NULL fraction"
          - "Average column width"
          - "Min/max values"
        multi_column:
          - "Extended statistics"
          - "Correlation statistics"
          - "Functional dependencies"
      staleness:
        causes:
          - "Data changes without ANALYZE"
          - "Sampling errors"
          - "Bulk loads"
        effects:
          - "Wrong index choice"
          - "Wrong join order"
          - "Wrong join algorithm"
        detection:
          postgresql: "pg_stat_user_tables.last_analyze"
          mysql: "information_schema.STATISTICS"
          sql_server: "STATS_DATE() function"
    
    estimation_challenges:
      correlation:
        description: "Columns are correlated"
        example: "city and state are correlated"
        solution: "Extended statistics, multi-column stats"
      
      skew:
        description: "Non-uniform data distribution"
        example: "90% orders from top 10% customers"
        solution: "Histogram-aware estimation"
      
      foreign_key_assumptions:
        description: "FK relationships affect join estimates"
        optimization: "Some optimizers assume FK join = inner table size"
      
      parameter_uncertainty:
        description: "Unknown parameter values at plan time"
        solutions:
          - "Average selectivity"
          - "Plan caching"
          - "Adaptive cursor sharing"

  # ---------------------------------------------------------------------------
  # Join Optimization
  # ---------------------------------------------------------------------------
  join_optimization:
    join_algorithms:
      nested_loop:
        description: "For each outer row, scan inner"
        best_for:
          - "Small outer table"
          - "Indexed inner table"
          - "Highly selective joins"
        complexity: "O(N * M) worst case, O(N * log M) with index"
        variants:
          simple: "Basic nested loop"
          index: "Use index on inner"
          block: "Process in blocks"
      
      hash_join:
        description: "Build hash table, probe with other side"
        phases:
          build: "Create hash table from smaller table"
          probe: "Scan larger table, probe hash"
        best_for:
          - "Larger tables"
          - "No useful indexes"
          - "Equality joins"
        memory: "Needs memory for hash table"
        variants:
          grace: "Partition to disk if needed"
          hybrid: "Keep some buckets in memory"
      
      merge_join:
        description: "Merge two sorted inputs"
        prerequisite: "Both sides sorted on join key"
        best_for:
          - "Already sorted data"
          - "Large tables"
          - "Equality and range joins"
        complexity: "O(N + M) if sorted"
      
      index_join:
        description: "Use index to find matching rows"
        best_for: "Highly selective joins"
    
    join_order_optimization:
      importance: "N tables = N! possible orders"
      approaches:
        exhaustive: "Try all orders (small N)"
        heuristic: "Rules to limit search"
        genetic: "Genetic algorithm (PostgreSQL GEQO)"
        dynamic_programming: "Build up optimal subplans"
      bushy_vs_linear:
        linear: "((A ⋈ B) ⋈ C) ⋈ D"
        bushy: "(A ⋈ B) ⋈ (C ⋈ D)"

  # ---------------------------------------------------------------------------
  # Plan Caching
  # ---------------------------------------------------------------------------
  plan_caching:
    purpose:
      - "Avoid repeated optimization"
      - "Share plans across sessions"
      - "Reduce CPU usage"
    
    implementations:
      postgresql:
        prepared_statements: "Plan cached per session"
        generic_plans: "After 5 executions"
        plan_invalidation: "DDL, statistics update"
      
      mysql:
        query_cache: "Deprecated in 8.0"
        prepared_statements: "Server-side prepare"
      
      sql_server:
        plan_cache: "Procedure cache"
        plan_guides: "Force specific plans"
        query_store: "Plan history and forcing"
      
      oracle:
        cursor_cache: "Shared cursor cache"
        adaptive_cursor_sharing: "Multiple plans per query"
        sql_plan_baselines: "Plan stability"
    
    issues:
      parameter_sniffing:
        description: "Plan optimized for first parameter"
        problem: "Poor plan for different parameters"
        solutions:
          - "OPTION (RECOMPILE)"
          - "OPTIMIZE FOR UNKNOWN"
          - "Plan guides"
          - "Local variables"
      
      plan_cache_pollution:
        description: "Many similar plans waste cache"
        causes:
          - "Ad-hoc queries with literals"
          - "ORM-generated queries"
        solutions:
          - "Parameterized queries"
          - "OPTIMIZE FOR AD HOC"
          - "Forced parameterization"

  # ---------------------------------------------------------------------------
  # Execution Plan Analysis
  # ---------------------------------------------------------------------------
  plan_analysis:
    reading_plans:
      postgresql:
        commands:
          - "EXPLAIN"
          - "EXPLAIN ANALYZE"
          - "EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON)"
        output:
          - "Node types"
          - "Cost estimates"
          - "Actual vs estimated rows"
          - "Buffer usage"
      
      mysql:
        commands:
          - "EXPLAIN"
          - "EXPLAIN ANALYZE (8.0.18+)"
          - "EXPLAIN FORMAT=JSON"
        output:
          - "Access type"
          - "Possible and used keys"
          - "Rows examined"
          - "Extra information"
      
      sql_server:
        commands:
          - "SET SHOWPLAN_ALL ON"
          - "SET STATISTICS PROFILE ON"
          - "Actual Execution Plan"
        output:
          - "Physical operators"
          - "Estimated vs actual"
          - "Warnings"
          - "Missing index hints"
      
      oracle:
        commands:
          - "EXPLAIN PLAN FOR"
          - "DBMS_XPLAN.DISPLAY"
          - "V$SQL_PLAN"
        output:
          - "Operation tree"
          - "Cost and cardinality"
          - "Bytes and time"
    
    key_metrics:
      estimated_vs_actual:
        description: "Compare optimizer estimates to reality"
        warning_threshold: "10x difference concerning"
        causes_of_mismatch:
          - "Stale statistics"
          - "Correlation not captured"
          - "Parameter values different"
      
      buffer_hits_vs_reads:
        description: "Cache efficiency"
        good: "High hit ratio"
        bad: "Mostly disk reads"
      
      rows_processed:
        description: "Rows flowing through operators"
        look_for: "Large row counts with filters late"
      
      operator_costs:
        description: "Relative cost of each step"
        focus: "Highest cost operators"

  # ---------------------------------------------------------------------------
  # Common Plan Problems
  # ---------------------------------------------------------------------------
  plan_problems:
    wrong_join_order:
      symptoms:
        - "Large intermediate results"
        - "Long execution time"
      causes:
        - "Bad cardinality estimates"
        - "Missing statistics"
      solutions:
        - "Update statistics"
        - "Join hints"
        - "Query rewrite"
    
    wrong_join_type:
      symptoms:
        - "Nested loop on large tables"
        - "Hash join with poor memory"
      causes:
        - "Cardinality misestimation"
        - "Missing indexes"
      solutions:
        - "Hints"
        - "Statistics update"
        - "Index creation"
    
    missing_predicate_pushdown:
      symptoms:
        - "Filter late in plan"
        - "Large row counts early"
      causes:
        - "Complex expressions"
        - "Function calls"
      solutions:
        - "Simplify predicates"
        - "Computed columns"
    
    unnecessary_operations:
      sorts:
        cause: "ORDER BY, DISTINCT, certain joins"
        solution: "Index providing order"
      spools:
        cause: "Repeated access to same data"
        solution: "Materialize CTEs, temp tables"
      hash_aggregates:
        cause: "GROUP BY without index"
        solution: "Index on group columns"

  # ---------------------------------------------------------------------------
  # Database-Specific Optimizer Features
  # ---------------------------------------------------------------------------
  database_specific:
    postgresql:
      features:
        - "Genetic Query Optimizer (GEQO)"
        - "JIT compilation"
        - "Parallel query"
        - "Partitionwise operations"
      settings:
        - "random_page_cost"
        - "effective_cache_size"
        - "work_mem"
        - "join_collapse_limit"
    
    mysql:
      features:
        - "Index merge"
        - "Hash join (8.0.18+)"
        - "Histograms (8.0+)"
        - "Invisible indexes"
      settings:
        - "optimizer_switch"
        - "optimizer_cost_model"
        - "join_buffer_size"
    
    sql_server:
      features:
        - "Query Store"
        - "Adaptive joins"
        - "Batch mode"
        - "Memory grant feedback"
      settings:
        - "MAXDOP"
        - "Cost threshold for parallelism"
        - "Compatibility level"
    
    oracle:
      features:
        - "Adaptive plans"
        - "SQL Plan Management"
        - "Auto-tuning optimizer"
        - "Result cache"
      settings:
        - "OPTIMIZER_MODE"
        - "OPTIMIZER_FEATURES_ENABLE"
        - "OPTIMIZER_INDEX_COST_ADJ"

# =============================================================================
# TRAP TYPES - 80+ Query Plan Traps
# =============================================================================

trap_types:
  # ---------------------------------------------------------------------------
  # Parameter Sniffing Traps
  # ---------------------------------------------------------------------------
  parameter_sniffing:
    - trap_id: "PS-001"
      name: "First Execution Skew"
      description: "Plan optimized for atypical first parameter"
      trigger: "First call with outlier value"
      difficulty: "hard"
      example: |
        -- First call: customer_id = 1 (has 1 million orders)
        -- Plan uses table scan
        -- Next call: customer_id = 99999 (has 5 orders)
        -- Same plan (table scan) - terrible for 5 rows!
      solutions:
        - "OPTION (RECOMPILE)"
        - "OPTIMIZE FOR UNKNOWN"
        - "Local variable trick"
        - "Plan guides"
    
    - trap_id: "PS-002"
      name: "Plan Cache Flip-Flopping"
      description: "Plan keeps changing based on parameters"
      trigger: "Bimodal data distribution"
      difficulty: "hard"
      consequence: "Inconsistent performance"
    
    - trap_id: "PS-003"
      name: "Multi-Statement Procedure Sniffing"
      description: "One statement sniffs, affects others"
      trigger: "Procedure with multiple queries"
      difficulty: "hard"

  # ---------------------------------------------------------------------------
  # Statistics Traps
  # ---------------------------------------------------------------------------
  statistics:
    - trap_id: "ST-001"
      name: "Stale Statistics"
      description: "Statistics not updated after data changes"
      trigger: "Large data load without ANALYZE"
      difficulty: "medium"
      detection: "Estimated vs actual rows diverge"
    
    - trap_id: "ST-002"
      name: "Sampling Inaccuracy"
      description: "Statistics sample missed skew"
      trigger: "Skewed data, low sample rate"
      difficulty: "hard"
      solution: "Increase sample size"
    
    - trap_id: "ST-003"
      name: "Correlated Columns"
      description: "Optimizer assumes independence"
      trigger: "WHERE city = 'NYC' AND state = 'NY'"
      difficulty: "hard"
      example: |
        -- City and state are correlated
        -- Optimizer estimates: selectivity(city) * selectivity(state)
        -- Reality: selectivity is just selectivity(city)
      solution: "Extended statistics, multi-column stats"
    
    - trap_id: "ST-004"
      name: "Histogram Bucket Boundary"
      description: "Value falls on bucket boundary"
      trigger: "Query on bucket edge"
      difficulty: "hard"
    
    - trap_id: "ST-005"
      name: "NULL Statistics"
      description: "NULL fraction affects estimates"
      trigger: "High NULL percentage"
      difficulty: "medium"

  # ---------------------------------------------------------------------------
  # Join Order Traps
  # ---------------------------------------------------------------------------
  join_order:
    - trap_id: "JO-001"
      name: "Explosive Intermediate Results"
      description: "Bad join order creates huge intermediate"
      trigger: "Many-to-many join early"
      difficulty: "hard"
      example: |
        -- Tables: A(10K), B(100K), C(10K)
        -- Bad: A ⋈ B (1M rows) ⋈ C
        -- Good: A ⋈ C (100 rows) ⋈ B
    
    - trap_id: "JO-002"
      name: "Missing Foreign Key Info"
      description: "Optimizer doesn't know FK relationship"
      trigger: "No declared FK, optimizer guesses"
      difficulty: "medium"
    
    - trap_id: "JO-003"
      name: "Too Many Tables"
      description: "Optimizer gives up on optimization"
      trigger: "More tables than join_collapse_limit"
      difficulty: "hard"
      databases: ["PostgreSQL"]
    
    - trap_id: "JO-004"
      name: "View Prevents Reordering"
      description: "View boundary blocks optimization"
      trigger: "Joining to complex view"
      difficulty: "hard"

  # ---------------------------------------------------------------------------
  # Cardinality Estimation Traps
  # ---------------------------------------------------------------------------
  cardinality:
    - trap_id: "CE-001"
      name: "Selectivity Underestimate"
      description: "Optimizer thinks more rows will match"
      trigger: "Skewed data, missing histogram"
      difficulty: "medium"
      consequence: "Chooses hash join when nested loop better"
    
    - trap_id: "CE-002"
      name: "Selectivity Overestimate"
      description: "Optimizer thinks fewer rows will match"
      trigger: "Filter has higher selectivity than estimated"
      difficulty: "medium"
      consequence: "Chooses nested loop when hash better"
    
    - trap_id: "CE-003"
      name: "Multiplication of Selectivities"
      description: "Independent assumption wrong"
      trigger: "Correlated predicates"
      difficulty: "hard"
    
    - trap_id: "CE-004"
      name: "Derived Table Estimation"
      description: "Subquery/CTE cardinality unknown"
      trigger: "Complex subquery"
      difficulty: "hard"
    
    - trap_id: "CE-005"
      name: "Function Return Estimate"
      description: "Optimizer doesn't know function selectivity"
      trigger: "WHERE custom_function(col) = value"
      difficulty: "medium"

  # ---------------------------------------------------------------------------
  # Sort and Aggregate Traps
  # ---------------------------------------------------------------------------
  sort_aggregate:
    - trap_id: "SA-001"
      name: "Unnecessary Sort"
      description: "Sort when index could provide order"
      trigger: "ORDER BY without matching index"
      difficulty: "easy"
    
    - trap_id: "SA-002"
      name: "Sort Spill to Disk"
      description: "Sort exceeds memory"
      trigger: "Large result set, low work_mem"
      difficulty: "medium"
    
    - trap_id: "SA-003"
      name: "Hash Aggregate Spill"
      description: "Aggregation exceeds memory"
      trigger: "Many groups, low memory"
      difficulty: "medium"
    
    - trap_id: "SA-004"
      name: "DISTINCT vs GROUP BY"
      description: "Different execution strategies"
      trigger: "Choice between approaches"
      difficulty: "medium"

  # ---------------------------------------------------------------------------
  # Subquery Traps
  # ---------------------------------------------------------------------------
  subqueries:
    - trap_id: "SQ-001"
      name: "Correlated Subquery Execution"
      description: "Subquery executed for each row"
      trigger: "Correlated subquery not unnested"
      difficulty: "hard"
      example: |
        -- Executed once per customer row
        SELECT * FROM customers c
        WHERE EXISTS (
          SELECT 1 FROM orders o 
          WHERE o.customer_id = c.id AND o.status = 'pending'
        )
    
    - trap_id: "SQ-002"
      name: "Subquery Not Flattened"
      description: "Optimizer can't unnest subquery"
      trigger: "Complex subquery structure"
      difficulty: "hard"
    
    - trap_id: "SQ-003"
      name: "IN vs EXISTS Performance"
      description: "Different optimization paths"
      trigger: "Large IN list"
      difficulty: "medium"
    
    - trap_id: "SQ-004"
      name: "Scalar Subquery Caching"
      description: "Repeated scalar subquery"
      trigger: "Scalar subquery returns same value"
      difficulty: "medium"

  # ---------------------------------------------------------------------------
  # Parallel Query Traps
  # ---------------------------------------------------------------------------
  parallel:
    - trap_id: "PQ-001"
      name: "Parallelism Overhead"
      description: "Parallel slower than serial for small data"
      trigger: "Small table with parallel plan"
      difficulty: "medium"
    
    - trap_id: "PQ-002"
      name: "Parallel Worker Starvation"
      description: "Not enough workers available"
      trigger: "Many concurrent queries"
      difficulty: "medium"
    
    - trap_id: "PQ-003"
      name: "Gather Node Bottleneck"
      description: "Single-threaded merge point"
      trigger: "Heavy processing after parallel"
      difficulty: "hard"
    
    - trap_id: "PQ-004"
      name: "Skewed Parallel Distribution"
      description: "One worker does most work"
      trigger: "Data skew with hash distribution"
      difficulty: "hard"

  # ---------------------------------------------------------------------------
  # CTE and Derived Table Traps
  # ---------------------------------------------------------------------------
  cte_derived:
    - trap_id: "CT-001"
      name: "CTE Materialization"
      description: "CTE forces materialization"
      trigger: "CTE referenced multiple times"
      difficulty: "medium"
      database: "PostgreSQL (pre-12 always materializes)"
    
    - trap_id: "CT-002"
      name: "CTE Optimization Fence"
      description: "Optimizer can't push predicates into CTE"
      trigger: "Filter on CTE output"
      difficulty: "hard"
    
    - trap_id: "CT-003"
      name: "Recursive CTE Performance"
      description: "Recursive query slow"
      trigger: "Deep recursion, large results"
      difficulty: "hard"

# =============================================================================
# EDGE CASES - 100+ Edge Cases
# =============================================================================

edge_cases:
  # ---------------------------------------------------------------------------
  # Data Distribution Edge Cases
  # ---------------------------------------------------------------------------
  distribution:
    - case_id: "DD-001"
      name: "Bimodal Distribution"
      description: "Data has two peaks"
      impact: "Single histogram misleads"
    
    - case_id: "DD-002"
      name: "Long Tail Distribution"
      description: "Many rare values"
      impact: "Histogram misses rare values"
    
    - case_id: "DD-003"
      name: "Temporal Skew"
      description: "Recent data much more frequent"
      impact: "Statistics represent historical, not current"
    
    - case_id: "DD-004"
      name: "NULL Skew"
      description: "Many NULLs in column"
      impact: "IS NULL has high selectivity"

  # ---------------------------------------------------------------------------
  # Query Structure Edge Cases
  # ---------------------------------------------------------------------------
  query_structure:
    - case_id: "QS-001"
      name: "UNION vs UNION ALL"
      description: "UNION removes duplicates (adds sort)"
      impact: "Unnecessary sort for known-distinct"
    
    - case_id: "QS-002"
      name: "Excessive OR Conditions"
      description: "Many OR conditions"
      impact: "Can't use single index efficiently"
    
    - case_id: "QS-003"
      name: "CASE in WHERE"
      description: "CASE expression in filter"
      impact: "Hard for optimizer to analyze"
    
    - case_id: "QS-004"
      name: "Window Function Ordering"
      description: "Window function needs sort"
      impact: "May sort large result"

  # ---------------------------------------------------------------------------
  # Plan Cache Edge Cases
  # ---------------------------------------------------------------------------
  plan_cache:
    - case_id: "PC-001"
      name: "Plan Cache Eviction"
      description: "Good plan evicted under memory pressure"
      impact: "Recompilation overhead"
    
    - case_id: "PC-002"
      name: "Schema Change Invalidation"
      description: "DDL invalidates plans"
      impact: "Mass recompilation after ALTER"
    
    - case_id: "PC-003"
      name: "Statistics Update Invalidation"
      description: "New statistics invalidate plans"
      impact: "Plans recompiled with new stats"

  # ---------------------------------------------------------------------------
  # Cross-Database Edge Cases
  # ---------------------------------------------------------------------------
  cross_database:
    - case_id: "CD-001"
      name: "Optimizer Behavior Differs"
      description: "Same query, different plans"
      impact: "Porting query changes performance"
    
    - case_id: "CD-002"
      name: "Hint Syntax Differs"
      description: "Hints not portable"
      impact: "Hints in application code break"

# =============================================================================
# DIFFICULTY MULTIPLIERS
# =============================================================================

difficulty_multipliers:
  difficulty_amplifiers:
    nightmare:
      multiplier: 3.0
      description: "Extreme difficulty requiring expert DBA knowledge across multiple systems"
      requirements:
        - "7+ interacting traps across storage engine, query optimizer, and lock manager"
        - "Requires understanding of database internals and source code behavior"
        - "Time estimate: 90+ minutes for senior database engineers"
        - "Cross-database compatibility issues that manifest differently"
        - "Requires synthesizing SQL, transactions, and distributed systems knowledge"
    
    nightmare_plus:
      multiplier: 5.0
      estimated_time: [28800, 172800]  # 8-48 hours
      command_steps: [400, 1500]
      techniques_required: 12
      description: "Database kernel development difficulty requiring optimizer algorithm and execution engine internals expertise"
      requirements:
        - "12+ deeply interacting traps across optimizer, statistics, execution, and resource management layers"
        - "Requires understanding of optimizer source code and cost model implementation"
        - "Time estimate: 8-48 hours for principal database architects"
        - "Cross-database optimizer behavior requiring deep algorithm knowledge"
        - "Requires synthesizing cardinality estimation, join enumeration, and execution strategies"
        - "Must debug complex queries with 10+ tables and multiple transformation options"
        - "Requires understanding of adaptive query processing and feedback mechanisms"
        - "Must analyze plan regressions across statistics updates and schema changes"
  
  storage_engine_internals:
    optimizer_architecture:
      description: "Query optimizer architecture and processing stages"
      factors:
        - "Parse tree to logical plan transformation"
        - "Logical to physical plan mapping"
        - "Plan enumeration algorithm complexity"
        - "Memo-based vs rule-based optimization"
      multiplier: 2.0
    
    cardinality_estimation_internals:
      description: "Cardinality estimation implementation details"
      factors:
        - "Histogram generation algorithms"
        - "Selectivity computation methods"
        - "Multi-column correlation handling"
        - "Runtime cardinality feedback"
      multiplier: 2.1
    
    cost_model_implementation:
      description: "Cost model calculation internals"
      factors:
        - "I/O cost calibration parameters"
        - "CPU costing for operators"
        - "Memory costing for hash operations"
        - "Parallel execution cost modeling"
      multiplier: 1.9
    
    join_optimization_algorithms:
      description: "Join order optimization algorithms"
      factors:
        - "Dynamic programming bottom-up"
        - "Greedy heuristics for large joins"
        - "Genetic algorithm parameters"
        - "Adaptive join algorithm selection"
      multiplier: 2.0
    
    execution_engine_internals:
      description: "Query execution engine implementation"
      factors:
        - "Iterator model vs vectorized execution"
        - "Memory management for operators"
        - "Spill-to-disk algorithms"
        - "Pipeline parallelism"
      multiplier: 1.8
    
    plan_cache_implementation:
      description: "Plan cache management internals"
      factors:
        - "Plan compilation caching"
        - "Plan sharing across sessions"
        - "Cache invalidation triggers"
        - "Generic vs specific plan selection"
      multiplier: 1.7
  
  distributed_database_complexity:
    distributed_query_optimization:
      cross_node_optimization:
        - "Distributed join strategies"
        - "Data movement cost estimation"
        - "Network latency in cost model"
        - "Partition-wise operations"
      query_coordinator:
        - "Plan fragmentation and distribution"
        - "Intermediate result routing"
        - "Parallel aggregation coordination"
        - "Error handling in distributed execution"
      multiplier: 2.4
    
    statistics_in_distributed:
      distributed_statistics:
        - "Global vs local statistics"
        - "Statistics freshness across nodes"
        - "Sampling in distributed environment"
        - "Correlation across partitions"
      multiplier: 1.9
    
    federated_query_optimization:
      heterogeneous_optimization:
        - "Cross-database cost comparison"
        - "Remote query capability detection"
        - "Pushdown optimization"
        - "Foreign data wrapper costing"
      multiplier: 2.2
    
    adaptive_distributed_execution:
      runtime_adaptation:
        - "Runtime repartitioning decisions"
        - "Skew handling in distributed joins"
        - "Adaptive parallelism adjustment"
        - "Resource grant feedback"
      multiplier: 2.0
  
  query_complexity:
    - factor: "many_tables"
      multiplier: 1.6
      description: "10+ tables in query"
    
    - factor: "complex_subqueries"
      multiplier: 1.5
      description: "Correlated and nested subqueries"
    
    - factor: "window_functions"
      multiplier: 1.4
      description: "Complex window functions"
  
  data_characteristics:
    - factor: "highly_skewed"
      multiplier: 1.5
      description: "Very skewed data distribution"
    
    - factor: "temporal_patterns"
      multiplier: 1.3
      description: "Strong temporal access patterns"
    
    - factor: "null_heavy"
      multiplier: 1.3
      description: "High NULL percentage"
  
  environment:
    - factor: "production_system"
      multiplier: 1.5
      description: "Cannot test freely"
    
    - factor: "no_statistics_control"
      multiplier: 1.4
      description: "Cannot update statistics on demand"

# =============================================================================
# DATABASE SYSTEM SPECIFICS
# =============================================================================

database_specifics:
  postgresql:
    versions: ["12", "13", "14", "15", "16"]
    plan_features:
      - "EXPLAIN ANALYZE"
      - "pg_stat_statements"
      - "auto_explain"
      - "JIT compilation"
    tuning_parameters:
      - "random_page_cost"
      - "effective_cache_size"
      - "work_mem"
      - "geqo_threshold"
    
  mysql:
    versions: ["5.7", "8.0", "8.1"]
    plan_features:
      - "EXPLAIN ANALYZE"
      - "optimizer_trace"
      - "performance_schema"
    tuning_parameters:
      - "optimizer_switch"
      - "join_buffer_size"
      - "sort_buffer_size"
    
  sql_server:
    versions: ["2016", "2017", "2019", "2022"]
    plan_features:
      - "Query Store"
      - "Actual execution plan"
      - "Plan forcing"
      - "Automatic tuning"
    tuning_parameters:
      - "MAXDOP"
      - "Cost threshold for parallelism"
      - "Compatibility level"
    
  oracle:
    versions: ["19c", "21c", "23c"]
    plan_features:
      - "DBMS_XPLAN"
      - "SQL Plan Management"
      - "AWR/ASH"
      - "SQL Tuning Advisor"
    tuning_parameters:
      - "OPTIMIZER_MODE"
      - "OPTIMIZER_FEATURES_ENABLE"

# =============================================================================
# PROBLEM STATEMENT TEMPLATES
# =============================================================================

problem_statement: |
  A {{ application_type }} database has query performance issues.
  The system uses {{ database_system }} {{ database_version }}.
  
  Query characteristics:
  {{ query_description }}
  
  Performance symptoms:
  {{ performance_symptoms }}
  
  Execution plan shows:
  {{ plan_observations }}
  
  Environment:
  - Data size: {{ data_size }}
  - Concurrent users: {{ concurrent_users }}
  - Available memory: {{ available_memory }}

requirements: |
  - Analyze the execution plan
  - Identify the root cause of poor performance
  - Propose optimizations
  - Ensure solution handles {{ edge_cases }}
  - Verify improvement with metrics

interface: |
  Input: {{ input_description }}
  Output: {{ output_description }}
  Validation: {{ validation_criteria }}

# =============================================================================
# REFERENCE SOLUTION PATTERNS
# =============================================================================

reference_solution: |
  -- Query Plan Optimization Patterns
  
  -- ============================================================
  -- PATTERN 1: Reading Execution Plans
  -- ============================================================
  
  -- PostgreSQL detailed plan
  EXPLAIN (ANALYZE, BUFFERS, COSTS, TIMING, FORMAT TEXT)
  SELECT * FROM orders o
  JOIN customers c ON o.customer_id = c.id
  WHERE c.country = 'US' AND o.status = 'pending';
  
  /*
  Key things to look for:
  
  1. Seq Scan vs Index Scan
     - Seq Scan on large table usually bad
     - Check if index exists and why not used
  
  2. Estimated vs Actual Rows
     - Big difference = statistics problem
     - E.g., "rows=1000" vs "actual rows=100000"
  
  3. Buffers (shared hit/read)
     - Hit = from cache (good)
     - Read = from disk (slower)
  
  4. Nested Loop on large tables
     - Usually indicates missing index
     - Or bad cardinality estimate
  
  5. Sort operations
     - Look for "Sort Method: external merge"
     - Means spilled to disk
  */
  
  -- ============================================================
  -- PATTERN 2: Parameter Sniffing Solutions
  -- ============================================================
  
  /*
  PROBLEM:
  Stored procedure optimized for first parameter value.
  
  CREATE PROCEDURE GetOrdersByCustomer @customer_id INT
  AS
    SELECT * FROM orders WHERE customer_id = @customer_id
  
  First call: @customer_id = 1 (whale customer, 1M orders)
  Plan: Table scan (appropriate for 1M rows)
  
  Second call: @customer_id = 999 (small customer, 10 orders)  
  Plan: Still table scan (TERRIBLE for 10 rows)
  */
  
  -- Solution 1: OPTION (RECOMPILE)
  -- Recompile every time - use for variable workloads
  SELECT * FROM orders WHERE customer_id = @customer_id
  OPTION (RECOMPILE);
  
  -- Solution 2: OPTIMIZE FOR UNKNOWN
  -- Use average statistics
  SELECT * FROM orders WHERE customer_id = @customer_id
  OPTION (OPTIMIZE FOR UNKNOWN);
  
  -- Solution 3: OPTIMIZE FOR specific value
  -- Use typical value's plan
  SELECT * FROM orders WHERE customer_id = @customer_id
  OPTION (OPTIMIZE FOR (@customer_id = 100));
  
  -- Solution 4: Local variable trick (breaks sniffing)
  DECLARE @local_id INT = @customer_id;
  SELECT * FROM orders WHERE customer_id = @local_id;
  
  -- ============================================================
  -- PATTERN 3: Statistics Management
  -- ============================================================
  
  -- PostgreSQL: Update statistics
  ANALYZE orders;
  ANALYZE customers;
  
  -- PostgreSQL: Detailed statistics check
  SELECT 
      attname,
      n_distinct,
      null_frac,
      most_common_vals,
      most_common_freqs
  FROM pg_stats
  WHERE tablename = 'orders';
  
  -- PostgreSQL: Extended statistics for correlated columns
  CREATE STATISTICS stat_city_state ON city, state FROM addresses;
  ANALYZE addresses;
  
  -- MySQL: Update statistics
  ANALYZE TABLE orders;
  
  -- MySQL: Check histogram
  SELECT * FROM information_schema.COLUMN_STATISTICS
  WHERE table_name = 'orders';
  
  -- SQL Server: Update statistics
  UPDATE STATISTICS orders;
  
  -- SQL Server: With full scan
  UPDATE STATISTICS orders WITH FULLSCAN;
  
  -- ============================================================
  -- PATTERN 4: Join Order Hints
  -- ============================================================
  
  /*
  Sometimes optimizer chooses wrong join order.
  Use hints to guide (carefully - hints can become stale).
  */
  
  -- PostgreSQL: Disable certain join types to force order
  SET enable_hashjoin = off;
  SET enable_mergejoin = off;
  -- Now nested loop only, may change order
  
  -- PostgreSQL: GEQO threshold
  SET geqo_threshold = 20;  -- Use genetic algorithm for 20+ tables
  
  -- MySQL: STRAIGHT_JOIN forces left-to-right
  SELECT STRAIGHT_JOIN * 
  FROM small_table
  JOIN large_table ON ...
  JOIN another_table ON ...;
  
  -- SQL Server: Join hint
  SELECT * FROM orders o
  INNER LOOP JOIN customers c ON o.customer_id = c.id;
  
  -- SQL Server: Force order
  SELECT * FROM orders o
  JOIN customers c ON o.customer_id = c.id
  OPTION (FORCE ORDER);
  
  -- ============================================================
  -- PATTERN 5: Query Rewriting
  -- ============================================================
  
  -- EXISTS vs IN (may have different plans)
  
  -- Original with IN
  SELECT * FROM customers
  WHERE id IN (SELECT customer_id FROM orders WHERE status = 'pending');
  
  -- Rewrite with EXISTS (often better for large subquery)
  SELECT * FROM customers c
  WHERE EXISTS (
      SELECT 1 FROM orders o 
      WHERE o.customer_id = c.id AND o.status = 'pending'
  );
  
  -- Rewrite with JOIN (explicit)
  SELECT DISTINCT c.*
  FROM customers c
  JOIN orders o ON c.id = o.customer_id
  WHERE o.status = 'pending';
  
  -- ============================================================
  -- PATTERN 6: CTE Optimization
  -- ============================================================
  
  -- PostgreSQL 12+: Control CTE materialization
  
  -- Force no materialization (allows predicate pushdown)
  WITH orders_cte AS NOT MATERIALIZED (
      SELECT * FROM orders WHERE status = 'pending'
  )
  SELECT * FROM orders_cte WHERE customer_id = 123;
  
  -- Force materialization (use if CTE referenced multiple times)
  WITH orders_cte AS MATERIALIZED (
      SELECT customer_id, COUNT(*) as order_count
      FROM orders
      GROUP BY customer_id
  )
  SELECT * FROM orders_cte o1
  JOIN orders_cte o2 ON o1.customer_id = o2.customer_id;
  
  -- ============================================================
  -- PATTERN 7: Parallel Query Tuning
  -- ============================================================
  
  -- PostgreSQL: Control parallelism
  SET max_parallel_workers_per_gather = 4;
  SET parallel_tuple_cost = 0.01;
  SET parallel_setup_cost = 1000;
  
  -- Force parallel
  SET force_parallel_mode = on;
  
  -- Disable parallel (if causing issues)
  SET max_parallel_workers_per_gather = 0;
  
  -- SQL Server: Control MAXDOP
  SELECT * FROM large_table
  OPTION (MAXDOP 4);  -- Use 4 threads
  
  SELECT * FROM small_table
  OPTION (MAXDOP 1);  -- Force serial
  
  -- ============================================================
  -- PATTERN 8: Plan Stability
  -- ============================================================
  
  -- SQL Server: Query Store plan forcing
  EXEC sp_query_store_force_plan 
      @query_id = 1234, 
      @plan_id = 5678;
  
  -- PostgreSQL: pg_hint_plan (extension)
  -- SELECT /*+ IndexScan(orders idx_customer) */ * FROM orders;
  
  -- Oracle: SQL Plan Baselines
  -- DBMS_SPM.LOAD_PLANS_FROM_CURSOR_CACHE
  
  -- ============================================================
  -- PATTERN 9: Monitoring Plan Changes
  -- ============================================================
  
  -- PostgreSQL: auto_explain extension
  LOAD 'auto_explain';
  SET auto_explain.log_min_duration = '1s';
  SET auto_explain.log_analyze = true;
  
  -- SQL Server: Query Store regressed plans
  SELECT 
      q.query_id,
      rs.avg_duration,
      p.query_plan
  FROM sys.query_store_query q
  JOIN sys.query_store_plan p ON q.query_id = p.query_id
  JOIN sys.query_store_runtime_stats rs ON p.plan_id = rs.plan_id
  WHERE rs.avg_duration > 10000000  -- > 10 seconds
  ORDER BY rs.avg_duration DESC;

# =============================================================================
# TEST CASES
# =============================================================================

fail_to_pass:
  - "test_parameter_sniffing_handled"
  - "test_statistics_current"
  - "test_join_order_optimal"
  - "test_no_unnecessary_sorts"
  - "test_cardinality_estimates_accurate"

pass_to_pass:
  - "test_basic_query_works"
  - "test_simple_index_lookup"

# =============================================================================
# VARIABLES FOR TASK GENERATION
# =============================================================================

variables:
  - name: application_type
    type: string
    options:
      - "OLTP application"
      - "reporting system"
      - "data warehouse"
      - "mixed workload"
  
  - name: database_system
    type: string
    options: ["PostgreSQL", "MySQL", "SQL Server", "Oracle"]
  
  - name: data_size
    type: string
    options: ["10 GB", "100 GB", "1 TB", "10 TB"]
  
  - name: concurrent_users
    type: int
    min: 10
    max: 1000

# =============================================================================
# ANTI-PATTERNS - LLM Failure Modes
# =============================================================================

anti_patterns:
  llm_failure_modes:
    - "Applying generic SQL patterns without considering database-specific behavior"
    - "Missing isolation level interactions between concurrent transactions"
    - "Ignoring lock manager implementation details"
    - "Not considering query optimizer behavior changes across versions"
    - "Missing hidden full table scans in seemingly optimized queries"
    - "Overlooking index maintenance overhead during writes"
    - "Assuming ORM generates efficient queries"
    - "Missing deadlock potential in cross-schema operations"
    - "Ignoring transaction log and recovery implications"
    - "Assuming estimated rows match actual rows without verification"
    - "Missing parameter sniffing as root cause of intermittent performance"
    - "Ignoring correlated column statistics impact on join estimation"
    - "Assuming CTE is always optimized away (materialization fence)"
    - "Overlooking GEQO randomness for large join queries"
    - "Missing implicit type conversion disabling index usage"
    - "Assuming parallel query always improves performance"
    - "Ignoring work_mem/sort_buffer impact on sort vs hash decisions"
    - "Missing subquery unnesting prerequisites"
    - "Assuming EXISTS and IN are always equivalent in optimization"
    - "Overlooking plan regression after ANALYZE/statistics update"
    - "Missing memory grant feedback impact on subsequent executions"
    - "Assuming FORCE ORDER hint is database-portable"
    - "Ignoring view expansion timing impact on optimization"
    - "Missing scalar subquery caching limitations"
    - "Assuming DISTINCT and GROUP BY optimize the same way"
    - "Overlooking semi-join/anti-join transformation prerequisites"
    - "Missing late materialization opportunities for complex expressions"
  
  query_optimizer_internals:
    optimization_phase_issues:
      - "Simplification rule ordering dependencies"
      - "Predicate pushdown blocking scenarios"
      - "View merge vs no-merge decision factors"
      - "Transformation rule applicability conditions"
    cost_model_calibration:
      - "SSD vs HDD cost assumption mismatch"
      - "Memory size assumption errors"
      - "CPU speed calibration staleness"
      - "Network cost for cloud deployments"
    plan_stability_factors:
      - "Statistics drift over time"
      - "Schema change plan invalidation"
      - "Hint staleness after evolution"
      - "Plan guide maintenance overhead"

# =============================================================================
# ANTI-HARDCODING MEASURES
# =============================================================================

anti_hardcoding:
  canary_tokens: true
  randomize_paths: true
  dynamic_content: true
  randomize_identifiers: true
  
  plan_issues:
    - "parameter_sniffing"
    - "stale_statistics"
    - "wrong_join_order"
    - "cardinality_misestimate"
    - "unnecessary_sort"
