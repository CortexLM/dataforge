id: "algo-dp-memoization-trap-001"
version: "2.0.0"
category: "algorithms"
subcategory: "dynamic-programming"

# =============================================================================
# LLM GENERATION FRAMEWORK
# =============================================================================
# This file is a comprehensive generation specification for LLMs to create
# unique, extremely challenging memoization and caching tasks. The goal is to
# enable generation of 10,000+ fundamentally different, genuinely hard tasks
# that require deep understanding of caching correctness.
# =============================================================================

generation_framework:
  overview: |
    This specification enables LLMs to generate memoization tasks that are
    genuinely difficult. Memoization looks straightforward but has numerous
    subtle pitfalls around cache invalidation, key collisions, mutable state,
    and memory leaks. Tasks should require understanding of caching invariants.

  multi_conversation_workflow:
    phase_1_research:
      description: "LLM researches obscure memoization failures"
      activities:
        - "Study cache coherence and invalidation problems"
        - "Research mutable default argument bugs"
        - "Investigate hash collision scenarios"
        - "Analyze memory leak patterns in caches"
        - "Study cross-call cache pollution"
        - "Research thread safety issues in memoization"
      output: "Comprehensive list of memoization traps"
    
    phase_2_creation:
      description: "LLM creates task with hidden caching flaws"
      activities:
        - "Design caches with subtle key collision issues"
        - "Create scenarios where mutable values corrupt cache"
        - "Embed cross-instance cache pollution"
        - "Add memory leaks that manifest over time"
        - "Include thread-unsafe memoization"
      output: "Complete task with hidden traps"
    
    phase_3_amplification:
      description: "LLM adds difficulty multipliers"
      activities:
        - "Layer multiple cache corruption issues"
        - "Add timing-dependent cache behavior"
        - "Include external state dependencies"
        - "Create scenarios where testing hides bugs"
        - "Add memory pressure scenarios"
      output: "Amplified task"
    
    phase_4_verification:
      description: "LLM validates task is genuinely hard"
      validation_criteria:
        - "Cannot be found by running code once"
        - "Requires understanding of cache semantics"
        - "Has at least 5 interacting hidden traps"
        - "Would take experienced developers 30+ minutes"
        - "Intermittent failures, not crashes"
        - "Has cascading failure modes that interact with each other"
        - "Requires multi-domain knowledge synthesis (algorithms + systems + performance)"
      output: "Verified task"

  quality_requirements:
    mandatory:
      - "Must require understanding of memoization semantics"
      - "Must have subtle failures, not crashes"
      - "Must include at least 5 interacting cache issues"
      - "Must manifest intermittently"
      - "Must resist simple testing"
      - "Must take 30+ minutes for experienced developers, 45+ for intermediate"
    
    difficulty_validation:
      - "Does running once reveal the bug? (must be NO)"
      - "Is the cache obviously wrong? (must be NO)"
      - "Do unit tests catch it? (must be NO)"
      - "Would this take 30+ minutes for experienced developers, 45+ for intermediate? (must be YES)"

# =============================================================================
# EXHAUSTIVE TOPIC UNIVERSE
# =============================================================================

topic_universe:
  # ---------------------------------------------------------------------------
  # Memoization Patterns (30 topics)
  # ---------------------------------------------------------------------------
  memoization_patterns:
    basic_memoization:
      - function_memoization
      - method_memoization
      - property_memoization
      - class_level_memoization
      - module_level_memoization
      - decorator_memoization
      - manual_cache_dict
      - lru_cache
      - ttl_cache
      - weak_ref_cache
    
    advanced_memoization:
      - recursive_memoization
      - mutual_recursion_memoization
      - generator_memoization
      - async_memoization
      - multi_argument_memoization
      - keyword_argument_memoization
      - variadic_memoization
      - default_argument_memoization
    
    cache_policies:
      - unbounded_cache
      - lru_eviction
      - lfu_eviction
      - fifo_eviction
      - ttl_expiration
      - size_bounded_cache
      - adaptive_replacement
      - clock_algorithm
      - random_eviction
      - two_queue_cache

  # ---------------------------------------------------------------------------
  # Dynamic Programming Topics (50 topics)
  # ---------------------------------------------------------------------------
  dp_algorithms:
    classic_dp:
      - fibonacci
      - factorial
      - binomial_coefficient
      - catalan_numbers
      - stirling_numbers
      - bell_numbers
      - partition_numbers
    
    sequence_dp:
      - longest_increasing_subsequence
      - longest_common_subsequence
      - longest_common_substring
      - edit_distance
      - longest_palindromic_subsequence
      - longest_palindromic_substring
      - longest_repeated_subsequence
      - shortest_common_supersequence
    
    optimization_dp:
      - knapsack_01
      - knapsack_unbounded
      - knapsack_bounded
      - coin_change_count
      - coin_change_minimum
      - rod_cutting
      - matrix_chain_multiplication
      - optimal_bst
      - egg_dropping
    
    grid_dp:
      - unique_paths
      - minimum_path_sum
      - maximal_square
      - maximal_rectangle
      - dungeon_game
      - cherry_pickup
      - paint_house
    
    tree_dp:
      - tree_diameter
      - tree_max_path_sum
      - house_robber_iii
      - tree_coloring
      - subtree_queries
    
    interval_dp:
      - palindrome_partitioning
      - burst_balloons
      - minimum_cost_to_merge_stones
      - boolean_parenthesization
      - scramble_string
    
    bitmask_dp:
      - traveling_salesman
      - assignment_problem
      - hamiltonian_path
      - subset_sum_bitmask
      - counting_tilings

  # ---------------------------------------------------------------------------
  # Cache Key Topics (20 topics)
  # ---------------------------------------------------------------------------
  cache_keys:
    key_types:
      - integer_keys
      - string_keys
      - tuple_keys
      - frozenset_keys
      - custom_hashable_keys
      - composite_keys
      - nested_keys
      - range_keys
      - partial_keys
      - normalized_keys
    
    key_issues:
      - mutable_key_modification
      - hash_collision
      - equal_but_not_identical
      - aliasing_issues
      - type_coercion_in_key
      - unhashable_types
      - key_normalization
      - canonical_form_keys
      - weak_key_reference
      - key_serialization

  # ---------------------------------------------------------------------------
  # Domain Contexts (25 topics)
  # ---------------------------------------------------------------------------
  domain_contexts:
    algorithmic:
      - pathfinding
      - graph_algorithms
      - string_matching
      - sequence_alignment
      - tree_traversal
      - game_theory
      - combinatorial_optimization
    
    applications:
      - web_request_caching
      - database_query_caching
      - api_response_caching
      - computation_caching
      - file_content_caching
      - compiled_regex_caching
      - config_parsing_caching
    
    machine_learning:
      - feature_computation
      - model_inference
      - embedding_lookup
      - distance_computation
      - kernel_evaluation
    
    system:
      - dns_caching
      - route_caching
      - permission_caching
      - session_caching
      - authentication_caching

# =============================================================================
# COMPLEXITY DIMENSIONS
# =============================================================================

complexity_dimensions:
  cache_correctness:
    key_correctness:
      - keys_must_be_hashable
      - keys_must_be_immutable
      - equal_keys_must_hash_equal
      - hash_must_be_stable
    
    value_correctness:
      - values_must_not_be_mutated
      - values_must_be_independent
      - values_must_be_consistent
      - values_must_be_complete
    
    invalidation_correctness:
      - stale_data_detection
      - dependency_tracking
      - cascade_invalidation
      - ttl_accuracy

  cache_safety:
    thread_safety:
      - read_write_races
      - cache_update_races
      - eviction_races
      - initialization_races
    
    memory_safety:
      - unbounded_growth
      - memory_leaks
      - circular_references
      - large_value_accumulation
    
    type_safety:
      - type_confusion
      - coercion_issues
      - polymorphic_keys
      - generic_type_erasure

  cache_performance:
    hit_rate:
      - cache_miss_cost
      - cold_start_penalty
      - key_distribution
      - access_patterns
    
    memory_usage:
      - cache_size_bounds
      - entry_size_variation
      - overhead_per_entry
      - fragmentation

# =============================================================================
# TRAP TAXONOMY
# =============================================================================

trap_taxonomy:
  # ---------------------------------------------------------------------------
  # Mutable Default Argument Traps (6 types)
  # ---------------------------------------------------------------------------
  mutable_default:
    dict_default:
      trap_id: "MD001"
      name: "Mutable Dict Default Argument"
      description: "Cache dict as default argument persists"
      example: |
        def memoized_func(x, cache={}):
            if x in cache:
                return cache[x]
            result = expensive_compute(x)
            cache[x] = result
            return result
        # Cache persists between calls!
      detection_difficulty: "medium"
      manifestation: "Old results returned"
      
    list_default:
      trap_id: "MD002"
      name: "Mutable List Default Accumulation"
      description: "Results accumulate in default list"
      example: |
        def collect_paths(node, paths=[]):
            paths.append(node)
            for child in node.children:
                collect_paths(child, paths)
            return paths
        # paths accumulates across calls
      detection_difficulty: "medium"
      manifestation: "Results from previous calls"
      
    set_default:
      trap_id: "MD003"
      name: "Mutable Set Default"
      description: "Visited set persists between calls"
      example: |
        def traverse(graph, node, visited=set()):
            if node in visited:
                return
            visited.add(node)
            # ...
        # visited persists
      detection_difficulty: "medium"
      manifestation: "Nodes appear already visited"
      
    nested_default:
      trap_id: "MD004"
      name: "Nested Mutable Default"
      description: "Default contains mutable objects"
      example: |
        def process(config={'options': []}):
            config['options'].append(new_option)
            # Modifies default!
      detection_difficulty: "hard"
      manifestation: "Config accumulates options"
      
    class_attribute_default:
      trap_id: "MD005"
      name: "Class Attribute as Implicit Default"
      description: "Class attribute used as cache"
      example: |
        class Computer:
            cache = {}  # Shared across instances!
            def compute(self, x):
                if x in self.cache:
                    return self.cache[x]
      detection_difficulty: "medium"
      manifestation: "Cross-instance cache pollution"
      
    closure_capture:
      trap_id: "MD006"
      name: "Closure Captures Mutable"
      description: "Closure captures mutable cache"
      example: |
        def make_memoized():
            cache = {}
            def inner(x):
                if x not in cache:
                    cache[x] = compute(x)
                return cache[x]
            return inner
        # Multiple calls to make_memoized share...?
      detection_difficulty: "hard"
      manifestation: "Depends on closure behavior"

  # ---------------------------------------------------------------------------
  # Cache Key Collision Traps (8 types)
  # ---------------------------------------------------------------------------
  key_collision:
    hash_collision:
      trap_id: "KC001"
      name: "Hash Collision"
      description: "Different inputs produce same hash"
      example: |
        # In Python, hash(-1) == hash(-2)
        cache = {}
        cache[-1] = compute(-1)
        cache[-2] = compute(-2)  # Overwrites -1!
        # Actually, Python handles this, but custom hashes might not
      detection_difficulty: "hard"
      manifestation: "Wrong cached result"
      
    tuple_unhashable:
      trap_id: "KC002"
      name: "Tuple with Unhashable Element"
      description: "Cache key contains unhashable nested element"
      example: |
        def memoized(data):
            key = tuple(data)  # Fails if data contains lists
            if key in cache:
                return cache[key]
      detection_difficulty: "medium"
      manifestation: "TypeError on some inputs"
      
    float_nan_key:
      trap_id: "KC003"
      name: "NaN as Cache Key"
      description: "NaN != NaN breaks cache lookup"
      example: |
        cache = {}
        cache[float('nan')] = 42
        cache[float('nan')]  # KeyError! NaN != NaN
      detection_difficulty: "hard"
      manifestation: "Cache miss for NaN keys"
      
    mutable_key_modification:
      trap_id: "KC004"
      name: "Mutable Key Modified After Insertion"
      description: "Key modified after caching"
      example: |
        key = [1, 2, 3]
        cache[tuple(key)] = result
        key.append(4)  # Original key object modified
        # But tuple was snapshot, so cache still works
        # UNLESS key is a mutable object used directly
      detection_difficulty: "hard"
      manifestation: "Key lookup fails"
      
    type_confusion_key:
      trap_id: "KC005"
      name: "Type Confusion in Key"
      description: "1 vs '1' vs 1.0 as keys"
      example: |
        cache = {}
        cache[1] = 'integer'
        cache['1'] = 'string'
        cache[1.0] = 'float'  # In Python, 1 == 1.0, so overwrites!
      detection_difficulty: "medium"
      manifestation: "Integer and float keys collide"
      
    none_key_confusion:
      trap_id: "KC006"
      name: "None as Cache Key"
      description: "None key vs missing key confusion"
      example: |
        cache = {}
        cache[None] = compute(None)
        # Later checking:
        if cache.get(x) is None:  # Is it missing or cached None?
      detection_difficulty: "medium"
      manifestation: "None result vs cache miss"
      
    recursive_structure_key:
      trap_id: "KC007"
      name: "Recursive Structure as Key"
      description: "Self-referential structure breaks hashing"
      example: |
        lst = []
        lst.append(lst)  # Self-referential
        # tuple(lst) or hash(lst) fails
      detection_difficulty: "medium"
      manifestation: "Infinite recursion or error"
      
    serialization_key:
      trap_id: "KC008"
      name: "Serialization-Based Key"
      description: "Serializing to string for key is fragile"
      example: |
        def key_for(obj):
            return str(obj)  # Depends on __str__
        # Different objects might have same str()
        # Or same object might have different str()
      detection_difficulty: "hard"
      manifestation: "Collisions or misses"

  # ---------------------------------------------------------------------------
  # Mutable Value Corruption Traps (8 types)
  # ---------------------------------------------------------------------------
  mutable_value:
    list_modification:
      trap_id: "MV001"
      name: "Cached List Modified"
      description: "Returned list is modified by caller"
      example: |
        @lru_cache
        def get_neighbors(node):
            return [n for n in node.neighbors]
        
        neighbors = get_neighbors(node)
        neighbors.append(extra)  # Modifies cached list!
      detection_difficulty: "hard"
      manifestation: "Cached list grows over time"
      
    dict_modification:
      trap_id: "MV002"
      name: "Cached Dict Modified"
      description: "Returned dict is modified"
      example: |
        @cache
        def get_config():
            return {'key': 'value'}
        
        config = get_config()
        config['new_key'] = 'new_value'  # Modifies cache!
      detection_difficulty: "hard"
      manifestation: "Config accumulates entries"
      
    nested_mutation:
      trap_id: "MV003"
      name: "Nested Mutable Modified"
      description: "Nested mutable in cached value modified"
      example: |
        @cache
        def get_data():
            return {'items': []}
        
        get_data()['items'].append(x)  # Modifies nested list!
      detection_difficulty: "very_hard"
      manifestation: "Deep mutations accumulate"
      
    object_attribute_modification:
      trap_id: "MV004"
      name: "Cached Object Attribute Modified"
      description: "Cached object's attributes modified"
      example: |
        @cache
        def get_user(id):
            return User(id)
        
        user = get_user(1)
        user.name = "Modified"  # Modifies cached user!
      detection_difficulty: "hard"
      manifestation: "Cached objects have wrong attributes"
      
    shared_reference:
      trap_id: "MV005"
      name: "Shared Reference in Cache"
      description: "Multiple cache entries share reference"
      example: |
        default = []
        @cache
        def get_list(key):
            return default  # All entries share same list!
      detection_difficulty: "hard"
      manifestation: "All cache entries identical"
      
    accumulator_mutation:
      trap_id: "MV006"
      name: "Accumulator Mutated"
      description: "DP accumulator modified after caching"
      example: |
        def dp(i, j, memo={}):
            if (i, j) in memo:
                return memo[(i, j)]
            result = []  # Accumulate into list
            # ... computation adds to result ...
            memo[(i, j)] = result
            return result
        # Later code modifies returned result
      detection_difficulty: "hard"
      manifestation: "DP results corrupted"
      
    generator_state:
      trap_id: "MV007"
      name: "Generator State Cached"
      description: "Generator cached but exhausted"
      example: |
        @cache
        def get_items():
            return (x for x in range(10))
        
        list(get_items())  # Exhausts generator
        list(get_items())  # Empty! Generator was cached
      detection_difficulty: "medium"
      manifestation: "Second call returns empty"
      
    partial_result:
      trap_id: "MV008"
      name: "Partial Result Cached"
      description: "Incomplete result cached"
      example: |
        def compute(x, memo={}):
            if x in memo:
                return memo[x]
            memo[x] = []  # Cache before computing
            memo[x].extend(expensive_compute(x))
            return memo[x]
        # If exception during compute, partial result cached
      detection_difficulty: "very_hard"
      manifestation: "Partial results returned"

  # ---------------------------------------------------------------------------
  # Cache Invalidation Traps (6 types)
  # ---------------------------------------------------------------------------
  cache_invalidation:
    stale_dependency:
      trap_id: "CI001"
      name: "Stale Dependency"
      description: "Cache depends on changed external data"
      example: |
        @cache
        def get_exchange_rate(currency):
            return fetch_rate(currency)
        # Rate changes but cache doesn't invalidate
      detection_difficulty: "medium"
      manifestation: "Stale exchange rates"
      
    config_change:
      trap_id: "CI002"
      name: "Configuration Change Ignored"
      description: "Config changes but cache not cleared"
      example: |
        @cache
        def compute(x):
            return x * CONFIG['multiplier']
        # CONFIG changes but compute returns old results
      detection_difficulty: "medium"
      manifestation: "Config changes ignored"
      
    file_modification:
      trap_id: "CI003"
      name: "File Modification Missed"
      description: "Cached file contents while file changes"
      example: |
        @cache
        def read_file(path):
            return open(path).read()
        # File modified but cache returns old contents
      detection_difficulty: "easy"
      manifestation: "Stale file contents"
      
    time_based_staleness:
      trap_id: "CI004"
      name: "Time-Based Value Stale"
      description: "Time-dependent value cached too long"
      example: |
        @cache
        def get_current_user():
            return lookup_user(get_session_token())
        # User logs out/in but old user cached
      detection_difficulty: "medium"
      manifestation: "Wrong user returned"
      
    cascade_invalidation:
      trap_id: "CI005"
      name: "Cascade Invalidation Missing"
      description: "Dependent caches not invalidated"
      example: |
        @cache
        def get_user(id): ...
        
        @cache
        def get_user_posts(user):
            user = get_user(user.id)  # Uses cached user
            return [p for p in posts if p.author == user]
        # User updated but posts cache still uses old user
      detection_difficulty: "hard"
      manifestation: "Inconsistent cached data"
      
    partial_invalidation:
      trap_id: "CI006"
      name: "Partial Invalidation"
      description: "Only some related entries invalidated"
      example: |
        # Cache entries for user_1, user_2, ...
        # Clear cache for user_1 but not user_1_posts, user_1_comments
      detection_difficulty: "hard"
      manifestation: "Inconsistent related data"

  # ---------------------------------------------------------------------------
  # Memory Leak Traps (6 types)
  # ---------------------------------------------------------------------------
  memory_leaks:
    unbounded_cache:
      trap_id: "ML001"
      name: "Unbounded Cache Growth"
      description: "Cache grows without limit"
      example: |
        cache = {}
        def memoized(x):
            if x not in cache:
                cache[x] = compute(x)
            return cache[x]
        # Called with millions of different x values
      detection_difficulty: "easy"
      manifestation: "Memory exhaustion"
      
    circular_reference:
      trap_id: "ML002"
      name: "Circular Reference in Cache"
      description: "Cached objects reference each other"
      example: |
        class Node:
            def __init__(self):
                self.neighbors = []
        
        @cache
        def get_graph():
            n1, n2 = Node(), Node()
            n1.neighbors.append(n2)
            n2.neighbors.append(n1)  # Circular!
            return n1
      detection_difficulty: "hard"
      manifestation: "GC can't collect"
      
    closure_leak:
      trap_id: "ML003"
      name: "Closure Captures Large Object"
      description: "Cached closure holds large reference"
      example: |
        def make_processor(large_data):
            @cache
            def process(x):
                return x in large_data  # Captures large_data
            return process
        # large_data never freed
      detection_difficulty: "hard"
      manifestation: "Large objects retained"
      
    key_retains_value:
      trap_id: "ML004"
      name: "Key Retains Large Value"
      description: "Cache key prevents garbage collection"
      example: |
        cache = {}
        def memoize(large_obj):
            key = id(large_obj)
            cache[key] = compute(large_obj)
            # large_obj could be freed but we have its id
            # Actually, id could be reused, causing bugs
      detection_difficulty: "hard"
      manifestation: "Memory fragmentation or bugs"
      
    exception_handler_leak:
      trap_id: "ML005"
      name: "Exception Handler Retains Cache"
      description: "Exception traceback holds cache reference"
      example: |
        try:
            result = memoized_func(x)
        except Exception as e:
            # e.__traceback__ holds reference to cache
            save_exception(e)
      detection_difficulty: "very_hard"
      manifestation: "Cache not freed after exception"
      
    thread_local_leak:
      trap_id: "ML006"
      name: "Thread Local Cache Leak"
      description: "Thread-local cache not cleaned up"
      example: |
        import threading
        local = threading.local()
        
        def memoized(x):
            if not hasattr(local, 'cache'):
                local.cache = {}
            # Thread dies but cache remains in local
      detection_difficulty: "hard"
      manifestation: "Thread pool memory growth"

  # ---------------------------------------------------------------------------
  # Thread Safety Traps (6 types)
  # ---------------------------------------------------------------------------
  thread_safety:
    race_condition:
      trap_id: "TS001"
      name: "Cache Access Race Condition"
      description: "Concurrent read/write to cache"
      example: |
        cache = {}
        def memoized(x):
            if x not in cache:  # Thread 1 checks
                # Thread 2 also checks, both miss
                cache[x] = compute(x)  # Both compute
            return cache[x]
      detection_difficulty: "hard"
      manifestation: "Duplicate computation, possible corruption"
      
    double_computation:
      trap_id: "TS002"
      name: "Double Computation Race"
      description: "Same value computed twice concurrently"
      example: |
        # Two threads call memoized(42) simultaneously
        # Both see cache miss, both compute
        # Wasted work (and possibly inconsistent if non-deterministic)
      detection_difficulty: "medium"
      manifestation: "Wasted computation"
      
    cache_corruption:
      trap_id: "TS003"
      name: "Cache Corruption During Resize"
      description: "Dict resize during concurrent access"
      example: |
        # Python dict resize is atomic but iteration isn't
        # Concurrent modification during iteration = undefined
      detection_difficulty: "very_hard"
      manifestation: "KeyError or corruption"
      
    partial_write:
      trap_id: "TS004"
      name: "Partial Write Visibility"
      description: "Partially constructed value visible"
      example: |
        def memoized(x):
            if x not in cache:
                result = SomeClass()  # Partially constructed
                cache[x] = result  # Other thread sees partial
                result.initialize()  # Completed
            return cache[x]
      detection_difficulty: "very_hard"
      manifestation: "Partial object returned"
      
    initialization_race:
      trap_id: "TS005"
      name: "Initialization Race"
      description: "Cache initialized multiple times"
      example: |
        _cache = None
        def get_cache():
            global _cache
            if _cache is None:  # Thread 1 checks
                # Thread 2 also checks
                _cache = {}  # Both create
            return _cache  # Might be different dicts!
      detection_difficulty: "hard"
      manifestation: "Multiple cache instances"
      
    eviction_race:
      trap_id: "TS006"
      name: "Eviction During Access"
      description: "Entry evicted while being accessed"
      example: |
        # Thread 1: reads cache[x]
        # Thread 2: evicts x due to size limit
        # Thread 1: accesses evicted data
      detection_difficulty: "very_hard"
      manifestation: "Dangling reference or KeyError"

# =============================================================================
# EDGE CASE CATALOG
# =============================================================================

edge_cases:
  input_edge_cases:
    - none_input
    - empty_collection_input
    - single_element_input
    - very_large_input
    - recursive_input
    - self_referential_input
    - nan_input
    - infinity_input
    - negative_zero_input
    - unicode_input
    - binary_input

  cache_state_edge_cases:
    - empty_cache
    - single_entry_cache
    - full_cache_at_limit
    - cache_just_evicted
    - cache_being_cleared
    - cache_being_resized
    - cache_with_expired_entries
    - cache_with_invalid_entries

  concurrency_edge_cases:
    - single_thread
    - two_threads_same_key
    - many_threads_same_key
    - many_threads_different_keys
    - thread_during_eviction
    - thread_during_clear
    - thread_during_resize

  memory_edge_cases:
    - low_memory_condition
    - gc_during_access
    - weak_reference_collected
    - large_value_allocation_failure

  timing_edge_cases:
    - first_access
    - immediate_reaccess
    - access_after_ttl
    - access_just_before_ttl
    - access_during_computation

# =============================================================================
# ANTI-PATTERNS
# =============================================================================

anti_patterns:
  llm_failure_modes:
    - "Pattern matching on similar-looking problems without understanding cache semantics"
    - "Applying textbook solutions without considering thread safety"
    - "Missing subtle complexity hidden in library memoization decorators"
    - "Ignoring cache invalidation requirements in stateful systems"
    - "Assuming all functions are safe to memoize without purity analysis"
    - "Skipping verification of hash key uniqueness and collision handling"
    - "Over-relying on @lru_cache without understanding its limitations"
    - "Missing memory leaks from closure captures in cached functions"
    - "Ignoring reference vs value semantics in cached mutable objects"
    - "Failing to consider cache coherence in multi-threaded contexts"
    - "Not recognizing when cache invalidation is necessary"
    - "Confusing referential transparency with determinism"
    - "Missing weak reference opportunities for memory-efficient caching"
    - "Ignoring serialization requirements for distributed caching"
    - "Failing to implement proper cache warming strategies"
    - "Not considering cache eviction policy implications on hit rate"
    - "Missing opportunities for hierarchical caching"
    - "Overlooking time-based invalidation requirements"
    - "Failing to handle exception propagation through cache"
    - "Not implementing proper metrics for cache effectiveness"
    - "Ignoring cache stampede/thundering herd problems"
    - "Missing cache key normalization requirements"
    - "Failing to consider memory pressure and GC interactions"
    - "Not implementing graceful degradation when cache fails"
    - "Overlooking security implications of cached data"

  task_anti_patterns:
    obvious_mutable_default:
      description: "Obviously wrong def f(x, cache={}):"
      why_bad: "Well-known pattern"
      
    immediate_failure:
      description: "Fails on first call"
      why_bad: "No subtlety"
      
    error_message_reveals:
      description: "Exception shows the bug"
      why_bad: "Just read the error"

  code_anti_patterns:
    single_bug:
      description: "Only one caching issue"
      why_bad: "Too easy once found"
      
    no_working_cases:
      description: "Never works correctly"
      why_bad: "No intermittent debugging"

# =============================================================================
# DIFFICULTY AMPLIFIERS
# =============================================================================

difficulty_amplifiers:
  nightmare:
    multiplier: 3.0
    description: "Extreme difficulty requiring expert-level multi-domain synthesis"
    requirements:
      - "7+ interacting traps across multiple domains"
      - "Requires understanding of hardware-level effects"
      - "Time estimate: 90+ minutes for senior engineers"
      - "Multiple red herrings that waste investigation time"
      - "Solution requires synthesizing knowledge from 3+ distinct areas"

  nightmare_plus:
    multiplier: 5.0
    estimated_time: [28800, 172800]  # 8-48 hours
    command_steps: [300, 1200]
    techniques_required: 12
    description: "Research paper difficulty requiring novel algorithm design or analysis"
    requirements:
      - "10+ deeply interacting traps across correctness/performance/edge-case domains"
      - "Requires original insights into cache coherence and memory consistency"
      - "Must synthesize knowledge from algorithms, systems, concurrency, and formal methods"
      - "Requires formal proof of cache correctness under all interleavings"
      - "Time estimate: 8-48 hours for competitive programmers with ICPC/IOI medal experience"
      - "Multiple cache invalidation scenarios that interact non-obviously"
      - "Must handle concurrent access patterns and memory ordering"
      - "Requires understanding of weak memory models"
      - "Solution involves novel data structure design for thread-safe memoization"

# =============================================================================
# MULTI-AGENT ORCHESTRATION COMPLEXITY
# =============================================================================

multi_agent_orchestration:
  description: "Coordinating 4-7 specialized agents for complex memoization analysis"
  
  specialized_agents:
    cache_coherence_analyst:
      role: "Analyze cache consistency and invalidation"
      capabilities:
        - "Identify stale cache scenarios"
        - "Detect cache pollution patterns"
        - "Analyze cache key collision probability"
        - "Memory consistency model analysis"
      handoff_triggers:
        - "Intermittent incorrect results suspected"
        - "Multi-threaded cache access"
    
    memory_leak_hunter:
      role: "Find memory leaks in caching systems"
      capabilities:
        - "Unbounded cache growth detection"
        - "Circular reference identification"
        - "Closure capture analysis"
        - "Thread-local leak detection"
      handoff_triggers:
        - "Memory usage grows over time"
        - "Long-running process degradation"
    
    concurrency_specialist:
      role: "Analyze thread safety of memoization"
      capabilities:
        - "Race condition detection"
        - "Double-checked locking analysis"
        - "Lock-free data structure verification"
        - "Memory barrier placement"
      handoff_triggers:
        - "Multi-threaded access to cache"
        - "Non-deterministic behavior"
    
    correctness_prover:
      role: "Formally verify memoization correctness"
      capabilities:
        - "Function purity verification"
        - "Key uniqueness proofs"
        - "Referential transparency analysis"
        - "Side-effect detection"
      handoff_triggers:
        - "Subtle correctness issues suspected"
        - "Complex key generation logic"
    
    performance_optimizer:
      role: "Optimize cache hit rates and performance"
      capabilities:
        - "Cache eviction policy analysis"
        - "Working set estimation"
        - "Temporal locality optimization"
        - "Cache warming strategies"
      handoff_triggers:
        - "Poor cache hit rates"
        - "Performance not meeting expectations"

  cross_algorithm_attack_chains:
    chain_1:
      name: "Mutable Default → Cache Pollution → Wrong Results → Silent Data Corruption"
      stages:
        - "Cache uses mutable default argument"
        - "Previous call's data persists in cache"
        - "New call gets polluted cached value"
        - "Computation proceeds with wrong data"
    
    chain_2:
      name: "Hash Collision → Cache Overwrite → Lost Computation → Incorrect Output"
      stages:
        - "Two different inputs hash to same key"
        - "Second computation overwrites first"
        - "Later lookup returns wrong cached value"
        - "Final result is incorrect"
    
    chain_3:
      name: "Thread Race → Partial Cache Entry → Read of Incomplete Data → Crash/Corruption"
      stages:
        - "Thread A starts populating cache entry"
        - "Thread B reads partially written entry"
        - "Thread B uses incomplete/corrupt data"
        - "System crash or incorrect computation"

# =============================================================================
# THEORETICAL COMPLEXITY REQUIREMENTS
# =============================================================================

theoretical_complexity_requirements:
  amortized_analysis:
    techniques_required:
      - "Aggregate method for cache operation sequences"
      - "Accounting method for cache eviction costs"
      - "Potential method for LRU cache analysis"
    application_scenarios:
      - "LRU cache with varying access patterns"
      - "Cache with lazy cleanup"
      - "Hierarchical caching systems"

  competitive_analysis:
    concepts_required:
      - "Online caching algorithms vs optimal offline"
      - "LRU competitive ratio proofs"
      - "Working set model analysis"
    application_scenarios:
      - "Memory-constrained memoization"
      - "Multi-level cache hierarchies"

  cache_complexity_model:
    concepts:
      - "Cache-oblivious algorithm analysis"
      - "External memory model"
      - "I/O complexity bounds"
    techniques:
      - "Block transfer counting"
      - "Tall cache assumption"
      - "Optimal cache replacement"

# =============================================================================
# ADVERSARIAL INPUT DESIGN
# =============================================================================

adversarial_input_design:
  worst_case_input_generation:
    cache_thrashing:
      description: "Access pattern that defeats cache"
      technique: "Cyclic access exceeding cache size"
      
    hash_collision_attack:
      description: "Keys designed to collide"
      technique: "Exploit hash function weaknesses"
      
    memory_exhaustion:
      description: "Input causing unbounded cache growth"
      technique: "Large number of distinct keys"

  anti_optimization_inputs:
    cold_cache:
      description: "Input requiring full computation"
      technique: "All cache misses"
    
    temporal_anti_locality:
      description: "Access pattern defeating LRU"
      technique: "Working set size = cache size + 1"

  numerical_stability_attacks:
    floating_point_keys:
      description: "Keys with precision issues"
      technique: "0.1 + 0.2 != 0.3 scenarios"

# =============================================================================
# FORMAL PROOF REQUIREMENTS
# =============================================================================

formal_proof_requirements:
  correctness_proofs:
    referential_transparency:
      requirements:
        - "Prove function is pure (no side effects)"
        - "Prove key uniquely identifies inputs"
        - "Prove cached value equals recomputed value"
      
    thread_safety:
      requirements:
        - "Prove linearizability of cache operations"
        - "Prove absence of data races"
        - "Prove memory ordering correctness"
      
    cache_coherence:
      requirements:
        - "Prove cached values never stale"
        - "Prove invalidation completeness"
        - "Prove consistency under concurrent access"

  code_structure:
    decorator_chain:
      description: "Multiple decorators including cache"
      amplification: "Must trace through decorators"
      
    inheritance_cache:
      description: "Cache in base class, bug in subclass"
      amplification: "Must understand inheritance"
      
    factory_pattern:
      description: "Cache created by factory"
      amplification: "Must trace factory"
      
    dependency_injection:
      description: "Cache injected as dependency"
      amplification: "Must trace injection"

  analysis_amplifiers:
    intermittent:
      description: "Only fails sometimes"
      amplification: "Must find trigger"
      
    sequence_dependent:
      description: "Depends on call sequence"
      amplification: "Must understand history"
      
    data_dependent:
      description: "Depends on specific data patterns"
      amplification: "Must find triggering data"

  context_amplifiers:
    realistic_codebase:
      description: "Embedded in real code"
      amplification: "Must filter noise"
      
    working_tests:
      description: "Tests pass but bug exists"
      amplification: "Tests give false confidence"

# =============================================================================
# VARIATION ENGINES
# =============================================================================

variation_engines:
  problem_type_variations:
    identify_bug:
      description: "Find the memoization bug"
      
    fix_bug:
      description: "Correct the memoization"
      
    design_cache:
      description: "Design cache with constraints"
      
    analyze_memory:
      description: "Analyze memory behavior"

  domain_variations:
    pathfinding:
      context: "Graph path memoization"
      
    api_caching:
      context: "API response caching"
      
    computation:
      context: "Expensive computation caching"
      
    config:
      context: "Configuration caching"

  language_variations:
    python:
      features: ["@lru_cache", "dict default", "__hash__"]
      
    java:
      features: ["HashMap", "ConcurrentHashMap", "WeakHashMap"]
      
    javascript:
      features: ["Map", "WeakMap", "closure"]
      
    cpp:
      features: ["std::unordered_map", "static variables"]

# =============================================================================
# SWE-bench_Pro style fields
# =============================================================================

problem_statement: |
  A memoized recursive function for computing optimal paths is returning incorrect 
  results intermittently. The function works correctly on small inputs but fails 
  on larger ones or when run multiple times in succession.
  
  Hidden issues:
  1. The memoization cache persists between calls with different parameters
  2. Mutable default arguments are used as cache storage
  3. Hash collisions in the cache key generation
  4. Cache entries are modified after storage (mutable values)

requirements: |
  - Identify and fix cache corruption issues
  - Implement proper cache isolation between calls
  - Handle hash collisions in cache keys
  - Ensure cached values are not mutated
  - Maintain O(n) memoization benefits

interface: |
  Input: Grid of costs (2D array), start position, end position
  Output: Minimum cost path and the path itself
  Constraint: Cache must be properly isolated

# terminal-bench style fields
difficulty:
  estimated: "nightmare_plus"
  time_range: [5400, 18000]  # 90-300 minutes for competitive programmers with ICPC/IOI medal experience
  command_steps: [60, 200]
  techniques_required: 12
  trap_count: "10+ deeply interacting traps across correctness/performance/edge-case domains"
  target_audience: "Competitive programmers with ICPC/IOI medal experience"

# LLM trap configurations
traps:
  - type: "mutable_default"
    description: "Cache dict is a mutable default argument, persists across calls"
    trigger: "Using def func(cache={}) pattern"
  
  - type: "global_cache_pollution"
    description: "Module-level cache not cleared between different problem instances"
    trigger: "Using global cache without invalidation"
  
  - type: "mutable_cache_values"
    description: "Cached lists/dicts are modified after retrieval"
    trigger: "Not deep copying cached mutable objects"
  
  - type: "hash_collision"
    description: "Different inputs produce same cache key"
    trigger: "Using weak hashing for tuple keys"

  - type: "thread_race_condition"
    description: "Multiple threads read/write cache simultaneously causing corruption"
    trigger: "Using non-thread-safe dict in multi-threaded context"

  - type: "closure_memory_leak"
    description: "Cached closures retain references to large objects preventing GC"
    trigger: "Memoizing functions that capture large scope variables"

  - type: "nan_key_failure"
    description: "NaN as cache key always causes cache miss since NaN != NaN"
    trigger: "Using floating point values that may be NaN as cache keys"

  - type: "weakref_premature_collection"
    description: "Weakly-referenced cache entries collected while still needed"
    trigger: "Using WeakValueDictionary without maintaining strong references"

  - type: "recursive_cache_deadlock"
    description: "Recursive memoized function deadlocks on re-entrant cache access"
    trigger: "Using locked cache in recursive calls"

  - type: "exception_partial_cache"
    description: "Exception during computation leaves partial result in cache"
    trigger: "Caching value before computation completes"

  - type: "time_based_staleness"
    description: "Cached values become stale when underlying data changes"
    trigger: "Missing cache invalidation on data updates"

# Task generation template
instruction_template: |
  You are debugging a {{ scenario_type }} optimization algorithm.
  The code is at {{ path }}.
  
  The algorithm returns wrong results intermittently. Your task:
  {{ task_steps }}
  
  Grid size: {{ grid_rows }}x{{ grid_cols }}
  Test runs to verify: {{ test_runs }}

# Reference solution
reference_solution: |
  #!/usr/bin/env python3
  from functools import lru_cache
  from copy import deepcopy
  from typing import List, Tuple, Optional
  import hashlib
  
  def make_cache_key(*args):
      key_str = repr(args)
      return hashlib.sha256(key_str.encode()).hexdigest()[:16]
  
  class PathFinder:
      def __init__(self, grid: List[List[int]]):
          self.grid = grid
          self.rows = len(grid)
          self.cols = len(grid[0]) if grid else 0
          self._cache = {}
      
      def clear_cache(self):
          self._cache.clear()
      
      def find_min_cost_path(
          self, 
          start: Tuple[int, int], 
          end: Tuple[int, int]
      ) -> Tuple[int, List[Tuple[int, int]]]:
          def helper(pos: Tuple[int, int]) -> Tuple[int, List[Tuple[int, int]]]:
              if pos == end:
                  return self.grid[pos[0]][pos[1]], [pos]
              
              cache_key = pos
              if cache_key in self._cache:
                  cached = self._cache[cache_key]
                  return cached[0], deepcopy(cached[1])
              
              r, c = pos
              if r < 0 or r >= self.rows or c < 0 or c >= self.cols:
                  return float('inf'), []
              
              current_cost = self.grid[r][c]
              neighbors = [(r, c+1), (r+1, c)]
              best_cost = float('inf')
              best_path = []
              
              for nr, nc in neighbors:
                  if 0 <= nr < self.rows and 0 <= nc < self.cols:
                      sub_cost, sub_path = helper((nr, nc))
                      if sub_cost < best_cost:
                          best_cost = sub_cost
                          best_path = sub_path
              
              if best_cost < float('inf'):
                  result_cost = current_cost + best_cost
                  result_path = [pos] + best_path
                  self._cache[cache_key] = (result_cost, deepcopy(result_path))
                  return result_cost, result_path
              
              return float('inf'), []
          
          return helper(start)

# Test cases
fail_to_pass:
  - "test_repeated_calls_different_grids"
  - "test_cache_isolation"
  - "test_mutable_path_not_corrupted"
  - "test_concurrent_calls"

pass_to_pass:
  - "test_basic_path_finding"
  - "test_single_cell_grid"
  - "test_no_path_exists"

# Variables for task generation
variables:
  - name: scenario_type
    type: string
    options: ["route optimization", "resource allocation", "game AI pathfinding", "network routing"]
  - name: path
    type: path
    generator: random_path
  - name: grid_rows
    type: int
    min: 10
    max: 100
  - name: grid_cols
    type: int
    min: 10
    max: 100
  - name: test_runs
    type: int
    min: 5
    max: 20
  - name: task_steps
    type: template
    value: |
      1. Identify cache-related bugs in the code
      2. Fix mutable default argument issues
      3. Implement proper cache isolation
      4. Ensure cached values are immutable or copied
      5. Verify with multiple consecutive runs

# Anti-hardcoding measures
anti_hardcoding:
  canary_tokens: true
  randomize_paths: true
  dynamic_content: true
  cache_corruption_types:
    - mutable_default_args
    - global_cache_pollution
    - mutable_value_mutation
    - hash_collisions

# =============================================================================
# GENERATION TARGETS
# =============================================================================

generation_targets:
  unique_tasks: 10000
  minimum_difficulty: "experienced developer needs 30+ minutes, requires deep domain expertise"
  trap_coverage: "all trap types used at least 100 times"
