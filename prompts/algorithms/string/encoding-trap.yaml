id: "algo-string-encoding-trap-001"
version: "2.0.0"
category: "algorithms"
subcategory: "string"

# =============================================================================
# LLM GENERATION FRAMEWORK
# =============================================================================
# This file is a comprehensive generation specification for LLMs to create
# unique, extremely challenging string encoding and Unicode tasks. The goal is
# to enable generation of 10,000+ fundamentally different, genuinely hard tasks
# that require deep understanding of text encoding.
# =============================================================================

generation_framework:
  overview: |
    This specification enables LLMs to generate string encoding tasks that are
    genuinely difficult. Unicode and text encoding have numerous subtle pitfalls
    around normalization, combining characters, zero-width characters, and
    encoding mismatches. Tasks should require deep understanding of Unicode.

  multi_conversation_workflow:
    phase_1_research:
      description: "LLM researches obscure encoding edge cases"
      activities:
        - "Study Unicode normalization forms (NFC, NFD, NFKC, NFKD)"
        - "Research combining character sequences"
        - "Investigate homoglyph attacks and confusables"
        - "Analyze zero-width and invisible characters"
        - "Study BOM handling across encodings"
        - "Research locale-dependent string operations"
      output: "Comprehensive list of encoding traps"
    
    phase_2_creation:
      description: "LLM creates task with hidden encoding flaws"
      activities:
        - "Design comparisons that fail due to normalization"
        - "Create scenarios with invisible character injection"
        - "Embed homoglyph-based security issues"
        - "Add encoding detection failures"
        - "Include locale-dependent surprises"
      output: "Complete task with encoding traps"
    
    phase_3_amplification:
      description: "LLM adds difficulty multipliers"
      activities:
        - "Layer multiple encoding issues"
        - "Add security-critical context"
        - "Include performance-sensitive scenarios"
        - "Create multi-language text mixing"
        - "Add legacy encoding compatibility"
      output: "Amplified task"
    
    phase_4_verification:
      description: "LLM validates task is genuinely hard"
      validation_criteria:
        - "Cannot be solved by simple string comparison"
        - "Requires understanding of Unicode"
        - "Has at least 5 interacting hidden traps"
        - "Would take experienced developers 30+ minutes"
        - "Has cascading failure modes that interact with each other"
        - "Requires multi-domain knowledge synthesis (algorithms + systems + performance)"
      output: "Verified task"

  quality_requirements:
    mandatory:
      - "Must require Unicode understanding"
      - "Must have non-obvious encoding issues"
      - "Must include at least 5 distinct traps"
      - "Must have security or correctness implications"
      - "Must resist simple byte-level fixes"
      - "Must take 30+ minutes for experienced developers, 45+ for intermediate"

# =============================================================================
# EXHAUSTIVE TOPIC UNIVERSE
# =============================================================================

topic_universe:
  # ---------------------------------------------------------------------------
  # Unicode Topics (40 topics)
  # ---------------------------------------------------------------------------
  unicode:
    normalization:
      - nfc_composed_form
      - nfd_decomposed_form
      - nfkc_compatibility_composed
      - nfkd_compatibility_decomposed
      - canonical_equivalence
      - compatibility_equivalence
      - normalization_stable
      - stream_safe_normalization
      - normalization_quick_check
      - normalization_boundary
    
    character_categories:
      - letters_general
      - marks_combining
      - numbers_general
      - punctuation
      - symbols
      - separators
      - control_characters
      - format_characters
      - surrogate_pairs
      - private_use
    
    special_characters:
      - zero_width_space
      - zero_width_non_joiner
      - zero_width_joiner
      - word_joiner
      - bom_byte_order_mark
      - replacement_character
      - object_replacement
      - line_separator
      - paragraph_separator
      - soft_hyphen
    
    combining_sequences:
      - combining_diacriticals
      - combining_marks
      - grapheme_clusters
      - extended_grapheme_clusters
      - emoji_sequences
      - flag_sequences
      - skin_tone_modifiers
      - variation_selectors
      - combining_enclosing
      - combining_half_marks

  # ---------------------------------------------------------------------------
  # Encoding Topics (30 topics)
  # ---------------------------------------------------------------------------
  encodings:
    unicode_encodings:
      - utf8
      - utf8_bom
      - utf16_le
      - utf16_be
      - utf16_bom
      - utf32_le
      - utf32_be
      - utf7
      - cesu8
      - mutf8
    
    legacy_encodings:
      - ascii
      - latin1_iso8859_1
      - windows_1252
      - iso8859_15
      - macroman
      - cp437
      - koi8_r
      - shift_jis
      - euc_jp
      - gb2312
      - gbk
      - gb18030
      - big5
      - euc_kr
    
    encoding_operations:
      - encoding_detection
      - encoding_conversion
      - encoding_validation
      - encoding_error_handling
      - mojibake_repair
      - double_encoding
      - encoding_normalization

  # ---------------------------------------------------------------------------
  # String Algorithm Topics (35 topics)
  # ---------------------------------------------------------------------------
  string_algorithms:
    comparison:
      - byte_comparison
      - codepoint_comparison
      - grapheme_comparison
      - case_insensitive_comparison
      - locale_aware_comparison
      - collation_comparison
      - normalized_comparison
      - fuzzy_comparison
      - similarity_metrics
      - edit_distance
    
    search:
      - substring_search
      - pattern_matching
      - regex_matching
      - unicode_aware_regex
      - word_boundary_detection
      - sentence_boundary
      - grapheme_boundary
      - line_breaking
    
    transformation:
      - case_conversion
      - case_folding
      - transliteration
      - stripping_diacritics
      - slug_generation
      - canonicalization
      - sanitization
      - escaping
    
    parsing:
      - tokenization
      - word_segmentation
      - sentence_segmentation
      - identifier_parsing
      - number_parsing
      - date_parsing
      - url_parsing
      - email_parsing

  # ---------------------------------------------------------------------------
  # Application Domains (25 topics)
  # ---------------------------------------------------------------------------
  domains:
    security:
      - homoglyph_attacks
      - invisible_character_injection
      - normalization_attacks
      - encoding_attacks
      - unicode_smuggling
      - rtl_override_attacks
      - idna_spoofing
      - confusable_detection
    
    search_and_matching:
      - search_indexing
      - fuzzy_search
      - autocomplete
      - spell_checking
      - plagiarism_detection
      - duplicate_detection
      - entity_matching
    
    data_processing:
      - csv_parsing
      - json_parsing
      - xml_parsing
      - log_parsing
      - file_path_handling
      - url_handling
      - email_handling

# =============================================================================
# COMPLEXITY DIMENSIONS
# =============================================================================

complexity_dimensions:
  unicode_complexity:
    normalization_issues:
      - multiple_representations
      - canonical_ordering
      - compatibility_mappings
      - stability_guarantees
    
    grapheme_complexity:
      - single_codepoint_graphemes
      - multi_codepoint_graphemes
      - emoji_sequences
      - regional_indicators
    
    bidirectional_complexity:
      - rtl_text_handling
      - bidi_algorithm
      - directional_overrides
      - embedding_levels

  encoding_complexity:
    detection_issues:
      - ambiguous_encodings
      - encoding_hints
      - statistical_detection
      - context_based_detection
    
    conversion_issues:
      - lossy_conversion
      - unmappable_characters
      - error_handling_modes
      - round_trip_safety

  security_complexity:
    attack_vectors:
      - visual_spoofing
      - invisible_content
      - encoding_exploits
      - normalization_exploits
    
    defense_complexity:
      - confusable_detection
      - script_mixing_detection
      - invisible_character_detection
      - normalization_enforcement

# =============================================================================
# TRAP TAXONOMY
# =============================================================================

trap_taxonomy:
  # ---------------------------------------------------------------------------
  # Normalization Traps (10 types)
  # ---------------------------------------------------------------------------
  normalization_traps:
    nfc_nfd_mismatch:
      trap_id: "NT001"
      name: "NFC vs NFD Mismatch"
      description: "Strings in different normalization forms compare unequal"
      example: |
        # "caf√©" in NFC (√© as single codepoint U+00E9)
        # "caf√©" in NFD (e + combining acute U+0065 U+0301)
        # s1 == s2 ‚Üí False (bytes differ)
        # But visually identical!
      detection_difficulty: "hard"
      manifestation: "Identical-looking strings differ"
      fix: "Normalize before comparison"
      
    missing_normalization:
      trap_id: "NT002"
      name: "Missing Normalization"
      description: "User input not normalized before storage/comparison"
      example: |
        # User types "na√Øve" (NFD from macOS)
        # Database has "na√Øve" (NFC)
        # Search fails!
      detection_difficulty: "hard"
      manifestation: "Search fails for valid queries"
      fix: "Normalize on input"
      
    double_normalization:
      trap_id: "NT003"
      name: "Double Normalization Issues"
      description: "Normalizing already-normalized text causes issues"
      example: |
        # Some characters change under NFKC
        # Ô¨Å (U+FB01) ‚Üí fi
        # Applying twice changes text
      detection_difficulty: "medium"
      manifestation: "Text changes unexpectedly"
      fix: "Normalize once at boundary"
      
    compatibility_vs_canonical:
      trap_id: "NT004"
      name: "NFKC vs NFC Confusion"
      description: "Using compatibility normalization loses information"
      example: |
        # "„éû" (U+33A2) ‚Üí "km" under NFKC
        # Information loss!
      detection_difficulty: "medium"
      manifestation: "Special characters destroyed"
      fix: "Use canonical (NFC) not compatibility (NFKC)"
      
    combining_sequence_order:
      trap_id: "NT005"
      name: "Combining Character Order"
      description: "Multiple combining marks in wrong order"
      example: |
        # a + combining_acute + combining_tilde
        # vs a + combining_tilde + combining_acute
        # Canonically equivalent but bytes differ
      detection_difficulty: "very_hard"
      manifestation: "Complex accented chars differ"
      fix: "Canonical ordering normalization"
      
    singleton_mappings:
      trap_id: "NT006"
      name: "Singleton Normalization Mappings"
      description: "Characters that map to different single characters"
      example: |
        # Œ© (U+2126 OHM SIGN) ‚Üí Œ© (U+03A9 GREEK CAPITAL OMEGA)
        # Same glyph, different codepoints
      detection_difficulty: "hard"
      manifestation: "Same character different codepoint"
      fix: "Always normalize"
      
    hangul_normalization:
      trap_id: "NT007"
      name: "Korean Hangul Composition"
      description: "Hangul syllables can be composed or decomposed"
      example: |
        # Í∞Ä can be single codepoint or jamo sequence
        # NFC composes, NFD decomposes
      detection_difficulty: "hard"
      manifestation: "Korean text comparison fails"
      fix: "Normalize Korean text"
      
    normalization_stability:
      trap_id: "NT008"
      name: "Unicode Version Normalization Change"
      description: "Normalization changed between Unicode versions"
      example: |
        # Some characters added or changed normalization
        # Old data normalized differently
      detection_difficulty: "very_hard"
      manifestation: "Old data doesn't match new"
      fix: "Pin Unicode version or re-normalize"
      
    stream_safe:
      trap_id: "NT009"
      name: "Stream-Safe Normalization Missing"
      description: "Very long combining sequences not handled"
      example: |
        # 1000 combining marks after base character
        # Must be normalized incrementally
      detection_difficulty: "hard"
      manifestation: "Performance or buffer issues"
      fix: "Use stream-safe normalization"
      
    locale_normalization:
      trap_id: "NT010"
      name: "Locale-Specific Normalization"
      description: "Some normalization is locale-dependent"
      example: |
        # Turkish i ‚Üí I mapping differs
        # "TITLE".lower() in Turkish ‚Üí "tƒ±tle"
      detection_difficulty: "hard"
      manifestation: "Locale-specific failures"
      fix: "Consider locale in case operations"

  # ---------------------------------------------------------------------------
  # Zero-Width and Invisible Character Traps (8 types)
  # ---------------------------------------------------------------------------
  invisible_traps:
    zwsp_injection:
      trap_id: "IV001"
      name: "Zero-Width Space Injection"
      description: "ZWSP (U+200B) invisible but affects operations"
      example: |
        "hello" vs "hel‚Äãlo" (with ZWSP)
        # Look identical, compare different
        # len() differs by 1
      detection_difficulty: "very_hard"
      manifestation: "Identical strings differ"
      fix: "Filter zero-width characters"
      
    zwnj_breaking:
      trap_id: "IV002"
      name: "Zero-Width Non-Joiner Issues"
      description: "ZWNJ affects ligature formation"
      example: |
        # In Persian/Arabic, ZWNJ prevents ligatures
        # "ŸÖ€å‚Äåÿ±ŸàŸÖ" vs "ŸÖ€åÿ±ŸàŸÖ" differ in display and meaning
      detection_difficulty: "hard"
      manifestation: "Text appears/means different"
      fix: "Preserve ZWNJ in relevant scripts"
      
    zwj_emoji:
      trap_id: "IV003"
      name: "ZWJ Emoji Sequence Issues"
      description: "ZWJ joins emoji into sequences"
      example: |
        # üë®‚Äçüë©‚Äçüëß = man + ZWJ + woman + ZWJ + girl
        # len() returns 5 in Python 3!
      detection_difficulty: "medium"
      manifestation: "Emoji length unexpected"
      fix: "Count grapheme clusters"
      
    bom_corruption:
      trap_id: "IV004"
      name: "BOM Marker Issues"
      description: "BOM appears as garbage at file start"
      example: |
        # UTF-8 BOM: EF BB BF
        # Shows as  in wrong encoding
        # Or breaks JSON parsing
      detection_difficulty: "medium"
      manifestation: "Garbled text at start"
      fix: "Detect and strip BOM"
      
    soft_hyphen:
      trap_id: "IV005"
      name: "Soft Hyphen Issues"
      description: "Soft hyphen invisible but present"
      example: |
        "anti¬≠disestablishment" (with soft hyphen)
        # Breaks word search, affects length
      detection_difficulty: "hard"
      manifestation: "Words don't match"
      fix: "Normalize or filter soft hyphens"
      
    rtl_override:
      trap_id: "IV006"
      name: "RTL Override Attack"
      description: "RLO character reverses text display"
      example: |
        "file‚ÄÆtxt.exe" displays as "fileexe.txt"
        # But it's actually txt.exe!
      detection_difficulty: "hard"
      manifestation: "Security vulnerability"
      fix: "Filter directional overrides"
      
    object_replacement:
      trap_id: "IV007"
      name: "Object Replacement Character"
      description: "U+FFFC placeholder in text"
      example: |
        # Copied from rich text, objects become U+FFFC
        # Shows as box or invisible
      detection_difficulty: "medium"
      manifestation: "Placeholder characters in text"
      fix: "Filter or replace"
      
    variation_selectors:
      trap_id: "IV008"
      name: "Variation Selector Issues"
      description: "VS invisible but affects rendering"
      example: |
        # Emoji vs text presentation selector
        # ‚ò∫ (text) vs ‚ò∫Ô∏è (emoji) differ by VS16
      detection_difficulty: "hard"
      manifestation: "Same character different display"
      fix: "Normalize variation selectors"

  # ---------------------------------------------------------------------------
  # Homoglyph and Confusable Traps (8 types)
  # ---------------------------------------------------------------------------
  homoglyph_traps:
    cyrillic_latin:
      trap_id: "HG001"
      name: "Cyrillic-Latin Confusables"
      description: "Cyrillic letters look like Latin"
      example: |
        "apple" (Latin) vs "–∞—Ä—Äl–µ" (–∞ and —Ä are Cyrillic)
        # Visually identical, different bytes
        # Security risk: spoofed domains
      detection_difficulty: "very_hard"
      manifestation: "Visual spoofing"
      fix: "Confusable detection, script mixing check"
      
    greek_latin:
      trap_id: "HG002"
      name: "Greek-Latin Confusables"
      description: "Greek letters look like Latin"
      example: |
        "A" (Latin) vs "Œë" (Greek Alpha)
        "H" (Latin) vs "Œó" (Greek Eta)
        "O" (Latin) vs "Œü" (Greek Omicron)
      detection_difficulty: "very_hard"
      manifestation: "Visual spoofing"
      fix: "Script consistency check"
      
    fullwidth_confusion:
      trap_id: "HG003"
      name: "Fullwidth Character Confusion"
      description: "Fullwidth variants look similar"
      example: |
        "ABC" vs "ABC" (fullwidth)
        # Both render as ABC in many fonts
      detection_difficulty: "medium"
      manifestation: "Different but looks same"
      fix: "NFKC normalization"
      
    mathematical_alphanumeric:
      trap_id: "HG004"
      name: "Mathematical Alphanumeric Symbols"
      description: "Math symbols look like letters"
      example: |
        "ùêÄ" (U+1D400 Mathematical Bold A)
        "ùë®" (U+1D468 Mathematical Bold Italic A)
        # Both look like "A"
      detection_difficulty: "hard"
      manifestation: "Visual confusion"
      fix: "NFKC or confusable mapping"
      
    digit_confusables:
      trap_id: "HG005"
      name: "Digit Confusables"
      description: "Different numeral systems look alike"
      example: |
        "123" vs "€±€≤€≥" (Persian digits)
        # Both look like 123 but different!
      detection_difficulty: "hard"
      manifestation: "Numbers don't parse"
      fix: "Normalize to ASCII digits"
      
    punctuation_confusables:
      trap_id: "HG006"
      name: "Punctuation Confusables"
      description: "Different punctuation marks look same"
      example: |
        "'" (U+0027) vs "'" (U+2019) vs " º" (U+02BC)
        # All look like apostrophe
      detection_difficulty: "medium"
      manifestation: "Parsing fails"
      fix: "Normalize punctuation"
      
    symbol_letter_confusion:
      trap_id: "HG007"
      name: "Symbol-Letter Confusion"
      description: "Symbols look like letters"
      example: |
        "Œ©" (Greek) vs "Œ©" (Ohm sign)
        "Œº" (Greek) vs "¬µ" (Micro sign)
      detection_difficulty: "hard"
      manifestation: "Same glyph, different codepoint"
      fix: "Canonicalize symbols"
      
    modifier_letter_confusion:
      trap_id: "HG008"
      name: "Modifier Letter Confusion"
      description: "Modifier letters vs regular letters"
      example: |
        "x¬≤" using superscript 2
        vs "x" + modifier letter
        # Different representations
      detection_difficulty: "medium"
      manifestation: "Formatting differs"
      fix: "NFKC normalization"

  # ---------------------------------------------------------------------------
  # Encoding Detection/Conversion Traps (8 types)
  # ---------------------------------------------------------------------------
  encoding_traps:
    mojibake:
      trap_id: "EN001"
      name: "Mojibake (Encoding Mismatch)"
      description: "Text decoded with wrong encoding"
      example: |
        "caf√©" in UTF-8: 63 61 66 C3 A9
        Decoded as Latin-1: "caf√É¬©"
        # Mojibake!
      detection_difficulty: "medium"
      manifestation: "Garbled text"
      fix: "Detect encoding correctly"
      
    double_encoding:
      trap_id: "EN002"
      name: "Double UTF-8 Encoding"
      description: "UTF-8 encoded twice"
      example: |
        "√©" ‚Üí C3 A9 (UTF-8)
        Encoded again ‚Üí C3 83 C2 A9
        Decodes as "√É¬©"
      detection_difficulty: "hard"
      manifestation: "Garbled with specific pattern"
      fix: "Detect and reverse double encoding"
      
    encoding_detection_failure:
      trap_id: "EN003"
      name: "Encoding Detection Failure"
      description: "Automatic detection picks wrong encoding"
      example: |
        # Short text with common bytes
        # Could be UTF-8, Latin-1, or Windows-1252
        # Detector guesses wrong
      detection_difficulty: "medium"
      manifestation: "Occasional garbled text"
      fix: "Use encoding hints, validate"
      
    invalid_sequences:
      trap_id: "EN004"
      name: "Invalid Byte Sequences"
      description: "Invalid bytes in encoding"
      example: |
        # Invalid UTF-8: 0x80 alone
        # Or overlong encoding
        # Or surrogate in UTF-8
      detection_difficulty: "medium"
      manifestation: "Decode error or replacement char"
      fix: "Handle invalid sequences gracefully"
      
    overlong_encoding:
      trap_id: "EN005"
      name: "UTF-8 Overlong Encoding"
      description: "Character encoded with more bytes than needed"
      example: |
        # "/" = 2F in shortest form
        # Overlong: C0 AF (2 bytes)
        # Security issue: bypasses filters
      detection_difficulty: "hard"
      manifestation: "Security vulnerability"
      fix: "Reject overlong encodings"
      
    surrogate_in_utf8:
      trap_id: "EN006"
      name: "Surrogate Codepoints in UTF-8"
      description: "UTF-16 surrogates encoded in UTF-8"
      example: |
        # D800-DFFF are surrogates
        # Invalid to encode in UTF-8
        # But some systems do it (CESU-8)
      detection_difficulty: "hard"
      manifestation: "Invalid UTF-8 or interop issues"
      fix: "Convert CESU-8 properly"
      
    null_in_string:
      trap_id: "EN007"
      name: "Embedded Null Characters"
      description: "NUL (U+0000) in string"
      example: |
        "hello\x00world"
        # C string terminates at null
        # Some APIs truncate
      detection_difficulty: "medium"
      manifestation: "Truncated strings"
      fix: "Handle or reject embedded nulls"
      
    encoding_bom_mismatch:
      trap_id: "EN008"
      name: "BOM vs Actual Encoding Mismatch"
      description: "BOM claims one encoding, data is another"
      example: |
        # File has UTF-8 BOM but contains Latin-1
        # Decoder trusts BOM, gets garbage
      detection_difficulty: "hard"
      manifestation: "Garbled after BOM"
      fix: "Validate encoding matches BOM"

  # ---------------------------------------------------------------------------
  # String Operation Traps (8 types)
  # ---------------------------------------------------------------------------
  operation_traps:
    length_mismatch:
      trap_id: "OP001"
      name: "String Length Mismatch"
      description: "Length differs by measure (bytes vs codepoints vs graphemes)"
      example: |
        s = "üë®‚Äçüë©‚Äçüëß"
        len(s.encode()) = 18 (bytes)
        len(s) = 5 (codepoints)
        grapheme count = 1
      detection_difficulty: "medium"
      manifestation: "Wrong length calculations"
      fix: "Use appropriate length measure"
      
    substring_boundary:
      trap_id: "OP002"
      name: "Substring Cuts Grapheme"
      description: "Slicing splits multi-codepoint grapheme"
      example: |
        s = "caf√©"  # NFD: c a f e ÃÅ
        s[:4] = "cafe"  # Cuts before combining acute!
      detection_difficulty: "hard"
      manifestation: "Mangled text"
      fix: "Slice on grapheme boundaries"
      
    case_folding_locale:
      trap_id: "OP003"
      name: "Locale-Dependent Case Folding"
      description: "Case conversion depends on locale"
      example: |
        # Turkish: i ‚Üí I (wrong), i ‚Üí ƒ∞ (correct)
        # Turkish: I ‚Üí ƒ± (correct), not I ‚Üí i
        "TITLE".lower() Turkish = "tƒ±tle"
      detection_difficulty: "hard"
      manifestation: "Case conversion wrong in locales"
      fix: "Use locale-aware case folding"
      
    case_length_change:
      trap_id: "OP004"
      name: "Case Conversion Changes Length"
      description: "Upper/lower case changes string length"
      example: |
        "√ü".upper() = "SS" (length 1 ‚Üí 2)
        "Ô¨Å".upper() = "FI" (length 1 ‚Üí 2)
      detection_difficulty: "medium"
      manifestation: "Length changes unexpectedly"
      fix: "Don't assume length preservation"
      
    reverse_breaks_graphemes:
      trap_id: "OP005"
      name: "String Reverse Breaks Graphemes"
      description: "Reversing string breaks combining sequences"
      example: |
        s = "caf√©"  # NFD
        s[::-1] = "ÃÅefac"  # Combining mark now on e!
      detection_difficulty: "hard"
      manifestation: "Reversed string garbled"
      fix: "Reverse grapheme clusters"
      
    regex_unicode:
      trap_id: "OP006"
      name: "Regex Unicode Issues"
      description: "Regex not Unicode-aware"
      example: |
        # \\w doesn't match accented letters by default
        # . doesn't match astral characters
        # \\b fails at grapheme boundaries
      detection_difficulty: "medium"
      manifestation: "Regex doesn't match expected"
      fix: "Use Unicode-aware regex flags"
      
    sort_locale:
      trap_id: "OP007"
      name: "Locale-Dependent Sorting"
      description: "Sort order depends on locale"
      example: |
        # Swedish: z < √∂ < √§
        # German: √§ < √∂ < z
        # Default sort uses codepoint order
      detection_difficulty: "medium"
      manifestation: "Wrong sort order"
      fix: "Use locale-aware collation"
      
    word_boundary:
      trap_id: "OP008"
      name: "Word Boundary Detection"
      description: "Word boundaries not obvious"
      example: |
        # "don't" - one word or two?
        # "rock'n'roll" - how many words?
        # Chinese has no spaces
      detection_difficulty: "medium"
      manifestation: "Wrong word count/split"
      fix: "Use proper word boundary algorithm"

# =============================================================================
# EDGE CASE CATALOG
# =============================================================================

edge_cases:
  character_edge_cases:
    - empty_string
    - ascii_only
    - latin1_extended
    - combining_characters
    - lone_combining_marks
    - emoji_simple
    - emoji_zwj_sequence
    - emoji_skin_tone
    - regional_indicators
    - variation_selectors
    - zero_width_characters
    - control_characters
    - surrogate_pairs
    - private_use_area
    - unassigned_codepoints

  encoding_edge_cases:
    - utf8_1byte
    - utf8_2byte
    - utf8_3byte
    - utf8_4byte
    - utf8_max_codepoint
    - utf16_bmp_only
    - utf16_with_surrogates
    - utf8_bom_present
    - utf16_bom_le
    - utf16_bom_be
    - no_bom
    - mixed_newlines

  normalization_edge_cases:
    - already_nfc
    - needs_nfc
    - already_nfd
    - needs_nfd
    - compatibility_characters
    - hangul_syllables
    - hangul_jamo

  script_edge_cases:
    - single_script
    - mixed_scripts_safe
    - mixed_scripts_suspicious
    - rtl_only
    - ltr_only
    - bidirectional_mixed

# =============================================================================
# ANTI-PATTERNS
# =============================================================================

anti_patterns:
  llm_failure_modes:
    - "Pattern matching without understanding Unicode normalization"
    - "Applying byte-level comparison to Unicode strings"
    - "Missing NFC vs NFD normalization differences"
    - "Ignoring zero-width character injection possibilities"
    - "Assuming string length equals grapheme count"
    - "Skipping homoglyph security analysis"
    - "Over-relying on encoding auto-detection"
    - "Missing BOM handling in file processing"
    - "Ignoring combining character sequences"
    - "Failing to handle surrogates in UTF-16"
    - "Not considering locale-dependent case folding"
    - "Missing RTL override security implications"
    - "Overlooking encoding conversion lossy failures"
    - "Failing to detect mojibake patterns"
    - "Not handling embedded null characters"
    - "Missing variation selector normalization"
    - "Overlooking grapheme cluster boundaries for substring"
    - "Failing to reverse strings at grapheme level"
    - "Not considering emoji ZWJ sequence handling"
    - "Missing regional indicator pair handling"
    - "Overlooking compatibility character mapping"
    - "Failing to handle Hangul jamo composition"
    - "Not considering bidirectional text properly"
    - "Missing word boundary algorithm for CJK"
    - "Ignoring script mixing in confusable detection"

  task_anti_patterns:
    obvious_encoding:
      description: "Encoding error visible in output"
      why_bad: "No analysis needed"
      
    single_issue:
      description: "Only one encoding problem"
      why_bad: "Too easy once found"
      
    always_fails:
      description: "Processing always fails"
      why_bad: "No intermittent debugging"

# =============================================================================
# DIFFICULTY AMPLIFIERS
# =============================================================================

difficulty_amplifiers:
  nightmare:
    multiplier: 3.0
    description: "Extreme difficulty requiring expert-level multi-domain synthesis"
    requirements:
      - "7+ interacting traps across multiple domains"
      - "Requires understanding of hardware-level effects"
      - "Time estimate: 90+ minutes for senior engineers"
      - "Multiple red herrings that waste investigation time"
      - "Solution requires synthesizing knowledge from 3+ distinct areas"

  nightmare_plus:
    multiplier: 5.0
    estimated_time: [28800, 172800]  # 8-48 hours
    command_steps: [300, 1200]
    techniques_required: 12
    description: "Research paper difficulty requiring novel Unicode handling"
    requirements:
      - "10+ deeply interacting traps across normalization/encoding/security domains"
      - "Requires novel confusable detection or homoglyph analysis"
      - "Must synthesize knowledge from Unicode standard, security, and NLP"
      - "Requires formal specification of string equivalence relations"
      - "Time estimate: 8-48 hours for competitive programmers with ICPC/IOI medal experience"
      - "Multiple invisible character injection scenarios"
      - "Must handle all Unicode normalization forms correctly"
      - "Requires understanding of text segmentation algorithms"
      - "Solution involves comprehensive Unicode-aware text processing"

# =============================================================================
# MULTI-AGENT ORCHESTRATION COMPLEXITY
# =============================================================================

multi_agent_orchestration:
  description: "Coordinating 4-7 specialized agents for complex Unicode/encoding analysis"
  
  specialized_agents:
    normalization_expert:
      role: "Handle Unicode normalization"
      capabilities:
        - "NFC/NFD/NFKC/NFKD conversion"
        - "Canonical equivalence detection"
        - "Combining character handling"
        - "Normalization form detection"
      handoff_triggers:
        - "Identical-looking strings differ"
        - "Normalization required"
    
    encoding_detector:
      role: "Detect and convert encodings"
      capabilities:
        - "Automatic encoding detection"
        - "BOM handling"
        - "Encoding conversion"
        - "Invalid sequence handling"
      handoff_triggers:
        - "Encoding errors"
        - "Mojibake detected"
    
    security_analyst:
      role: "Detect security-relevant Unicode issues"
      capabilities:
        - "Homoglyph detection"
        - "Invisible character detection"
        - "RTL override detection"
        - "Confusable mapping"
      handoff_triggers:
        - "Security review needed"
        - "Visual spoofing suspected"
    
    text_segmenter:
      role: "Handle text segmentation"
      capabilities:
        - "Grapheme cluster segmentation"
        - "Word boundary detection"
        - "Sentence boundary detection"
        - "Line breaking"
      handoff_triggers:
        - "Incorrect text splitting"
        - "Length calculation issues"
    
    locale_specialist:
      role: "Handle locale-dependent operations"
      capabilities:
        - "Locale-aware comparison"
        - "Case folding"
        - "Collation"
        - "Locale-specific formatting"
      handoff_triggers:
        - "Locale-dependent failures"
        - "Case conversion issues"

  cross_algorithm_attack_chains:
    chain_1:
      name: "Normalization Mismatch ‚Üí Comparison Failure ‚Üí Authentication Bypass ‚Üí Security Breach"
      stages:
        - "Input in NFD, database in NFC"
        - "String comparison returns false"
        - "Username not matched"
        - "Attacker creates duplicate account"
    
    chain_2:
      name: "Homoglyph Injection ‚Üí Visual Spoofing ‚Üí User Deception ‚Üí Phishing Success"
      stages:
        - "Cyrillic '–∞' looks like Latin 'a'"
        - "Domain appears legitimate"
        - "User doesn't notice difference"
        - "Credentials stolen"
    
    chain_3:
      name: "Invisible Character ‚Üí Hidden Content ‚Üí Malicious Payload ‚Üí Code Execution"
      stages:
        - "Zero-width characters hide malicious content"
        - "Displayed text looks innocent"
        - "Hidden content executes"
        - "System compromised"

# =============================================================================
# THEORETICAL COMPLEXITY REQUIREMENTS
# =============================================================================

theoretical_complexity_requirements:
  unicode_theory:
    concepts:
      - "Code points vs code units vs grapheme clusters"
      - "Unicode normalization forms"
      - "Canonical and compatibility equivalence"
      - "Unicode text segmentation"
    algorithms:
      - "Normalization algorithm"
      - "Grapheme cluster break algorithm"
      - "Word break algorithm"
      - "Line break algorithm"

  security_theory:
    concepts:
      - "Homograph attacks"
      - "IDN spoofing"
      - "Invisible text attacks"
      - "Bidirectional text attacks"
    defenses:
      - "Confusable detection"
      - "Script mixing detection"
      - "Punycode validation"

  encoding_theory:
    concepts:
      - "Character encoding models"
      - "Variable-width encodings"
      - "Stateful vs stateless encodings"
      - "Error handling modes"
    encodings:
      - "UTF-8, UTF-16, UTF-32"
      - "Legacy encodings (Latin-1, Windows-1252)"
      - "CJK encodings (Shift-JIS, GB2312)"

# =============================================================================
# ADVERSARIAL INPUT DESIGN
# =============================================================================

adversarial_input_design:
  worst_case_input_generation:
    normalization_attack:
      description: "Input in unusual normalization form"
      technique: "NFD with many combining characters"
      
    homoglyph_attack:
      description: "Visually identical but different codepoints"
      technique: "Mix Latin and Cyrillic lookalikes"
      
    invisible_injection:
      description: "Invisible characters in visible text"
      technique: "Zero-width spaces throughout"

  anti_optimization_inputs:
    pathological_normalization:
      description: "Input requiring maximum normalization work"
      technique: "Deeply nested combining character sequences"
    
    encoding_detection_confusion:
      description: "Input ambiguous between encodings"
      technique: "Valid in multiple encodings with different meanings"

  security_attacks:
    rtl_override:
      description: "Right-to-left override hiding file extension"
      technique: "file[RLO]txt.exe displays as fileexe.txt"
    
    combining_overflow:
      description: "Excessive combining characters"
      technique: "Single base character with 1000 combining marks"

# =============================================================================
# FORMAL PROOF REQUIREMENTS
# =============================================================================

formal_proof_requirements:
  correctness_proofs:
    normalization:
      requirements:
        - "Prove normalization is idempotent"
        - "Prove canonical equivalence is preserved"
        - "Prove stability across Unicode versions"
      
    comparison:
      requirements:
        - "Define equivalence relation precisely"
        - "Prove comparison is reflexive, symmetric, transitive"
        - "Prove handling of all normalization forms"

  security_proofs:
    requirements:
      - "Prove homoglyph detection completeness"
      - "Prove invisible character filtering completeness"
      - "Prove RTL override detection"

  context_amplifiers:
    security_critical:
      description: "Encoding issue has security implications"
      amplification: "Must understand attack vectors"
      
    multi_language:
      description: "Text contains multiple languages/scripts"
      amplification: "More edge cases to handle"
      
    legacy_compatibility:
      description: "Must work with legacy encodings"
      amplification: "More encoding variations"

  analysis_amplifiers:
    intermittent:
      description: "Only fails on specific inputs"
      amplification: "Must find triggering inputs"
      
    visually_identical:
      description: "Problems invisible in display"
      amplification: "Must examine bytes/codepoints"

# =============================================================================
# VARIATION ENGINES
# =============================================================================

variation_engines:
  problem_type_variations:
    comparison_fix:
      description: "Fix string comparison"
      
    normalization_fix:
      description: "Fix normalization handling"
      
    security_fix:
      description: "Fix security vulnerability"
      
    encoding_fix:
      description: "Fix encoding handling"

  domain_variations:
    user_authentication:
      context: "Username comparison"
      
    search:
      context: "Search query matching"
      
    data_import:
      context: "File import processing"
      
    api:
      context: "API string handling"

# =============================================================================
# SWE-bench_Pro style fields
# =============================================================================

problem_statement: |
  A text processing pipeline produces inconsistent results when handling user-generated 
  content. String comparisons, searches, and transformations fail intermittently.
  
  Hidden encoding issues:
  1. Unicode normalization forms (NFC vs NFD) cause "identical" strings to differ
  2. Zero-width characters are invisible but affect string operations
  3. Homoglyph characters look identical but have different codepoints
  4. Combining characters create visual equivalence but byte differences
  5. BOM markers corrupt file starts

requirements: |
  - Implement robust string comparison that handles normalization
  - Detect and handle zero-width characters
  - Identify homoglyph-based attacks or confusion
  - Handle combining characters correctly
  - Process files with various BOM markers

interface: |
  Input: Text content (potentially with encoding issues)
  Output: Normalized text, list of detected issues
  Format: UTF-8 normalized output

difficulty:
  estimated: "nightmare_plus"
  time_range: [5400, 18000]  # 90-300 minutes for competitive programmers with ICPC/IOI medal experience
  command_steps: [60, 200]
  techniques_required: 12
  trap_count: "10+ deeply interacting traps across correctness/performance/edge-case domains"
  target_audience: "Competitive programmers with ICPC/IOI medal experience"

traps:
  - type: "normalization_mismatch"
    description: "caf√© (precomposed) vs caf√© (e + combining accent) look identical"
    trigger: "Direct string comparison without normalization"
  
  - type: "zero_width_injection"
    description: "Zero-width spaces/joiners make strings appear identical"
    trigger: "Not filtering control characters"
  
  - type: "homoglyph_attack"
    description: "Cyrillic '–∞' (U+0430) looks like Latin 'a' (U+0061)"
    trigger: "Visual comparison without codepoint check"
  
  - type: "bom_corruption"
    description: "UTF-8 BOM appears as garbled characters at file start"
    trigger: "Not handling BOM markers"

  - type: "grapheme_cluster_split"
    description: "Substring operation splits multi-codepoint grapheme"
    trigger: "Slicing at codepoint rather than grapheme boundary"

  - type: "string_length_confusion"
    description: "len() returns codepoints not graphemes for emoji/combining"
    trigger: "Using len() to count visible characters"

  - type: "case_folding_locale"
    description: "Turkish 'i' uppercases to 'ƒ∞' not 'I'"
    trigger: "Using locale-independent case conversion"

  - type: "double_encoding"
    description: "UTF-8 encoded twice creates mojibake pattern"
    trigger: "Not detecting and reversing double encoding"

  - type: "rtl_override_attack"
    description: "RTL override hides file extension (file[RLO]txt.exe)"
    trigger: "Not filtering bidirectional override characters"

  - type: "combining_sequence_order"
    description: "Multiple combining marks in different order"
    trigger: "Not canonicalizing combining character order"

  - type: "surrogate_in_utf8"
    description: "UTF-16 surrogates encoded in UTF-8 (CESU-8)"
    trigger: "Not handling invalid UTF-8 surrogate sequences"

instruction_template: |
  You are fixing a {{ scenario_type }} text processing system.
  The code is at {{ path }}.
  
  Users report text appearing identical but comparing as different. Your task:
  {{ task_steps }}
  
  Dataset: {{ file_count }} files with {{ total_lines }} lines
  Reported issues: {{ issue_count }} inconsistencies

reference_solution: |
  #!/usr/bin/env python3
  import unicodedata
  import re
  from typing import List, Dict, Tuple, Set
  from dataclasses import dataclass
  from enum import Enum
  
  class IssueType(Enum):
      NORMALIZATION = "normalization_difference"
      ZERO_WIDTH = "zero_width_character"
      HOMOGLYPH = "homoglyph_detected"
      BOM = "byte_order_mark"
      COMBINING = "combining_character"
      CONTROL = "control_character"
  
  @dataclass
  class TextIssue:
      type: IssueType
      position: int
      description: str
      original: str
      suggestion: str
  
  HOMOGLYPHS = {
      '\u0430': 'a', '\u0435': 'e', '\u043e': 'o',
      '\u0440': 'p', '\u0441': 'c', '\u0443': 'y', '\u0445': 'x',
      '\u0391': 'A', '\u0392': 'B', '\u0395': 'E', '\u0397': 'H',
      '\u0399': 'I', '\u039a': 'K', '\u039c': 'M', '\u039d': 'N',
      '\u039f': 'O', '\u03a1': 'P', '\u03a4': 'T', '\u03a7': 'X',
  }
  
  ZERO_WIDTH_CHARS = {
      '\u200b', '\u200c', '\u200d', '\u2060', '\ufeff', '\u180e',
  }
  
  BOMS = {
      b'\xef\xbb\xbf': 'UTF-8',
      b'\xff\xfe': 'UTF-16-LE',
      b'\xfe\xff': 'UTF-16-BE',
  }
  
  def detect_bom(data: bytes) -> Tuple[str, bytes]:
      for bom, encoding in sorted(BOMS.items(), key=lambda x: -len(x[0])):
          if data.startswith(bom):
              return encoding, data[len(bom):]
      return None, data
  
  def analyze_text(text: str) -> Tuple[str, List[TextIssue]]:
      issues = []
      normalized_chars = []
      
      for i, char in enumerate(text):
          codepoint = ord(char)
          
          if char in ZERO_WIDTH_CHARS:
              issues.append(TextIssue(
                  type=IssueType.ZERO_WIDTH, position=i,
                  description=f"Zero-width character U+{codepoint:04X}",
                  original=char, suggestion="Remove"
              ))
              continue
          
          if char in HOMOGLYPHS:
              issues.append(TextIssue(
                  type=IssueType.HOMOGLYPH, position=i,
                  description=f"Homoglyph U+{codepoint:04X}",
                  original=char, suggestion=HOMOGLYPHS[char]
              ))
              normalized_chars.append(HOMOGLYPHS[char])
              continue
          
          category = unicodedata.category(char)
          if category.startswith('C') and char not in '\n\r\t':
              issues.append(TextIssue(
                  type=IssueType.CONTROL, position=i,
                  description=f"Control character U+{codepoint:04X}",
                  original=char, suggestion="Remove"
              ))
              continue
          
          if category.startswith('M'):
              issues.append(TextIssue(
                  type=IssueType.COMBINING, position=i,
                  description=f"Combining character U+{codepoint:04X}",
                  original=char, suggestion="Keep (will be normalized)"
              ))
          
          normalized_chars.append(char)
      
      result = ''.join(normalized_chars)
      result = unicodedata.normalize('NFC', result)
      return result, issues
  
  def safe_string_compare(s1: str, s2: str) -> Tuple[bool, List[str]]:
      norm1, _ = analyze_text(s1)
      norm2, _ = analyze_text(s2)
      differences = []
      if s1 != s2 and norm1 == norm2:
          differences.append("Strings differ only in encoding/normalization")
      return norm1 == norm2, differences

fail_to_pass:
  - "test_normalization_comparison"
  - "test_zero_width_detection"
  - "test_homoglyph_detection"
  - "test_bom_handling"

pass_to_pass:
  - "test_basic_text_processing"
  - "test_ascii_only"
  - "test_empty_string"

variables:
  - name: scenario_type
    type: string
    options: ["search engine", "user authentication", "content management", "data deduplication"]
  - name: path
    type: path
    generator: random_path
  - name: file_count
    type: int
    min: 10
    max: 100
  - name: total_lines
    type: int
    min: 1000
    max: 100000
  - name: issue_count
    type: int
    min: 5
    max: 500
  - name: task_steps
    type: template
    value: |
      1. Identify encoding-related inconsistencies
      2. Implement Unicode normalization
      3. Detect and handle zero-width characters
      4. Identify homoglyph attacks
      5. Process BOM markers correctly
      6. Create robust string comparison function

anti_hardcoding:
  canary_tokens: true
  randomize_paths: true
  dynamic_content: true
  encoding_traps:
    - nfc_vs_nfd_normalization
    - zero_width_injection
    - cyrillic_latin_homoglyphs
    - bom_markers
    - combining_diacriticals

generation_targets:
  unique_tasks: 10000
  minimum_difficulty: "experienced developer needs 30+ minutes, requires deep domain expertise"
  trap_coverage: "all trap types used at least 100 times"
