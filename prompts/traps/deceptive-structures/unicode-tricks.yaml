id: "trap-deceptive-unicode-001"
version: "1.0.0"
category: "traps"
subcategory: "deceptive-structures"

# === LLM GENERATION FRAMEWORK ===
generation_framework:
  multi_conversation_workflow:
    phase_1_research: "Research LLM blind spots around Unicode handling and text comparison"
    phase_2_creation: "Create task that exploits LLM assumptions about text equality"
    phase_3_amplification: "Add multiple layers of deception with confusable characters"
    phase_4_verification: "Validate task successfully traps most LLMs through Unicode tricks"
  
  llm_trap_design:
    common_llm_weaknesses:
      - "Pattern matching without Unicode normalization"
      - "Assuming visual appearance equals byte equality"
      - "Missing homoglyph detection in security checks"
      - "Over-reliance on training data for text handling"
      - "Difficulty reasoning about bidirectional text"
      - "Inability to detect zero-width characters"
      - "Confusion between different Unicode normalizations"
      - "Failure to consider encoding-based attacks"

# === UNICODE TRAP TAXONOMY ===
trap_taxonomy:
  category: "deceptive-structures"
  trap_type: "unicode-tricks"
  description: "Text that appears legitimate but contains deceptive Unicode characters"
  
  trap_families:
    - family: "homoglyph_attacks"
      description: "Visually similar characters from different scripts"
      difficulty_range: [4, 8]
      
    - family: "bidi_attacks"
      description: "Bidirectional text override attacks"
      difficulty_range: [5, 9]
      
    - family: "zero_width_attacks"
      description: "Invisible zero-width characters"
      difficulty_range: [4, 8]
      
    - family: "normalization_attacks"
      description: "Different normalizations of same text"
      difficulty_range: [5, 9]
      
    - family: "combining_attacks"
      description: "Combining characters to create deceptive text"
      difficulty_range: [5, 8]
      
    - family: "punycode_attacks"
      description: "IDN homograph attacks in domains"
      difficulty_range: [5, 8]
      
    - family: "case_mapping_attacks"
      description: "Unicode case conversion anomalies"
      difficulty_range: [5, 8]
      
    - family: "confusable_attacks"
      description: "Unicode confusable characters"
      difficulty_range: [4, 9]

# === LLM WEAKNESS EXPLOITATION TECHNIQUES ===
llm_weakness_exploitation:
  unicode_techniques:
    # Homoglyph Attack Techniques (40+ techniques)
    homoglyph_attacks:
      - technique: "cyrillic_a"
        description: "Cyrillic '–∞' (U+0430) looks like Latin 'a' (U+0061)"
        llm_failure_mode: "LLM doesn't check codepoints"
        implementation: |
          # Username "–∞dmin" with Cyrillic '–∞'
          username = '–∞dmin'  # U+0430, not U+0061!
          if username == 'admin':  # TRAP: False!
              grant_admin()
        detection: "Check script consistency"
        difficulty: 5
        
      - technique: "cyrillic_full_word"
        description: "Entire word in Cyrillic looks identical"
        llm_failure_mode: "LLM sees familiar word shape"
        implementation: |
          # "—Ä–∞ssword" where —Ä=Cyrillic —Ä, –∞=Cyrillic –∞, s=Cyrillic —ï
          password = '—Ä–∞—ï—ïw–ærd'  # Multiple Cyrillic chars
          if password in COMMON_PASSWORDS:  # TRAP: Not found!
              reject()
        detection: "Verify all chars are ASCII/Latin"
        difficulty: 6
        
      - technique: "greek_omicron"
        description: "Greek 'Œø' (U+03BF) looks like Latin 'o' (U+006F)"
        llm_failure_mode: "LLM accepts visually matching text"
        implementation: |
          domain = 'gŒøŒøgle.com'  # Greek omicron, not Latin o!
          if domain == 'google.com':  # TRAP: False!
              allow_request()
        detection: "Check for mixed scripts"
        difficulty: 5
        
      - technique: "mathematical_alphanumeric"
        description: "Mathematical symbols look like letters"
        llm_failure_mode: "LLM doesn't recognize math symbols as letters"
        implementation: |
          # ùêöùêùùê¶ùê¢ùêß using Mathematical Bold
          username = 'ùêöùêùùê¶ùê¢ùêß'  # U+1D41A etc.
          blocked = ['admin', 'root', 'administrator']
          if username.lower() not in blocked:  # TRAP: Not found!
              create_user(username)
        detection: "Normalize confusables first"
        difficulty: 6
        
      - technique: "fullwidth_characters"
        description: "Fullwidth ASCII looks like regular ASCII"
        llm_failure_mode: "LLM doesn't recognize fullwidth as different"
        implementation: |
          # ÔΩÅÔΩÑÔΩçÔΩâÔΩé in fullwidth
          username = 'ÔΩÅÔΩÑÔΩçÔΩâÔΩé'  # U+FF41 etc.
          if username != 'admin':  # True! Different codepoints
              allow_registration()  # TRAP: admin homoglyph registered
        detection: "Normalize width forms"
        difficulty: 5
        
      - technique: "subscript_superscript"
        description: "Subscript/superscript letters"
        llm_failure_mode: "LLM doesn't detect script variations"
        implementation: |
          # Using subscript and superscript
          # ·µÉ·µà·µê‚Å±‚Åø
          username = '·µÉ·µà·µê‚Å±‚Åø'  # Superscript chars
          if username.lower() == 'admin':  # TRAP: False!
              reject()
        detection: "Check Unicode categories"
        difficulty: 6
        
      - technique: "roman_numerals"
        description: "Roman numeral characters look like letters"
        llm_failure_mode: "LLM sees letters, not numerals"
        implementation: |
          # ‚Ö† ‚Ö§ ‚Ö© ‚Ö¨ ‚Ö≠ ‚ÖÆ ‚ÖØ look like I V X L C D M
          code = '‚Ö≠‚Ö§‚Ö©‚Ö†'  # Looks like CVXI
          if code.isalpha():  # TRAP: False! They're numerals
              process_code(code)
        detection: "Check if Number category"
        difficulty: 5
        
      - technique: "circled_letters"
        description: "Enclosed/circled letters"
        llm_failure_mode: "LLM treats circled as plain"
        implementation: |
          # ‚ìê‚ìì‚ìú‚ìò‚ìù
          username = '‚ìê‚ìì‚ìú‚ìò‚ìù'
          if 'admin' in username:  # TRAP: False!
              reject()
        detection: "Decompose enclosed characters"
        difficulty: 5
        
      - technique: "small_caps"
        description: "Small capitals look similar"
        llm_failure_mode: "LLM sees lowercase"
        implementation: |
          # ·¥Ä·¥Ö·¥ç…™…¥ (small capitals)
          text = '·¥Ä·¥Ö·¥ç…™…¥'
          if text.lower() == 'admin':  # TRAP: False!
              block()
        detection: "Normalize letter forms"
        difficulty: 5

    # Bidirectional Text Attacks (30+ techniques)
    bidi_attacks:
      - technique: "rlo_filename_spoof"
        description: "RLO makes filename appear different"
        llm_failure_mode: "LLM trusts visual appearance"
        implementation: |
          # "photo\u202Egnp.exe" displays as "photoexe.png"
          filename = 'photo\u202Egnp.exe'  # RLO character
          if filename.endswith('.png'):  # TRAP: False! Ends in .exe
              display_image(filename)
        detection: "Strip bidi control characters"
        difficulty: 7
        
      - technique: "rle_code_comment"
        description: "RLE hides malicious code in comment"
        llm_failure_mode: "LLM reads code linearly"
        implementation: |
          # /*\u202B admin = true; \u202C*/
          # Visual: /* eulav = nimda ;*/
          # But evaluates: admin = true;
          code = '/*\u202Badmin = true;\u202C*/'
        detection: "Remove bidi from code"
        difficulty: 8
        
      - technique: "rtl_number_reordering"
        description: "RTL context reorders numbers"
        llm_failure_mode: "LLM doesn't consider RTL number rendering"
        implementation: |
          # "$\u202E100" displays as "001$" 
          price = '$\u202E100'
          # User sees "001$" but it's really $100!
        detection: "Strip bidi before processing"
        difficulty: 6
        
      - technique: "mixed_bidi_confusion"
        description: "Mix of LTR and RTL text"
        llm_failure_mode: "LLM can't reason about bidi algorithm"
        implementation: |
          # Mixing Hebrew/Arabic with Latin
          text = 'Hello \u202B◊©◊ú◊ï◊ù\u202C World'
          # Display order complex
        detection: "Handle bidi explicitly"
        difficulty: 7
        
      - technique: "lro_override"
        description: "LRO forces left-to-right on RTL text"
        llm_failure_mode: "LLM doesn't detect LRO"
        implementation: |
          # Forcing LTR display of RTL text
          text = '\u202DŸÖÿ±ÿ≠ÿ®ÿß'  # Arabic forced LTR
        detection: "Check for override characters"
        difficulty: 6
        
      - technique: "isolate_attack"
        description: "Bidi isolates hide text direction"
        llm_failure_mode: "LLM doesn't understand isolates"
        implementation: |
          # Using FSI, PDI to isolate
          text = '\u2068evil\u2069.txt'
        detection: "Strip isolate characters"
        difficulty: 7
        
      - technique: "trojan_source"
        description: "Bidi characters hide malicious code"
        llm_failure_mode: "LLM code analysis misled"
        implementation: |
          # CVE-2021-42574 style attack
          # if (isAdmin\u202E \u2066// Check if admin\u2069 \u2066) {
          # Visual: if (isAdmin) { // Check if admin
          # Actual: if (isAdmin /*) { // Check if admin */
        detection: "Disallow bidi in source code"
        difficulty: 9

    # Zero-Width Character Attacks (25+ techniques)
    zero_width_attacks:
      - technique: "zwsp_username"
        description: "Zero-width space in username"
        llm_failure_mode: "LLM doesn't see invisible chars"
        implementation: |
          # "admin" vs "ad\u200Bmin" with ZWSP
          username = 'ad\u200Bmin'
          if username == 'admin':  # TRAP: False!
              deny()
          else:
              create_user(username)  # Looks like admin!
        detection: "Strip zero-width characters"
        difficulty: 5
        
      - technique: "zwnj_password"
        description: "Zero-width non-joiner in password"
        llm_failure_mode: "LLM doesn't detect ZWNJ"
        implementation: |
          # "password\u200C123"
          password = 'password\u200C123'
          if len(password) < 12:  # 15 chars but 14 visible
              reject_short()  # TRAP: Not triggered!
        detection: "Count visible characters only"
        difficulty: 5
        
      - technique: "zwj_emoji_bypass"
        description: "ZWJ creates composite emoji"
        llm_failure_mode: "LLM doesn't understand ZWJ sequences"
        implementation: |
          # üë®‚Äçüíª = üë® + ZWJ + üíª
          text = 'üë®\u200Düíª'
          if len(text) == 1:  # TRAP: len() varies by system!
              process_single_char(text)
        detection: "Use grapheme counting"
        difficulty: 6
        
      - technique: "bom_injection"
        description: "Byte Order Mark at unexpected position"
        llm_failure_mode: "LLM doesn't see BOM in middle of string"
        implementation: |
          # "admin\uFEFFroot"
          text = 'admin\uFEFFroot'
          if 'adminroot' in text:  # TRAP: False!
              block()
        detection: "Strip BOM everywhere"
        difficulty: 5
        
      - technique: "word_joiner_bypass"
        description: "Word joiner prevents word matching"
        llm_failure_mode: "LLM doesn't see word joiner"
        implementation: |
          # "pass\u2060word"
          text = 'pass\u2060word'
          if 'password' in text:  # TRAP: False!
              flag_common_password()
        detection: "Strip word joiners"
        difficulty: 5
        
      - technique: "invisible_separator"
        description: "Invisible separator looks like space"
        llm_failure_mode: "LLM confuses separator types"
        implementation: |
          # "hello\u2063world" with invisible separator
          text = 'hello\u2063world'
          parts = text.split()  # TRAP: Doesn't split!
          if len(parts) == 2:
              process_words(parts)
        detection: "Normalize whitespace"
        difficulty: 5

    # Normalization Attacks (25+ techniques)
    normalization_attacks:
      - technique: "nfc_nfd_difference"
        description: "Same visual but different normalization"
        llm_failure_mode: "LLM doesn't normalize before compare"
        implementation: |
          # '√©' NFC (U+00E9) vs '√©' NFD (U+0065 U+0301)
          nfc = '\u00E9'  # Single char
          nfd = 'e\u0301'  # Two chars
          if nfc == nfd:  # TRAP: False without normalization!
              same_user()
        detection: "Normalize to NFC before comparison"
        difficulty: 6
        
      - technique: "nfkc_compatibility"
        description: "NFKC normalizes to different codepoint"
        llm_failure_mode: "LLM doesn't apply compatibility normalization"
        implementation: |
          # 'Ô¨Å' (U+FB01) -> 'fi' in NFKC
          ligature = '\uFB01le'  # 'Ô¨Åle'
          if ligature == 'file':  # TRAP: False!
              process_file(ligature)
        detection: "Apply NFKC normalization"
        difficulty: 6
        
      - technique: "case_folding_surprise"
        description: "Case folding expands characters"
        llm_failure_mode: "LLM doesn't know case fold rules"
        implementation: |
          # '√ü' case folds to 'ss'
          german = 'stra√üe'
          if german.casefold() == 'strasse':  # True!
              # But len(german) != len('strasse')
              pass
        detection: "Understand case folding rules"
        difficulty: 6
        
      - technique: "singleton_equivalence"
        description: "Different codepoints canonically equivalent"
        llm_failure_mode: "LLM compares bytes, not canonical"
        implementation: |
          # √Ö (U+00C5) vs √Ö (U+212B ANGSTROM)
          a1 = '\u00C5'  # Latin A with ring
          a2 = '\u212B'  # Angstrom sign
          if a1 == a2:  # TRAP: False!
              same_char()
        detection: "Apply canonical equivalence"
        difficulty: 7
        
      - technique: "precomposed_decomposed"
        description: "Precomposed vs decomposed comparison"
        llm_failure_mode: "LLM doesn't handle decomposed forms"
        implementation: |
          # '√±' (U+00F1) vs 'n' + 'ÃÉ' (U+006E U+0303)
          pre = '\u00F1'
          dec = 'n\u0303'
          if pre == dec:  # TRAP: False!
              match()
        detection: "Normalize before comparison"
        difficulty: 6
        
      - technique: "hangul_composition"
        description: "Korean Hangul syllable composition"
        llm_failure_mode: "LLM doesn't understand Hangul composition"
        implementation: |
          # Í∞Ä = ·ÑÄ + ·Ö° (composed vs jamo)
          composed = 'Í∞Ä'  # U+AC00
          jamo = '·ÑÄ·Ö°'    # U+1100 U+1161
          if composed == jamo:  # TRAP: False!
              same_syllable()
        detection: "Normalize Korean text"
        difficulty: 7

    # Combining Character Attacks (20+ techniques)
    combining_attacks:
      - technique: "zalgo_text"
        description: "Excessive combining characters"
        llm_failure_mode: "LLM doesn't limit combiners"
        implementation: |
          # "Hello" with 100 combining chars each
          zalgo = 'HÃ∏Ã®ÃõÃØÃ±ÃüÃ†ÃôÃ¶ÃÆÃ≤Ã£ÃûÃèÃåÃÜÃëÃëÃÖÃáÃöeÃµÃ°ÃõÃÆÃ±Ã≠Ã≥Ã∫Ã≤Ã£Ã†ÃøÃàÃÅÃëÃÉÃÄÃàÃÅÃïÕ†lÃ∏Ã®ÃõÃü...'
          if len(zalgo) > 100:  # Might be huge!
              reject()
        detection: "Limit combining characters"
        difficulty: 5
        
      - technique: "combining_override"
        description: "Combining marks change appearance"
        llm_failure_mode: "LLM doesn't see combining marks"
        implementation: |
          # 'a' + combining solidus (strikethrough)
          text = 'a\u0338'  # Displays as crossed a
          if text == 'a':  # TRAP: False!
              same_char()
        detection: "Handle combining marks"
        difficulty: 5
        
      - technique: "stacked_diacritics"
        description: "Multiple diacritics stack"
        llm_failure_mode: "LLM doesn't track diacritic count"
        implementation: |
          # 'e' with acute, grave, and cedilla
          text = 'e\u0301\u0300\u0327'
          if text.isprintable():  # Still True!
              display(text)  # TRAP: Renders weird
        detection: "Validate diacritic combinations"
        difficulty: 6
        
      - technique: "enclosing_mark"
        description: "Enclosing mark changes appearance drastically"
        llm_failure_mode: "LLM doesn't detect enclosing marks"
        implementation: |
          # 'A' + enclosing circle = ‚í∂-like
          text = 'A\u20DD'  # A with enclosing circle
          if text == 'A':  # TRAP: False!
              plain_a()
        detection: "Detect enclosing marks"
        difficulty: 6

    # Punycode/IDN Attacks (20+ techniques)
    punycode_attacks:
      - technique: "idn_homograph"
        description: "IDN domain looks like legitimate domain"
        llm_failure_mode: "LLM doesn't decode punycode"
        implementation: |
          # xn--pple-43d.com looks like apple.com
          domain = 'xn--pple-43d.com'  # '–∞pple.com' with Cyrillic –∞
          if domain == 'apple.com':  # TRAP: False!
              allow()
        detection: "Decode and check for homoglyphs"
        difficulty: 6
        
      - technique: "mixed_script_domain"
        description: "Domain mixes multiple scripts"
        llm_failure_mode: "LLM doesn't detect script mixing"
        implementation: |
          # 'g–æ–ægle.com' with Cyrillic –æ
          domain = 'g–æ–ægle.com'  # IDN: xn--ggle-55da.com
          if is_safe_domain(domain):  # TRAP: Looks safe!
              redirect(domain)
        detection: "Reject mixed-script domains"
        difficulty: 6
        
      - technique: "subdomain_confusion"
        description: "Punycode subdomain tricks"
        llm_failure_mode: "LLM doesn't decode subdomain"
        implementation: |
          # 'login.xn--80ak6aa92e.com' = 'login.–∞—Ä—Äle.com'
          url = 'https://login.xn--80ak6aa92e.com/signin'
        detection: "Decode all domain parts"
        difficulty: 6
        
      - technique: "tld_homograph"
        description: "TLD contains homoglyph"
        llm_failure_mode: "LLM trusts TLD"
        implementation: |
          # '.cŒøm' with Greek omicron
          domain = 'google.cŒøm'
          if domain.endswith('.com'):  # TRAP: False!
              safe()
        detection: "Validate TLD against list"
        difficulty: 6

    # Case Mapping Attacks (15+ techniques)
    case_mapping_attacks:
      - technique: "turkish_i_problem"
        description: "Turkish dotless i case mapping"
        llm_failure_mode: "LLM uses ASCII case rules"
        implementation: |
          # Turkish: 'i' uppercases to 'ƒ∞', 'I' lowercases to 'ƒ±'
          text = 'Istanbul'
          if text.lower() == 'istanbul':  # TRAP: Locale dependent!
              match()
        detection: "Use casefold() or specify locale"
        difficulty: 6
        
      - technique: "german_eszett"
        description: "√ü uppercase expansion"
        llm_failure_mode: "LLM doesn't handle √ü correctly"
        implementation: |
          # '√ü'.upper() = 'SS'
          text = 'stra√üe'
          upper = text.upper()  # 'STRASSE' - length changed!
          if len(text) == len(upper):  # TRAP: False!
              same_length()
        detection: "Account for length changes"
        difficulty: 5
        
      - technique: "greek_sigma"
        description: "Greek final sigma case mapping"
        llm_failure_mode: "LLM doesn't know about final sigma"
        implementation: |
          # Final œÉ (œÇ) vs medial œÉ
          word = 'ŒüŒîŒ•Œ£Œ£ŒïŒ•Œ£'
          lower = word.lower()  # 'ŒøŒ¥œÖœÉœÉŒµœÖœÇ' with final œÇ
          if lower == 'ŒøŒ¥œÖœÉœÉŒµœÖœÉ':  # TRAP: depends on position!
              match()
        detection: "Use proper Greek case mapping"
        difficulty: 7
        
      - technique: "titlecase_surprise"
        description: "Titlecase different from uppercase"
        llm_failure_mode: "LLM confuses title and upper"
        implementation: |
          # '«â' (U+01C9) -> titlecase '«à', uppercase '«á'
          text = '\u01C9'
          if text.upper() == text.title():  # TRAP: False!
              same()
        detection: "Understand titlecase rules"
        difficulty: 6

# === TRAP MECHANISMS ===
trap_mechanisms:
  attack_vectors:
    - mechanism: "visual_spoofing"
      description: "Text looks legitimate but contains deceptive chars"
      applications:
        - "Username impersonation"
        - "URL spoofing"
        - "Filename deception"
      detection_difficulty: 6
      
    - mechanism: "comparison_bypass"
      description: "Text fails equality check due to Unicode differences"
      applications:
        - "Blocklist bypass"
        - "Authentication bypass"
        - "Filter evasion"
      detection_difficulty: 5
      
    - mechanism: "length_confusion"
      description: "Visual length differs from codepoint/byte length"
      applications:
        - "Buffer overflow"
        - "Validation bypass"
        - "Truncation attacks"
      detection_difficulty: 5
      
    - mechanism: "rendering_attacks"
      description: "Text renders differently than stored"
      applications:
        - "Bidi code injection"
        - "Comment-based attacks"
        - "UI deception"
      detection_difficulty: 7

# === ANTI-DETECTION TECHNIQUES ===
anti_detection_techniques:
  evasion_methods:
    - method: "subtle_substitution"
      description: "Replace only few characters with homoglyphs"
      difficulty: 7
      
    - method: "partial_script"
      description: "Use homoglyphs from related scripts"
      difficulty: 6
      
    - method: "combining_camouflage"
      description: "Hide malicious chars with combining marks"
      difficulty: 7
      
    - method: "normalization_aware"
      description: "Use chars that survive normalization"
      difficulty: 8

# === LLM-SPECIFIC TRAPS ===
llm_specific_traps:
  assumption_exploitation:
    - trap: "visual_equals_semantic"
      description: "LLM trusts visual appearance of text"
      examples:
        - name: "admin_homoglyph"
          code: |
            def is_admin(username):
                return username == 'admin'  # TRAP: homoglyph bypass!
          llm_failure: "LLM doesn't check for homoglyphs"
          
    - trap: "string_length_trust"
      description: "LLM trusts len() for text length"
      examples:
        - name: "length_validation"
          code: |
            def validate_pin(pin):
                if len(pin) != 4:  # TRAP: ZWJ/ZWSP chars!
                    raise ValueError("PIN must be 4 digits")
          llm_failure: "LLM counts codepoints, not graphemes"
          
    - trap: "comparison_is_safe"
      description: "LLM trusts string comparison"
      examples:
        - name: "direct_compare"
          code: |
            if user_input == 'caf√©':  # TRAP: NFC vs NFD!
                process()
          llm_failure: "LLM doesn't normalize before compare"

# === DIFFICULTY MULTIPLIERS ===
difficulty_multipliers:
  complexity_factors:
    - factor: "subtle_homoglyphs"
      multiplier: 1.5
      description: "Using very similar-looking chars"
      
    - factor: "multiple_techniques"
      multiplier: 1.7
      description: "Combining multiple Unicode tricks"
      
    - factor: "context_dependent"
      multiplier: 1.4
      description: "Attack depends on rendering context"
      
    - factor: "locale_specific"
      multiplier: 1.3
      description: "Attack works in specific locales"

# === COMPREHENSIVE TASK TEMPLATES ===
task_templates:
  - id: "unicode_input_validation"
    name: "Unicode Input Validation"
    description: "Implement secure Unicode input validation"
    difficulty: [5, 9]
    template: |
      A {{ system_type }} needs to validate user input that may contain
      Unicode tricks.
      
      Current validation:
      {{ current_code }}
      
      Reported bypasses:
      {{ bypasses }}
      
      Your tasks:
      1. Identify all Unicode attack vectors
      2. Implement proper normalization
      3. Detect homoglyphs
      4. Handle bidi characters
      5. Strip invisible characters
      
  - id: "domain_validation"
    name: "Secure Domain Validation"
    description: "Validate domains against IDN homograph attacks"
    difficulty: [5, 8]
    template: |
      A {{ service_type }} needs to validate URLs/domains.
      
      Current validation:
      {{ current_code }}
      
      Known attack domains:
      {{ attack_domains }}
      
      Your tasks:
      1. Decode punycode domains
      2. Detect mixed scripts
      3. Check for homoglyphs
      4. Implement confusable detection
      5. Build comprehensive validation

# === GENERATION CONFIGURATION ===
generation_config:
  min_variants_per_template: 100
  total_combinations_target: 10000
  
  randomization_rules:
    - rule: "homoglyph_selection"
      description: "Vary which homoglyphs are used"
    - rule: "attack_combination"
      description: "Combine different Unicode tricks"
    - rule: "context_variation"
      description: "Vary attack context (URL, username, etc.)"

# === REFERENCE SOLUTION PATTERNS ===
reference_solutions:
  unicode_sanitizer: |
    import unicodedata
    import re
    
    ZERO_WIDTH = {
        '\u200B', '\u200C', '\u200D', '\uFEFF',
        '\u2060', '\u180E', '\u2063', '\u2064'
    }
    
    BIDI_CONTROL = {
        '\u200E', '\u200F', '\u202A', '\u202B',
        '\u202C', '\u202D', '\u202E', '\u2066',
        '\u2067', '\u2068', '\u2069'
    }
    
    def sanitize(text):
        """Sanitize Unicode text for safe comparison."""
        # Remove zero-width characters
        text = ''.join(c for c in text if c not in ZERO_WIDTH)
        
        # Remove bidi control characters  
        text = ''.join(c for c in text if c not in BIDI_CONTROL)
        
        # Normalize to NFKC (compatibility decomposition)
        text = unicodedata.normalize('NFKC', text)
        
        return text
    
    def is_safe_string(text):
        """Check if string contains only safe characters."""
        for char in text:
            if unicodedata.category(char) in ('Cf', 'Cc', 'Co', 'Cn'):
                return False
        return True
  
  homoglyph_detector: |
    import unicodedata
    
    def detect_mixed_scripts(text):
        """Detect if text mixes multiple scripts."""
        scripts = set()
        for char in text:
            try:
                name = unicodedata.name(char)
                if 'LATIN' in name:
                    scripts.add('Latin')
                elif 'CYRILLIC' in name:
                    scripts.add('Cyrillic')
                elif 'GREEK' in name:
                    scripts.add('Greek')
                # Add more scripts...
            except ValueError:
                pass
        return len(scripts) > 1
  
  safe_comparison: |
    import unicodedata
    
    def safe_compare(s1, s2):
        """Compare strings safely with normalization."""
        # Normalize both to NFKC
        n1 = unicodedata.normalize('NFKC', s1)
        n2 = unicodedata.normalize('NFKC', s2)
        
        # Case fold for case-insensitive
        return n1.casefold() == n2.casefold()

# === ANTI-HARDCODING CONFIGURATION ===
anti_hardcoding:
  canary_tokens: true
  randomize_paths: true
  dynamic_content: true
  
  unicode_variations:
    - vary: "homoglyph_chars"
    - vary: "bidi_positions"
    - vary: "zero_width_types"
    - vary: "normalization_forms"

# === TEST CASE GENERATION ===
test_case_generation:
  fail_to_pass:
    - "test_homoglyph_detected"
    - "test_bidi_stripped"
    - "test_zero_width_removed"
    - "test_normalization_applied"
    - "test_mixed_scripts_detected"

  pass_to_pass:
    - "test_valid_unicode_accepted"
    - "test_legitimate_text_unchanged"
    - "test_proper_normalization"

# === ADDITIONAL UNICODE TECHNIQUES ===
additional_unicode_techniques:
  # Script-Specific Attacks (20+ techniques)
  script_specific_attacks:
    - technique: "arabic_digit_confusion"
      description: "Arabic-Indic digits look like ASCII"
      llm_failure_mode: "LLM doesn't detect digit script"
      implementation: |
        # Ÿ†Ÿ°Ÿ¢Ÿ£Ÿ§Ÿ•Ÿ¶ŸßŸ®Ÿ© Arabic-Indic digits
        pin = 'Ÿ°Ÿ¢Ÿ£Ÿ§'  # Looks like 1234 but different!
        if pin.isdigit():  # True!
            validate_pin(pin)  # TRAP: Wrong digits!
      difficulty: 6
      
    - technique: "devanagari_digit_confusion"
      description: "Devanagari digits look similar"
      llm_failure_mode: "LLM treats all digits same"
      implementation: |
        # ‡•¶‡•ß‡•®‡•©‡•™‡•´‡•¨‡•≠‡•Æ‡•Ø Devanagari digits
        amount = '‡•´‡•¶‡•¶'  # Looks like 500
        if amount.isdigit():  # True!
            process_payment(amount)  # TRAP!
      difficulty: 6
      
    - technique: "han_character_confusion"
      description: "CJK characters with multiple readings"
      llm_failure_mode: "LLM doesn't understand Han unification"
      implementation: |
        # Same codepoint, different meaning in Chinese/Japanese
        char = 'ÁùÄ'  # Different meaning in Chinese vs Japanese
        if is_chinese(char):  # TRAP: Might be Japanese!
            translate_chinese(char)
      difficulty: 7
      
    - technique: "yi_syllable_confusion"
      description: "Yi syllables resemble letters"
      llm_failure_mode: "LLM sees familiar shapes"
      implementation: |
        # ÍÅ•ÍÄïÍÅï Yi syllables look like letters
        text = 'ÍÄïÍÅ•ÍÄï'
        if text.isalpha():  # True for Yi!
            process_text(text)
      difficulty: 7
      
    - technique: "thai_digit_confusion"
      description: "Thai digits with unfamiliar shapes"
      llm_failure_mode: "LLM doesn't validate digit range"
      implementation: |
        # ‡πê‡πë‡πí‡πì‡πî‡πï‡πñ‡πó‡πò‡πô Thai digits
        if '‡πë‡πí‡πì'.isdigit():  # True!
            int('‡πë‡πí‡πì')  # Might not work as expected
      difficulty: 6

  # Emoji and Symbol Attacks (15+ techniques)
  emoji_attacks:
    - technique: "emoji_modifier_hiding"
      description: "Skin tone modifier changes meaning"
      llm_failure_mode: "LLM counts emojis wrong"
      implementation: |
        # üëçüèª = üëç + üèª (2 codepoints, 1 grapheme)
        text = 'üëçüèªüëçüèæüëç'
        if len(text) == 3:  # TRAP: codepoints != graphemes
            process_three_items(text)
      difficulty: 5
      
    - technique: "family_emoji_expansion"
      description: "Family emoji is many codepoints"
      llm_failure_mode: "LLM treats as single char"
      implementation: |
        # üë®‚Äçüë©‚Äçüëß‚Äçüë¶ = 7 codepoints with ZWJ
        family = 'üë®‚Äçüë©‚Äçüëß‚Äçüë¶'
        if len(family) == 1:  # TRAP: It's 7!
            single_char_process(family)
      difficulty: 6
      
    - technique: "flag_emoji_confusion"
      description: "Flag emojis from regional indicators"
      llm_failure_mode: "LLM doesn't understand flag composition"
      implementation: |
        # üá∫üá∏ = üá∫ + üá∏ (Regional Indicator U + S)
        flag = '\U0001F1FA\U0001F1F8'  # US flag
        # TRAP: Looks like one char, is two!
      difficulty: 5
      
    - technique: "variation_selector_hiding"
      description: "Variation selectors change rendering"
      llm_failure_mode: "LLM doesn't see variation selectors"
      implementation: |
        # VS15 (text) vs VS16 (emoji)
        heart = '‚ù§Ô∏è'  # With VS16
        heart_text = '‚ù§Ô∏é'  # With VS15
        if heart == heart_text:  # TRAP: False!
            same_heart()
      difficulty: 6
      
    - technique: "tag_sequence_hiding"
      description: "Tag sequences for subdivision flags"
      llm_failure_mode: "LLM doesn't understand tag chars"
      implementation: |
        # üè¥Û†ÅßÛ†Å¢Û†Å≥Û†Å£Û†Å¥Û†Åø = Scotland flag
        # Contains invisible tag characters
        scotland = 'üè¥Û†ÅßÛ†Å¢Û†Å≥Û†Å£Û†Å¥Û†Åø'
        # TRAP: Contains hidden tag sequence!
      difficulty: 7

  # Control Character Attacks (15+ techniques)
  control_char_attacks:
    - technique: "paragraph_separator"
      description: "Paragraph separator breaks text"
      llm_failure_mode: "LLM doesn't see paragraph break"
      implementation: |
        # U+2029 Paragraph Separator
        text = 'first\u2029second'
        lines = text.split('\n')  # TRAP: Doesn't split!
        if len(lines) == 1:  # True! But visually two lines
            single_line_process(text)
      difficulty: 5
      
    - technique: "line_separator"
      description: "Line separator vs newline"
      llm_failure_mode: "LLM expects \\n only"
      implementation: |
        # U+2028 Line Separator
        text = 'line1\u2028line2'
        if '\n' in text:  # TRAP: False!
            handle_multiline(text)
      difficulty: 5
      
    - technique: "soft_hyphen_hiding"
      description: "Soft hyphen invisible but affects search"
      llm_failure_mode: "LLM doesn't see soft hyphen"
      implementation: |
        # U+00AD Soft Hyphen
        word = 'pass\u00ADword'
        if 'password' in word:  # TRAP: False!
            block_common_password()
      difficulty: 5
      
    - technique: "interlinear_annotation"
      description: "Annotation characters for ruby text"
      llm_failure_mode: "LLM doesn't understand annotation"
      implementation: |
        # U+FFF9 - U+FFFB Interlinear Annotation
        text = '\uFFF9base\uFFFA annotation\uFFFB'
        if text == 'base':  # TRAP: Has annotation!
            simple_text()
      difficulty: 6
      
    - technique: "object_replacement"
      description: "Object replacement character"
      llm_failure_mode: "LLM doesn't see placeholder"
      implementation: |
        # U+FFFC Object Replacement Character
        text = 'Click \uFFFC to continue'
        if '\uFFFC' in text:  # Placeholder for embedded object
            replace_object(text)
      difficulty: 5

  # Encoding-Specific Attacks (10+ techniques)
  encoding_attacks:
    - technique: "overlong_utf8"
      description: "Overlong UTF-8 encoding bypass"
      llm_failure_mode: "LLM doesn't validate UTF-8 form"
      implementation: |
        # '/' can be encoded as C0 AF (overlong)
        # Should be rejected but some parsers accept
        overlong_slash = b'\xc0\xaf'  # Invalid overlong '/'
      difficulty: 7
      
    - technique: "utf7_injection"
      description: "UTF-7 encoding in wrong context"
      llm_failure_mode: "LLM doesn't consider UTF-7"
      implementation: |
        # +ADw- = < in UTF-7
        text = '+ADw-script+AD4-'  # <script> in UTF-7
        if '<' not in text:  # TRAP: True for UTF-8 check!
            safe_html(text)
      difficulty: 7
      
    - technique: "byte_order_mark_utf16"
      description: "UTF-16 BOM changes interpretation"
      llm_failure_mode: "LLM doesn't check BOM"
      implementation: |
        # FE FF = UTF-16 BE, FF FE = UTF-16 LE
        # Reading as UTF-8 gives garbage
        with open(file, 'r', encoding='utf-8') as f:
            text = f.read()  # TRAP: Might be UTF-16!
      difficulty: 6

# === COMPREHENSIVE UNICODE HANDLING ===
unicode_handling_patterns:
  secure_comparison: |
    import unicodedata
    from confusables import is_confusable  # pypi package
    
    def secure_string_compare(s1, s2):
        """Compare strings securely with Unicode handling."""
        # Normalize both
        n1 = unicodedata.normalize('NFKC', s1)
        n2 = unicodedata.normalize('NFKC', s2)
        
        # Remove zero-width and control
        n1 = ''.join(c for c in n1 if unicodedata.category(c) not in ('Cc', 'Cf'))
        n2 = ''.join(c for c in n2 if unicodedata.category(c) not in ('Cc', 'Cf'))
        
        return n1 == n2

  grapheme_counting: |
    import regex  # pip install regex
    
    def grapheme_len(text):
        """Count grapheme clusters, not codepoints."""
        return len(regex.findall(r'\X', text))
    
    def grapheme_slice(text, start, end):
        """Slice by grapheme clusters."""
        graphemes = regex.findall(r'\X', text)
        return ''.join(graphemes[start:end])

  script_detection: |
    import unicodedata
    
    def get_scripts(text):
        """Get all scripts used in text."""
        scripts = set()
        for char in text:
            try:
                name = unicodedata.name(char)
                for script in ['LATIN', 'CYRILLIC', 'GREEK', 'ARABIC', 'HEBREW']:
                    if script in name:
                        scripts.add(script)
                        break
            except ValueError:
                pass
        return scripts
    
    def is_mixed_script(text):
        """Detect if text mixes scripts (potential homoglyph)."""
        scripts = get_scripts(text)
        return len(scripts) > 1

# === ENHANCED DIFFICULTY CONFIGURATION ===
enhanced_difficulty_config:
  time_range: [7200, 18000]  # 120-300 minutes for expert reverse engineers
  command_steps: [150, 500]  # Minimum 150 distinct analysis steps required
  trap_count: "15+ nested, mutually-reinforcing traps"
  
  quality_requirements:
    description: "120-300 minutes for expert reverse engineers and security researchers"
    expertise_level: "Elite security researcher with mastery of Unicode internals, text rendering engines, and internationalization security"
    prerequisite_knowledge:
      - "Unicode normalization forms (NFC, NFD, NFKC, NFKD) and their edge cases"
      - "Bidirectional text algorithm (UAX #9) and all control characters"
      - "Homoglyph detection, confusable character mapping, and script detection"
      - "IDN/Punycode encoding, TR46, and domain spoofing techniques"
      - "Unicode Security Mechanisms (UTR #36 and UTR #39)"
      - "Text rendering engine internals (HarfBuzz, FreeType, CoreText)"
      - "Font fallback mechanisms and glyph substitution rules"
      - "Unicode collation algorithm and locale-specific sorting"
  
  phase_4_verification:
    validation_criteria:
      - "Contains 15+ interacting deceptive patterns that cascade across abstraction layers"
      - "Has cascading state corruption that spreads across text processing pipelines"
      - "Requires understanding of font rendering and text shaping"
      - "Would take expert security researchers 120+ minutes"
      - "Visual spoofing that survives multiple normalization passes"
      - "Script mixing attacks that exploit locale-specific behavior"
      - "Bidi attacks that defeat multiple layers of defense"
      - "Normalization bypass through decomposition sequences"
      - "Font-specific rendering attacks"
  
  difficulty_amplifiers:
    nightmare:
      multiplier: 4.0
      description: "Deceptive complexity designed to defeat even expert analysis"
      requirements:
        - "7+ interacting traps that mask each other's symptoms"
        - "Requires understanding of compiler optimizations and runtime behavior"
        - "Time estimate: 120+ minutes for senior engineers"
        - "Timing-dependent behavior that varies across environments"
        - "Requires synthesizing debugging, systems, and language internals"
    
    nightmare_plus:
      multiplier: 8.0
      estimated_time: [43200, 259200]  # 12-72 hours
      command_steps: [600, 3000]
      techniques_required: 25
      description: "CTF championship-level difficulty with adversarial anti-analysis mechanisms"
      requirements:
        - "15+ interacting Unicode tricks with cascading visual deception"
        - "Rendering-engine-specific attacks that vary by platform"
        - "Locale and font-dependent behavior exploitation"
        - "Requires expertise in Unicode standard, text rendering, and security"
        - "Time estimate: 12-72 hours for elite security researchers"
        - "Tricks that appear different in different viewers/editors"
        - "Normalization-resistant deception techniques"
        - "Multi-layer bidi attacks with nested isolates"
  
  anti_patterns:
    llm_failure_modes:
      - "Trusting visible code structure over actual execution behavior"
      - "Missing compiler optimization effects on code semantics"
      - "Ignoring Unicode normalization and rendering differences"
      - "Not considering filesystem race conditions with symlinks"
      - "Missing singleton initialization order dependencies"
      - "Overlooking environment variable and config file interactions"
      - "Assuming deterministic behavior in timing-sensitive code"
      - "Missing state corruption that only manifests under specific sequences"
      - "Ignoring encoding round-trip issues that corrupt data silently"
      - "Failing to detect bidi control characters in source code"
      - "Missing homoglyphs from obscure scripts"
      - "Not considering font-specific rendering variations"
  
  generation_targets:
    minimum_difficulty: "120-300 minutes, requires elite expertise in Unicode security and text rendering"
    target_audience: "Expert reverse engineers, CTF champions, and elite security researchers"
    success_criteria: "LLM fails to identify all Unicode tricks in first 5 attempts"

# === MULTI-AGENT ORCHESTRATION COMPLEXITY ===
multi_agent_orchestration:
  description: "Coordinating specialized Unicode deception detection agents"
  
  required_agents:
    - agent: "codepoint_analyzer"
      role: "Analyze Unicode codepoints and their properties"
      capabilities:
        - "Script property detection per codepoint"
        - "Confusable character mapping"
        - "Control character identification"
      handoff_triggers:
        - "Discovers mixed scripts in identifier"
        - "Finds suspicious control characters"
    
    - agent: "normalization_tracker"
      role: "Track text through normalization transformations"
      capabilities:
        - "Pre and post normalization comparison"
        - "Decomposition sequence analysis"
        - "Canonical equivalence detection"
      handoff_triggers:
        - "Text changes through normalization"
        - "Discovers normalization bypass"
    
    - agent: "bidi_analyst"
      role: "Analyze bidirectional text rendering"
      capabilities:
        - "Bidi algorithm simulation"
        - "Control character effect prediction"
        - "Visual vs logical order mapping"
      handoff_triggers:
        - "Discovers bidi override attacks"
        - "Finds trojan source patterns"
    
    - agent: "rendering_simulator"
      role: "Simulate text rendering across platforms"
      capabilities:
        - "Multi-platform rendering comparison"
        - "Font fallback simulation"
        - "Glyph substitution tracking"
      handoff_triggers:
        - "Text renders differently across platforms"
        - "Font-specific attacks detected"
    
    - agent: "homoglyph_detector"
      role: "Detect visually similar characters"
      capabilities:
        - "Cross-script confusable detection"
        - "Mathematical symbol detection"
        - "Width and style variant detection"
      handoff_triggers:
        - "Discovers homoglyph substitution"
        - "Finds script mixing"
    
    - agent: "idn_specialist"
      role: "Analyze internationalized domain names"
      capabilities:
        - "Punycode decoding and analysis"
        - "TR46 compliance checking"
        - "Visual similarity detection"
      handoff_triggers:
        - "Discovers IDN homograph attack"
        - "Finds punycode encoding tricks"
    
    - agent: "locale_analyst"
      role: "Analyze locale-specific behavior"
      capabilities:
        - "Case folding rule analysis"
        - "Collation order checking"
        - "Locale-specific normalization"
      handoff_triggers:
        - "Discovers locale-dependent behavior"
        - "Finds Turkish I problem variants"
    
    - agent: "grapheme_specialist"
      role: "Analyze grapheme clusters and segmentation"
      capabilities:
        - "Grapheme cluster boundary detection"
        - "Emoji sequence analysis"
        - "Combining character handling"
      handoff_triggers:
        - "Discovers grapheme counting attacks"
        - "Finds emoji-based deception"
  
  cross_artifact_deception_chains:
    - chain: "homoglyph_to_auth_bypass"
      description: "Homoglyph in username bypasses authentication"
      stages:
        - "Username appears identical to admin"
        - "Different codepoints bypass string comparison"
        - "Gains unauthorized access"
    
    - chain: "bidi_to_code_injection"
      description: "Bidi characters hide malicious code"
      stages:
        - "Source code appears to show safe logic"
        - "Bidi control characters reorder displayed code"
        - "Actual execution path is different from displayed"
    
    - chain: "normalization_to_data_corruption"
      description: "Normalization changes corrupt data semantically"
      stages:
        - "Data appears valid before storage"
        - "Normalization on retrieval changes meaning"
        - "Semantic corruption of stored data"
  
  parallel_deception_analysis:
    shared_symbolic_state:
      - "Codepoint sequences across text boundaries"
      - "Normalization state across processing steps"
      - "Bidi embedding level stack"
      - "Script detection state"
    
    synchronization_requirements:
      - "Consistent view of text across encoding transformations"
      - "Coordinated analysis of normalized vs original"
      - "Synchronized rendering simulation"
  
  agent_handoff_protocols:
    protocol: "multi_layer_unicode_deception_unwinding"
    steps:
      - "Initial agent detects surface-level unicode anomaly"
      - "Handoff to specialist for specific attack vector"
      - "Specialist discovers combined attack"
      - "Recursive handoff for each deception layer"
      - "Final synthesis of complete attack chain"

# === META TRAP LAYERS ===
meta_trap_layers:
  first_order_traps:
    description: "Obvious Unicode tricks detectable with basic tools"
    examples:
      - "Simple Cyrillic substitution for Latin"
      - "Obvious zero-width characters"
      - "Basic bidi override"
      - "Simple homoglyph substitution"
    detection_time: "5-15 minutes"
    llm_detection_rate: "70-90%"
  
  second_order_traps:
    description: "Unicode tricks hidden behind normalization"
    examples:
      - "Confusables that survive NFKC normalization"
      - "Bidi attacks using isolates instead of overrides"
      - "Combining character abuse"
      - "Mathematical alphanumeric symbols"
    detection_time: "30-60 minutes"
    llm_detection_rate: "30-50%"
  
  third_order_traps:
    description: "Platform and locale-dependent tricks"
    examples:
      - "Font-specific rendering that changes meaning"
      - "Locale-specific case folding exploitation"
      - "Collation order manipulation"
      - "Grapheme segmentation differences"
    detection_time: "60-120 minutes"
    llm_detection_rate: "10-20%"
  
  nth_order_traps:
    description: "Cascading and adaptive Unicode deception"
    examples:
      - "Normalization changes that trigger other normalizations"
      - "Text that changes meaning based on analysis tool"
      - "Rendering-dependent semantic changes"
      - "Cross-layer attacks (Unicode + encoding + filesystem)"
    detection_time: "120+ minutes per additional layer"
    llm_detection_rate: "<5%"
    
  recursive_trap_structures:
    - structure: "normalization_cascade"
      description: "Each normalization reveals new confusables"
      depth: "depends on normalization chain"
      
    - structure: "bidi_nesting"
      description: "Nested bidi controls with escalating complexity"
      depth: "limited by bidi embedding level limit (125)"
      
    - structure: "font_fallback_chain"
      description: "Different fonts reveal different glyphs"
      depth: "depends on system font configuration"

# === ANTI-ANALYSIS TECHNIQUES ===
anti_analysis_techniques:
  environment_detection:
    - technique: "font_detection"
      description: "Detect available fonts to adjust attack"
      methods:
        - "Test rendering of rare characters"
        - "Detect font fallback behavior"
        - "Identify platform through font availability"
      behavior_change: "Adjust homoglyphs based on available fonts"
    
    - technique: "locale_detection"
      description: "Detect system locale for targeted attack"
      methods:
        - "Test case folding behavior"
        - "Check collation order"
        - "Detect language-specific normalization"
      behavior_change: "Use locale-specific confusables"
    
    - technique: "editor_detection"
      description: "Detect text editor or viewer"
      methods:
        - "Test bidi rendering behavior"
        - "Check invisible character display"
        - "Detect syntax highlighting patterns"
      behavior_change: "Adjust attacks to evade specific editor"
  
  anti_debugging_measures:
    - measure: "display_aware"
      description: "Text appears different when copied/inspected"
      implementation: "Use characters that change appearance in different contexts"
      
    - measure: "length_deception"
      description: "Visual length differs from byte/codepoint length"
      implementation: "Use zero-width or combining characters"
      
    - measure: "clipboard_mutation"
      description: "Text changes when copied to clipboard"
      implementation: "Use characters with copy/paste transformation"
  
  anti_tracing_mechanisms:
    - mechanism: "rendering_variance"
      description: "Text renders differently in analysis tools"
      implementation: "Use characters with tool-specific rendering"
      
    - mechanism: "normalization_evasion"
      description: "Bypass normalization-based detection"
      implementation: "Use characters that resist normalization"
      
    - mechanism: "script_detection_evasion"
      description: "Evade mixed-script detection"
      implementation: "Use characters from Common/Inherited scripts"
  
  time_based_anti_analysis:
    - technique: "font_loading_delay"
      description: "Different rendering before/after font load"
      implementation: "Use characters from slow-loading fonts"
      
    - technique: "locale_change_sensitivity"
      description: "Text meaning changes with locale changes"
      implementation: "Use locale-sensitive characters"
      
    - technique: "version_dependent"
      description: "Different behavior in different Unicode versions"
      implementation: "Use characters with changed properties"

# === PSYCHOLOGICAL MISDIRECTION ===
psychological_misdirection:
  obvious_false_positives:
    description: "Intentional red herrings to waste analyst time"
    examples:
      - name: "suspicious_cyrillic_comment"
        description: "Obvious Cyrillic text that is actually legitimate"
        time_waste: "15-30 minutes"
        
      - name: "visible_bidi_marker"
        description: "Bidi characters that are properly handled"
        time_waste: "20-40 minutes"
        
      - name: "emoji_sequences"
        description: "Complex emoji that are visually harmless"
        time_waste: "30-60 minutes"
  
  misleading_comments:
    description: "Comments that direct attention away from real tricks"
    examples:
      - "# Unicode normalized to NFKC for safety"
      - "// All non-ASCII characters stripped"
      - "/* Bidi characters removed by sanitizer */"
      - "// Confusables checked by security library"
  
  misleading_variable_names:
    description: "Variable names that hide Unicode issues"
    examples:
      - "ascii_only_string - actually contains look-alike Unicode"
      - "normalized_input - pre-normalization copy used elsewhere"
      - "safe_username - contains invisible characters"
      - "validated_domain - punycode not fully decoded"
  
  red_herring_code_paths:
    description: "Code paths that appear to handle Unicode but miss tricks"
    examples:
      - name: "incomplete_sanitizer"
        description: "Sanitizer that removes some but not all dangerous characters"
        
      - name: "wrong_normalization"
        description: "Uses NFC when NFKC needed, or vice versa"
        
      - name: "partial_script_check"
        description: "Checks some scripts but misses others"
  
  partial_solutions:
    description: "Solutions that work initially but enable harder attacks"
    examples:
      - name: "homoglyph_blocklist"
        description: "Blocking known confusables enables unknown ones"
        
      - name: "bidi_stripping"
        description: "Stripping bidi controls breaks legitimate RTL text"
        
      - name: "nfkc_normalization"
        description: "NFKC changes some characters, enabling new attacks"

# === VERSION HISTORY ===
version_history:
  - version: "1.0.0"
    date: "2024-01-01"
    changes: "Initial Unicode tricks"
  - version: "2.0.0"
    date: "2024-06-01"
    changes: "Expanded to 1000+ lines with comprehensive Unicode attacks"
  - version: "3.0.0"
    date: "2024-12-01"
    changes: "Enhanced difficulty with nightmare mode and comprehensive anti-patterns"
