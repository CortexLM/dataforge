id: "sys-dist-split-brain-001"
version: "1.0.0"
category: "systems"
subcategory: "distributed-systems"

# SWE-bench_Pro style fields
problem_statement: |
  A distributed database cluster experienced a network partition and is now in a 
  split-brain state. Both partitions believe they are the primary and are accepting 
  writes, leading to data inconsistency.
  
  Complications:
  1. Quorum mechanism was incorrectly configured
  2. Fencing mechanism failed to prevent dual-primary
  3. Network partition healed but data diverged
  4. Clients cached stale cluster topology

requirements: |
  - Detect split-brain condition
  - Implement proper quorum-based leader election
  - Configure fencing mechanisms (STONITH)
  - Merge diverged data after partition heals
  - Update client topology cache handling

interface: |
  Input: Cluster state, node configurations, diverged data
  Output: Recovered consistent cluster, merged data, prevention measures
  Monitoring: Cluster health, partition detection

# terminal-bench style fields
difficulty:
  estimated: "hard"
  time_range: [2700, 8100]
  command_steps: [40, 100]

# === QUALITY REQUIREMENTS ===
quality_requirements:
  minimum_time: "120-300 minutes for senior systems engineers with kernel contribution experience"
  expertise_level: "Principal distributed systems engineer with HA cluster and fencing expertise"
  validation_standard: "Must expose subtle split-brain scenarios, quorum edge cases, and data divergence"

# === MULTI-AGENT ORCHESTRATION ===
multi_agent_orchestration:
  required_agents: 10
  agent_specializations:
    - name: "quorum_analyst"
      role: "Debug quorum configuration and calculation"
      expertise: ["majority quorum", "weighted voting", "witness nodes", "tiebreakers"]
    - name: "fencing_expert"
      role: "Debug STONITH and fencing mechanisms"
      expertise: ["IPMI fencing", "SBD", "fabric fencing", "self-fencing"]
    - name: "partition_detector"
      role: "Detect and analyze network partitions"
      expertise: ["symmetric vs asymmetric", "partial partitions", "connectivity graphs"]
    - name: "conflict_resolver"
      role: "Debug data conflict resolution strategies"
      expertise: ["LWW", "vector clocks", "CRDTs", "merge functions"]
    - name: "topology_manager"
      role: "Debug cluster topology and client routing"
      expertise: ["topology cache", "failover", "client redirect", "leader discovery"]
    - name: "clock_skew_analyst"
      role: "Debug clock-related split-brain issues"
      expertise: ["leader lease", "clock bounds", "bounded uncertainty", "fencing tokens"]
    - name: "recovery_specialist"
      role: "Debug split-brain recovery procedures"
      expertise: ["data reconciliation", "manual intervention", "anti-entropy"]
    - name: "network_partition_injector"
      role: "Test behavior under various partition scenarios"
      expertise: ["iptables", "tc netem", "chaos engineering", "Jepsen nemesis"]
    - name: "ha_cluster_expert"
      role: "Debug Pacemaker/Corosync cluster issues"
      expertise: ["resource agents", "colocation", "ordering", "node attributes"]
    - name: "geo_replication_specialist"
      role: "Debug cross-datacenter split-brain"
      expertise: ["async replication", "conflict-free types", "eventual consistency"]
  cross_subsystem_chains:
    - "Network partition → Quorum miscalculation → Dual primaries → Data divergence"
    - "Fencing failure → Node not killed → Split write → Permanent inconsistency"
    - "Clock skew → Lease appears valid → Stale primary writes → Corruption"
    - "Asymmetric partition → One-way heartbeat → False failure detection → Premature failover"
  parallel_debugging_requirements:
    - "Correlated network connectivity status across all nodes"
    - "Simultaneous fencing action and data write analysis"
    - "Agent handoff for complex multi-partition scenarios"

# === TRAP CONFIGURATION ===
trap_configuration:
  trap_count: "12+ deeply interacting traps across kernel/userspace/hardware boundaries"
  trap_categories:
    quorum_traps:
      - "Quorum set to n/2 instead of n/2+1 allowing dual primaries"
      - "Even node count without witness causing deadlock"
      - "Weighted voting allowing minority to have quorum"
      - "Witness participating in data operations incorrectly"
    fencing_traps:
      - "STONITH disabled or misconfigured in production"
      - "Fencing agent timeout before completion"
      - "Fencing loop between nodes"
      - "Self-fencing failure when isolated"
    partition_traps:
      - "Asymmetric partition not detected"
      - "Partial connectivity creating multiple minorities"
      - "Partition healing race condition"
      - "DNS resolution failure causing false partition"

# === NIGHTMARE_PLUS DIFFICULTY ===
difficulty_levels:
  nightmare_plus:
    estimated_time: [43200, 259200]  # 12-72 hours
    command_steps: [600, 2500]
    techniques_required: 18
    description: "Production HA cluster incident-level difficulty requiring deep operational expertise"
    requirements:
      - "Requires understanding of all split-brain prevention mechanisms"
      - "Must handle complex multi-datacenter partition scenarios"
      - "Involves implementing safe data reconciliation strategies"
      - "Cross-platform behavior differences (Pacemaker vs proprietary)"
      - "Time estimate: 12-72 hours for HA cluster consultants"
      - "Requires synthesizing networking, storage, and distributed systems knowledge"

# === KERNEL SUBSYSTEM REQUIREMENTS ===
kernel_subsystem_requirements:
  networking_subsystem:
    - "Network namespace isolation effects"
    - "Bonding and team driver failure detection"
    - "VLAN and bridge partition behavior"
    - "TCP keepalive and connection timeout"
  storage_subsystem:
    - "Shared storage fencing mechanisms"
    - "SCSI reservations and fencing"
    - "Multipath failover behavior"
    - "iSCSI session timeout effects"
  process_management:
    - "Watchdog timer integration"
    - "Resource agent lifecycle"
    - "Process monitoring and restart"
    - "Signal handling in HA services"
  timing_subsystem:
    - "Corosync token timeout"
    - "Heartbeat interval tuning"
    - "Failure detection timing"
    - "Quorum decision timing"

# === HARDWARE INTERACTION ===
hardware_interaction:
  fencing_hardware:
    - "IPMI/iLO/iDRAC out-of-band management"
    - "PDU (Power Distribution Unit) fencing"
    - "SAN fabric fencing (zoning)"
    - "KVM switch fencing"
  network_hardware:
    - "Switch failure modes (spanning tree)"
    - "Router failure and convergence"
    - "NIC failure detection (carrier loss)"
    - "Network interface bonding"
  storage_hardware:
    - "Shared LUN behavior under partition"
    - "SBD (Storage Based Death) device"
    - "Quorum disk implementation"
    - "Storage controller failover"

# === FORMAL VERIFICATION REQUIREMENTS ===
formal_verification_requirements:
  modeling_requirements:
    - "Model quorum calculation in TLA+"
    - "Express fencing requirements formally"
    - "Verify split-brain impossibility"
    - "Model partition healing protocol"
  proof_obligations:
    - "Prove at most one primary at any time"
    - "Verify fencing completes before failover"
    - "Establish data consistency after recovery"
    - "Prove quorum intersection property"

# === LLM GENERATION FRAMEWORK ===
generation_framework:
  multi_conversation_workflow:
    phase_1_research: "Research obscure distributed systems behaviors and failure modes"
    phase_2_creation: "Create task with subtle split-brain scenarios and timing issues"
    phase_3_amplification: "Add network partition timing dependencies and quorum edge cases"
    phase_4_verification:
      description: "Validate task requires deep understanding of consensus and fencing"
      criteria:
        - "Has at least 5 interacting system-level traps across subsystems"
        - "Has cascading failures across process, memory, and I/O subsystems"
        - "Requires knowledge of OS kernel internals and scheduler behavior"
        - "Would take experienced systems programmers 45+ minutes"
  
  difficulty_amplifiers:
    nightmare:
      multiplier: 3.5
      description: "Kernel-level debugging complexity requiring deep systems expertise"
      requirements:
        - "7+ interacting system-level traps across subsystems"
        - "Requires understanding of kernel memory management and scheduling"
        - "Time estimate: 120+ minutes for senior systems engineers"
        - "Cross-platform behavior differences (Linux vs BSD vs Windows)"
        - "Requires synthesizing concurrency, memory, distributed systems knowledge"
  
  generation_targets:
    minimum_difficulty: "45+ minutes, requires deep kernel internals and distributed systems expertise"
  
  complexity_levels:
    level_1_basic:
      description: "Simple quorum misconfiguration with clear symptoms"
      elements: ["even_node_count", "missing_tiebreaker", "basic_data_divergence"]
    level_2_intermediate:
      description: "Partial network partitions with asymmetric visibility"
      elements: ["asymmetric_partition", "stale_topology_cache", "multiple_leaders"]
    level_3_advanced:
      description: "Byzantine failures mixed with network partitions"
      elements: ["byzantine_node", "message_corruption", "clock_skew_impact"]
    level_4_expert:
      description: "Multi-datacenter split-brain with complex recovery"
      elements: ["geo_partition", "wan_latency", "crdt_conflict_resolution"]
    level_5_research:
      description: "Novel split-brain scenarios in emerging systems"
      elements: ["serverless_partition", "container_orchestration", "service_mesh_split"]

# === COMPREHENSIVE TOPIC UNIVERSE ===
topic_universe:
  # Core Consensus Protocols (30+ topics)
  consensus_protocols:
    paxos_family:
      - "Classic Paxos with single decree consensus"
      - "Multi-Paxos with stable leader optimization"
      - "Fast Paxos with reduced round trips"
      - "Cheap Paxos with auxiliary nodes"
      - "Generalized Paxos with commutative operations"
      - "Vertical Paxos with reconfiguration"
      - "Egalitarian Paxos (EPaxos) with conflict detection"
      - "Flexible Paxos with variable quorums"
      - "WPaxos with wide-area deployment"
      - "CASPaxos for CAS-based registers"
    
    raft_family:
      - "Basic Raft with leader election"
      - "Multi-Raft with sharded consensus"
      - "Parallel Raft with out-of-order commits"
      - "Raft with prevote extension"
      - "Joint consensus membership changes"
      - "Raft learners for read scaling"
      - "Raft snapshots and log compaction"
      - "Raft witness nodes for odd quorum"
      - "Tangaroa: Byzantine fault tolerant Raft"
      - "Raft with lease-based reads"
    
    viewstamped_replication:
      - "View changes and primary election"
      - "State transfer during recovery"
      - "Garbage collection of old views"
      - "Optimized view change protocol"
    
    pbft_family:
      - "PBFT with 3f+1 fault tolerance"
      - "Speculative BFT optimizations"
      - "SBFT with trusted hardware"
      - "HotStuff with linear view change"
      - "RBFT with robust performance"
    
    zab_protocol:
      - "ZooKeeper Atomic Broadcast"
      - "Discovery phase in Zab"
      - "Synchronization phase"
      - "Broadcast phase operations"
  
  # Failure Detectors (20+ topics)
  failure_detectors:
    theoretical_models:
      - "Perfect failure detector (P)"
      - "Eventually perfect detector (◇P)"
      - "Strong completeness guarantees"
      - "Eventual strong accuracy"
      - "Weak failure detectors (W)"
      - "Eventually weak detector (◇W)"
    
    practical_implementations:
      - "Phi Accrual failure detector"
      - "Adaptive timeout calculation"
      - "Heartbeat-based detection"
      - "Gossip-based failure propagation"
      - "SWIM protocol for membership"
      - "Indirect probing strategies"
      - "Suspicion mechanism with exponential backoff"
    
    challenges:
      - "False positive handling during GC pauses"
      - "Network congestion vs node failure"
      - "Asymmetric network failures"
      - "Gray failures and partial outages"
      - "Byzantine failure detection limits"
  
  # Network Partitions (25+ topics)
  network_partitions:
    partition_types:
      - "Symmetric partitions (mutual isolation)"
      - "Asymmetric partitions (one-way communication)"
      - "Partial partitions (subset connectivity)"
      - "Transient partitions (flapping)"
      - "Cascading partitions from switch failures"
      - "Datacenter-level partitions"
      - "Rack-level partitions"
      - "Pod/availability zone partitions"
    
    detection_methods:
      - "Heartbeat timeout detection"
      - "Quorum-based partition detection"
      - "External witness verification"
      - "Multi-path connectivity probing"
      - "Leader lease expiration"
    
    recovery_strategies:
      - "Automatic partition healing"
      - "Manual operator intervention"
      - "Gradual reconnection protocols"
      - "Data reconciliation after healing"
      - "Version vector merging"
      - "Conflict resolution callbacks"
  
  # Split-Brain Resolution (30+ topics)
  split_brain_resolution:
    fencing_mechanisms:
      - "STONITH (Shoot The Other Node In The Head)"
      - "Power fencing via IPMI/iLO"
      - "SBD (Storage Based Death)"
      - "Fabric fencing for SANs"
      - "Network isolation fencing"
      - "Virtual machine fencing"
      - "Container orchestration fencing"
      - "Cloud provider API fencing"
    
    quorum_strategies:
      - "Majority quorum (n/2+1)"
      - "Weighted quorum voting"
      - "Witness/tiebreaker nodes"
      - "External arbitration services"
      - "Last-man-standing with delay"
      - "Quorum disk/partition"
      - "Resource-based quorum"
    
    leader_lease_mechanisms:
      - "Time-based leader leases"
      - "Lease renewal protocols"
      - "Lease expiration handling"
      - "Clock-bound leader validity"
      - "Hybrid logical clock leases"
    
    data_reconciliation:
      - "Last-write-wins (LWW)"
      - "First-write-wins"
      - "Merge functions for CRDTs"
      - "Application-level conflict resolution"
      - "Tombstone propagation"
      - "Anti-entropy repair"
  
  # Logical Clocks (20+ topics)
  logical_clocks:
    lamport_clocks:
      - "Scalar Lamport timestamps"
      - "Happened-before relation"
      - "Concurrent event detection limitations"
    
    vector_clocks:
      - "Full vector clock implementation"
      - "Version vectors for replicas"
      - "Dotted version vectors"
      - "Compressed vector clocks"
      - "Interval tree clocks"
    
    matrix_clocks:
      - "Knowledge tracking matrices"
      - "Garbage collection with matrix clocks"
    
    hybrid_logical_clocks:
      - "HLC combining physical and logical"
      - "Clock skew tolerance"
      - "Bounded drift guarantees"
      - "HLC for distributed snapshots"
    
    truetime_spanner:
      - "TrueTime API uncertainties"
      - "Commit wait for external consistency"
      - "Clock synchronization requirements"
  
  # Distributed Snapshots (15+ topics)
  distributed_snapshots:
    chandy_lamport:
      - "Marker-based snapshot algorithm"
      - "Channel state recording"
      - "Snapshot consistency guarantees"
      - "Concurrent snapshot handling"
    
    variations:
      - "Optimized snapshot for FIFO channels"
      - "Non-FIFO channel adaptations"
      - "Incremental snapshots"
      - "Fuzzy snapshots"
    
    applications:
      - "Distributed debugging"
      - "Checkpointing for recovery"
      - "Deadlock detection via snapshots"
      - "Termination detection"
  
  # Consistent Hashing (15+ topics)
  consistent_hashing:
    basic_mechanisms:
      - "Ring-based consistent hashing"
      - "Virtual nodes for load balancing"
      - "Bounded-load consistent hashing"
      - "Jump consistent hashing"
      - "Rendezvous/HRW hashing"
      - "Maglev consistent hashing"
    
    rebalancing:
      - "Minimal data movement guarantees"
      - "Gradual rebalancing strategies"
      - "Hot spot mitigation"
      - "Token ranges and splits"
    
    integration:
      - "Consistent hashing in Dynamo"
      - "Cassandra token ring"
      - "Redis Cluster slots"
  
  # Gossip Protocols (20+ topics)
  gossip_protocols:
    epidemic_protocols:
      - "Push gossip dissemination"
      - "Pull gossip for convergence"
      - "Push-pull hybrid protocols"
      - "Anti-entropy sessions"
    
    membership_gossip:
      - "SWIM failure detection"
      - "HyParView for overlay networks"
      - "Plumtree for broadcast trees"
      - "T-MAN topology management"
    
    optimizations:
      - "Bloom filter optimizations"
      - "Rumor mongering termination"
      - "Biased gossip peer selection"
      - "Adaptive fanout strategies"
    
    applications:
      - "Database anti-entropy (Cassandra)"
      - "Cluster membership (Serf)"
      - "Configuration propagation"
      - "Aggregate computation"
  
  # CRDTs (25+ topics)
  crdts:
    counters:
      - "G-Counter (grow-only counter)"
      - "PN-Counter (positive-negative counter)"
      - "Bounded counter"
      - "State-based counters"
      - "Op-based counters"
    
    sets:
      - "G-Set (grow-only set)"
      - "2P-Set (two-phase set)"
      - "LWW-Element-Set"
      - "OR-Set (observed-remove set)"
      - "Add-wins set"
      - "Remove-wins set"
    
    registers:
      - "LWW-Register"
      - "MV-Register (multi-value)"
    
    sequences:
      - "RGA (Replicated Growable Array)"
      - "LSEQ for collaborative editing"
      - "Logoot for document editing"
      - "WOOT algorithm"
    
    maps_and_graphs:
      - "OR-Map"
      - "Add-only monotonic DAG"
      - "2P2P-Graph"
    
    composition:
      - "Nesting CRDTs"
      - "Delta-state CRDTs"
      - "Pure operation-based CRDTs"
  
  # Distributed Transactions (25+ topics)
  distributed_transactions:
    two_phase_commit:
      - "2PC coordinator role"
      - "Participant voting phase"
      - "Commit/abort decision"
      - "Blocking nature of 2PC"
      - "Timeout handling in 2PC"
      - "Recovery after coordinator crash"
    
    three_phase_commit:
      - "Pre-commit phase addition"
      - "Non-blocking guarantees"
      - "3PC timeout scenarios"
    
    saga_pattern:
      - "Choreography-based sagas"
      - "Orchestration-based sagas"
      - "Compensating transactions"
      - "Semantic rollback"
      - "Saga execution coordinator"
    
    tcc_pattern:
      - "Try-Confirm-Cancel lifecycle"
      - "Reservation-based consistency"
      - "TCC timeout handling"
    
    optimizations:
      - "Presumed abort optimization"
      - "Presumed commit optimization"
      - "Early prepare optimization"
      - "Single-phase commit"
  
  # Replication Protocols (20+ topics)
  replication_protocols:
    chain_replication:
      - "Chain replication topology"
      - "Head and tail roles"
      - "Chain reconfiguration"
      - "CRAQ for strong consistency reads"
    
    primary_backup:
      - "Synchronous replication"
      - "Asynchronous replication"
      - "Semi-synchronous replication"
      - "Quorum writes"
    
    state_machine_replication:
      - "Deterministic execution requirements"
      - "Command ordering guarantees"
      - "Handling non-determinism"
      - "Checkpoint and recovery"
    
    leaderless_replication:
      - "Sloppy quorum"
      - "Hinted handoff"
      - "Read repair"
      - "Merkle tree anti-entropy"
  
  # Leader Election (15+ topics)
  leader_election:
    algorithms:
      - "Bully algorithm"
      - "Ring algorithm"
      - "Raft leader election"
      - "Paxos-based election"
    
    optimizations:
      - "Pre-vote mechanism"
      - "Priority-based election"
      - "Check quorum for liveness"
      - "Transfer leadership"
    
    challenges:
      - "Split vote prevention"
      - "Network partition handling"
      - "Byzantine leader election"
  
  # Distributed Locking (15+ topics)
  distributed_locking:
    services:
      - "Chubby lock service"
      - "ZooKeeper recipes"
      - "etcd locking"
      - "Consul sessions"
      - "Redis Redlock algorithm"
    
    patterns:
      - "Ephemeral lock nodes"
      - "Sequence nodes for fairness"
      - "Lease-based locks"
      - "Fencing tokens"
    
    challenges:
      - "Lock holder crash detection"
      - "Clock-based lock expiry risks"
      - "Redlock controversy"

# === BUG PATTERNS ===
bug_patterns:
  quorum_configuration:
    - pattern: "quorum_too_small"
      description: "Quorum set to n/2 instead of n/2+1 allowing dual primaries"
      severity: "critical"
      detection: "Two nodes simultaneously claiming leadership"
      fix: "Require strict majority: quorum = floor(n/2) + 1"
    
    - pattern: "even_node_count_no_tiebreaker"
      description: "Even number of nodes without witness/tiebreaker"
      severity: "high"
      detection: "Cluster hangs on partition"
      fix: "Add odd witness node or use external arbitrator"
    
    - pattern: "witness_misconfiguration"
      description: "Witness node participates in data but shouldn't"
      severity: "medium"
      detection: "Data inconsistency on witness"
      fix: "Configure witness as non-voting for data"
    
    - pattern: "weighted_quorum_imbalance"
      description: "Weight distribution allows minority to have quorum"
      severity: "critical"
      detection: "Split brain with weighted voting"
      fix: "Ensure no subset < majority can achieve quorum"

  fencing_failures:
    - pattern: "stonith_disabled_in_production"
      description: "Fencing disabled for 'convenience' in production"
      severity: "critical"
      detection: "No fencing actions in logs during partition"
      fix: "Mandate fencing in production clusters"
    
    - pattern: "fencing_agent_timeout"
      description: "Fencing agent times out before completing"
      severity: "high"
      detection: "Fencing started but not completed"
      fix: "Tune timeouts, add backup fencing methods"
    
    - pattern: "fencing_loop"
      description: "Nodes fence each other in a loop"
      severity: "critical"
      detection: "Repeated power cycles of all nodes"
      fix: "Implement fencing delay/backoff"
    
    - pattern: "self_fencing_failure"
      description: "Node can't fence itself when isolated"
      severity: "medium"
      detection: "Isolated node continues operating"
      fix: "Implement suicide on loss of quorum"

  network_partition_handling:
    - pattern: "asymmetric_partition_not_detected"
      description: "One-way network failure not recognized"
      severity: "high"
      detection: "Node receives but can't send, still acts as leader"
      fix: "Require bidirectional heartbeats"
    
    - pattern: "partition_detection_too_slow"
      description: "Large timeout delays partition detection"
      severity: "medium"
      detection: "Extended period of split-brain"
      fix: "Tune detection timeouts appropriately"
    
    - pattern: "partial_partition_corner_case"
      description: "Some nodes connected, others not, complex topology"
      severity: "high"
      detection: "Inconsistent cluster view across nodes"
      fix: "Implement proper graph-based connectivity analysis"
    
    - pattern: "healing_partition_race"
      description: "Race condition during partition healing"
      severity: "medium"
      detection: "Brief inconsistency during reconnection"
      fix: "Implement orderly reconnection protocol"

  data_divergence:
    - pattern: "no_conflict_resolution_strategy"
      description: "No defined behavior when data conflicts"
      severity: "critical"
      detection: "Random/undefined conflict resolution"
      fix: "Define explicit conflict resolution policy"
    
    - pattern: "timestamp_based_lww_clock_skew"
      description: "LWW with skewed clocks causes wrong winner"
      severity: "high"
      detection: "Stale data wins due to clock ahead"
      fix: "Use logical clocks or hybrid timestamps"
    
    - pattern: "tombstone_not_replicated"
      description: "Delete marker not propagated during partition"
      severity: "medium"
      detection: "Deleted data reappears after healing"
      fix: "Ensure tombstones propagate during anti-entropy"
    
    - pattern: "vector_clock_overflow"
      description: "Vector clock entries grow unbounded"
      severity: "low"
      detection: "Memory growth in version vectors"
      fix: "Implement clock pruning with causality preservation"

  client_behavior:
    - pattern: "stale_topology_cache"
      description: "Client caches old cluster topology"
      severity: "high"
      detection: "Client writes to old primary"
      fix: "Implement topology refresh on errors"
    
    - pattern: "no_fencing_token"
      description: "Lock holder doesn't use fencing token"
      severity: "critical"
      detection: "Stale lock holder corrupts data"
      fix: "Require fencing tokens for all writes"
    
    - pattern: "retry_amplification"
      description: "Client retries amplify during partition"
      severity: "medium"
      detection: "Request storms during partition"
      fix: "Implement exponential backoff with jitter"

  timing_issues:
    - pattern: "leader_lease_clock_skew"
      description: "Leader lease valid on slow clock, expired on others"
      severity: "critical"
      detection: "Two leaders within lease period"
      fix: "Account for clock drift in lease duration"
    
    - pattern: "election_timeout_too_similar"
      description: "Election timeouts not sufficiently randomized"
      severity: "medium"
      detection: "Repeated split votes"
      fix: "Use wide random range for election timeout"
    
    - pattern: "gc_pause_false_failure"
      description: "GC pause triggers false failure detection"
      severity: "medium"
      detection: "Healthy node marked failed during GC"
      fix: "Use Phi Accrual detector with adaptive thresholds"

# === EDGE CASES ===
edge_cases:
  cluster_topology:
    - "Single node cluster (degenerate case)"
    - "Two node cluster (no quorum possible on partition)"
    - "Three node cluster (minimum for quorum)"
    - "Even number of nodes (requires tiebreaker)"
    - "Heterogeneous nodes (different capabilities)"
    - "Geo-distributed nodes (high latency)"
    - "Witness-only nodes (non-data)"
    - "Learner nodes during membership change"
    - "Maximum cluster size limits"
    - "Minimum safe cluster size"

  partition_scenarios:
    - "Complete network isolation"
    - "One-way communication failure"
    - "Partial mesh connectivity"
    - "Cascading switch failures"
    - "DNS resolution failures"
    - "TLS certificate expiry during partition"
    - "Firewall rule changes"
    - "IP address changes"
    - "Port exhaustion"
    - "MTU mismatch causing packet loss"
    - "Network interface flapping"
    - "BGP route withdrawal"

  timing_edge_cases:
    - "Clock jump forward (NTP step)"
    - "Clock jump backward (should not happen but does)"
    - "Leap second handling"
    - "Daylight saving transitions"
    - "Time zone changes"
    - "NTP server unreachable"
    - "Hardware clock drift"
    - "Virtual machine clock drift after suspend"
    - "Container clock namespace issues"
    - "Maximum clock skew tolerance"

  resource_exhaustion:
    - "Memory pressure causing slow responses"
    - "CPU saturation causing timeouts"
    - "Disk I/O causing checkpoint delays"
    - "Network bandwidth saturation"
    - "Connection pool exhaustion"
    - "File descriptor limits"
    - "Thread pool exhaustion"
    - "Log file fills disk"

  recovery_edge_cases:
    - "Simultaneous recovery of multiple nodes"
    - "Recovery during active partition"
    - "Recovery with corrupted state"
    - "Recovery with version mismatch"
    - "Partial recovery (some data lost)"
    - "Recovery ordering dependencies"
    - "Recovery timeout"
    - "Recovery with configuration drift"

  membership_changes:
    - "Adding node during partition"
    - "Removing node during partition"
    - "Multiple simultaneous membership changes"
    - "Membership change with pending writes"
    - "Rolling upgrade during partition"
    - "Configuration change races"

  data_edge_cases:
    - "Empty cluster initial state"
    - "Maximum data size per key"
    - "Maximum number of keys"
    - "Binary vs text data"
    - "Unicode handling in keys"
    - "Null values vs missing keys"
    - "Maximum write rate"
    - "Maximum concurrent transactions"
    - "Cross-shard transactions"

# === DIFFICULTY MULTIPLIERS ===
difficulty_multipliers:
  architecture_requirements:
    multi_datacenter:
      multiplier: 2.0
      description: "Task requires handling geo-distributed deployment"
      considerations:
        - "WAN latency effects on consensus"
        - "Datacenter-level failure modes"
        - "Cross-datacenter quorum strategies"
        - "Regional data residency requirements"
    
    hybrid_cloud:
      multiplier: 1.8
      description: "Mixed on-premises and cloud deployment"
      considerations:
        - "Heterogeneous network characteristics"
        - "Cloud API integration for fencing"
        - "Cost-aware replication"
        - "Data sovereignty compliance"
    
    containerized:
      multiplier: 1.5
      description: "Kubernetes or similar orchestration"
      considerations:
        - "Pod IP changes"
        - "StatefulSet guarantees"
        - "PersistentVolume handling"
        - "Network policy effects"

  consistency_requirements:
    linearizability:
      multiplier: 2.0
      description: "Strongest consistency guarantee required"
      considerations:
        - "Read-your-writes within session"
        - "Real-time ordering across all clients"
        - "Performance implications"
    
    serializability:
      multiplier: 1.8
      description: "Transaction isolation requirement"
      considerations:
        - "Conflict detection and resolution"
        - "Deadlock prevention"
        - "Long transaction handling"
    
    causal_consistency:
      multiplier: 1.5
      description: "Causal ordering must be preserved"
      considerations:
        - "Dependency tracking"
        - "Vector clock overhead"
        - "Session guarantees"

  fault_tolerance_requirements:
    byzantine_tolerance:
      multiplier: 2.5
      description: "Must handle malicious/arbitrary failures"
      considerations:
        - "3f+1 node requirement"
        - "Cryptographic verification"
        - "Computational overhead"
    
    f_plus_one_tolerance:
      multiplier: 1.5
      description: "Tolerate f+1 simultaneous failures"
      considerations:
        - "Larger cluster requirements"
        - "Recovery complexity"
        - "Cost implications"

  performance_requirements:
    low_latency:
      multiplier: 1.5
      description: "Sub-millisecond operation latency required"
      considerations:
        - "Consensus protocol choice"
        - "Batching strategies"
        - "Read optimization"
    
    high_throughput:
      multiplier: 1.5
      description: "Millions of operations per second"
      considerations:
        - "Sharding strategy"
        - "Parallel replication"
        - "Batching and pipelining"

# === SCENARIO TEMPLATES ===
scenario_templates:
  basic_split_brain:
    description: "Classic two-partition split-brain"
    setup: |
      - 5 node cluster
      - Network partition creates 2+3 split
      - Both partitions attempt to elect leader
    expected_behavior: |
      - 3-node partition maintains quorum
      - 2-node partition should step down
      - Fencing should prevent dual writes
    common_mistakes:
      - "Not configuring proper quorum"
      - "Fencing disabled or misconfigured"
      - "No client-side retry with backoff"

  asymmetric_partition:
    description: "One-way network failure"
    setup: |
      - 3 node cluster A, B, C
      - A can reach B, B can reach C, but A cannot reach C
      - C cannot reach A
    expected_behavior: |
      - Should not result in dual leaders
      - Must detect asymmetric connectivity
      - May require external witness
    common_mistakes:
      - "Only checking outbound connectivity"
      - "Not implementing bidirectional heartbeats"
      - "Trusting unidirectional success"

  rolling_partition:
    description: "Sequential node isolation"
    setup: |
      - 5 node cluster
      - Nodes isolated one by one
      - Later nodes reconnect while earlier stay isolated
    expected_behavior: |
      - Cluster maintains availability until quorum lost
      - No data loss during transitions
      - Clean recovery as nodes rejoin
    common_mistakes:
      - "Not handling rapid membership changes"
      - "Log replication gaps"
      - "Snapshot timing issues"

  geo_distributed_partition:
    description: "Multi-datacenter partition"
    setup: |
      - 3 datacenter deployment (2+2+1)
      - WAN link failure isolates one datacenter
      - High latency on remaining links
    expected_behavior: |
      - System remains available if majority DCs connected
      - Latency-aware leader election
      - Cross-DC replication handles correctly
    common_mistakes:
      - "Timeout tuning for WAN"
      - "Quorum calculation across DCs"
      - "Witness placement"

  byzantine_node_during_partition:
    description: "Malicious node exploiting partition"
    setup: |
      - 4 node cluster (3f+1 for f=1)
      - One node acting maliciously
      - Network partition occurs
    expected_behavior: |
      - Byzantine node cannot cause inconsistency
      - Partition + Byzantine handled correctly
      - System remains safe if not live
    common_mistakes:
      - "Not using BFT protocol"
      - "Insufficient nodes for Byzantine tolerance"
      - "Trusting unsigned messages"

# === REFERENCE SOLUTION ===
reference_solution: |
  #!/usr/bin/env python3
  """
  Comprehensive split-brain detection and resolution implementation.
  Handles quorum configuration, fencing, and data reconciliation.
  """
  import time
  import hashlib
  import json
  import threading
  import socket
  import struct
  from typing import Dict, List, Set, Tuple, Optional, Callable, Any
  from dataclasses import dataclass, field
  from enum import Enum, auto
  from collections import defaultdict
  from abc import ABC, abstractmethod
  import logging
  
  logging.basicConfig(level=logging.INFO)
  logger = logging.getLogger(__name__)
  
  # === CORE TYPES ===
  
  class NodeState(Enum):
      LEADER = auto()
      FOLLOWER = auto()
      CANDIDATE = auto()
      FENCED = auto()
      RECOVERING = auto()
      WITNESS = auto()
  
  class PartitionType(Enum):
      NONE = auto()
      SYMMETRIC = auto()
      ASYMMETRIC = auto()
      PARTIAL = auto()
      TRANSIENT = auto()
  
  class FencingResult(Enum):
      SUCCESS = auto()
      FAILED = auto()
      TIMEOUT = auto()
      ALREADY_FENCED = auto()
      SELF_FENCED = auto()
  
  @dataclass(frozen=True)
  class NodeId:
      """Unique identifier for a cluster node."""
      id: str
      datacenter: str = "default"
      rack: str = "default"
      
      def __str__(self) -> str:
          return f"{self.datacenter}/{self.rack}/{self.id}"
  
  @dataclass
  class HybridTimestamp:
      """Hybrid Logical Clock timestamp for ordering."""
      physical: int  # Wall clock in microseconds
      logical: int   # Logical counter
      node_id: str   # Node identifier for tie-breaking
      
      def __lt__(self, other: 'HybridTimestamp') -> bool:
          if self.physical != other.physical:
              return self.physical < other.physical
          if self.logical != other.logical:
              return self.logical < other.logical
          return self.node_id < other.node_id
      
      def __le__(self, other: 'HybridTimestamp') -> bool:
          return self == other or self < other
      
      def __eq__(self, other: object) -> bool:
          if not isinstance(other, HybridTimestamp):
              return NotImplemented
          return (self.physical == other.physical and 
                  self.logical == other.logical and
                  self.node_id == other.node_id)
      
      def __hash__(self) -> int:
          return hash((self.physical, self.logical, self.node_id))
  
  @dataclass
  class ClusterNode:
      """Represents a node in the cluster."""
      node_id: NodeId
      address: str
      port: int
      state: NodeState = NodeState.FOLLOWER
      term: int = 0
      voted_for: Optional[NodeId] = None
      last_heartbeat: float = 0.0
      data: Dict[str, Any] = field(default_factory=dict)
      version_vector: Dict[str, int] = field(default_factory=dict)
      is_witness: bool = False
      weight: int = 1
      
      def update_heartbeat(self) -> None:
          self.last_heartbeat = time.time()
  
  @dataclass
  class WriteEntry:
      """Represents a write operation with metadata."""
      key: str
      value: Any
      timestamp: HybridTimestamp
      term: int
      node_id: NodeId
      vector_clock: Dict[str, int] = field(default_factory=dict)
      tombstone: bool = False
      checksum: str = ""
      
      def __post_init__(self):
          if not self.checksum:
              self.checksum = self._compute_checksum()
      
      def _compute_checksum(self) -> str:
          data = f"{self.key}:{self.value}:{self.timestamp}:{self.term}"
          return hashlib.sha256(data.encode()).hexdigest()[:16]
      
      def verify(self) -> bool:
          return self.checksum == self._compute_checksum()
  
  # === QUORUM MANAGER ===
  
  class QuorumManager:
      """
      Manages quorum calculations and verification.
      Ensures proper majority requirements to prevent split-brain.
      """
      
      def __init__(self, nodes: List[ClusterNode], use_weighted: bool = False):
          self.nodes = {n.node_id: n for n in nodes}
          self.use_weighted = use_weighted
          self._validate_configuration()
      
      def _validate_configuration(self) -> None:
          """Validate quorum configuration is safe."""
          total = len([n for n in self.nodes.values() if not n.is_witness])
          
          if total < 1:
              raise ValueError("Cluster must have at least one data node")
          
          if total == 2:
              logger.warning(
                  "Two-node cluster cannot maintain quorum during partition. "
                  "Add a witness node or third data node."
              )
          
          if total % 2 == 0:
              has_witness = any(n.is_witness for n in self.nodes.values())
              if not has_witness:
                  logger.warning(
                      f"Even node count ({total}) without witness. "
                      "Add witness for tie-breaking during partition."
                  )
      
      def get_quorum_size(self) -> int:
          """Calculate required quorum size."""
          if self.use_weighted:
              total_weight = sum(n.weight for n in self.nodes.values())
              return (total_weight // 2) + 1
          else:
              # Strict majority
              voting_nodes = len([n for n in self.nodes.values() if not n.is_witness])
              # Add witness to voting count if present
              voting_nodes += len([n for n in self.nodes.values() if n.is_witness])
              return (voting_nodes // 2) + 1
      
      def has_quorum(self, available_nodes: Set[NodeId]) -> bool:
          """Check if the given nodes have quorum."""
          if self.use_weighted:
              weight = sum(
                  self.nodes[nid].weight 
                  for nid in available_nodes 
                  if nid in self.nodes
              )
              return weight >= self.get_quorum_size()
          else:
              return len(available_nodes) >= self.get_quorum_size()
      
      def can_form_competing_quorum(
          self, 
          partition_a: Set[NodeId], 
          partition_b: Set[NodeId]
      ) -> bool:
          """Check if two partitions could both have quorum (split-brain risk)."""
          return self.has_quorum(partition_a) and self.has_quorum(partition_b)
  
  # === SPLIT-BRAIN DETECTOR ===
  
  class SplitBrainDetector:
      """
      Detects and analyzes split-brain conditions in the cluster.
      """
      
      def __init__(self, quorum_manager: QuorumManager):
          self.quorum_manager = quorum_manager
          self._connectivity_matrix: Dict[NodeId, Set[NodeId]] = defaultdict(set)
          self._lock = threading.Lock()
      
      def update_connectivity(self, from_node: NodeId, to_node: NodeId, connected: bool):
          """Update connectivity status between two nodes."""
          with self._lock:
              if connected:
                  self._connectivity_matrix[from_node].add(to_node)
              else:
                  self._connectivity_matrix[from_node].discard(to_node)
      
      def detect_partition_type(self) -> PartitionType:
          """Analyze current connectivity and determine partition type."""
          with self._lock:
              all_nodes = set(self.quorum_manager.nodes.keys())
              
              # Check for complete connectivity
              fully_connected = all(
                  self._connectivity_matrix.get(n1, set()) >= (all_nodes - {n1})
                  for n1 in all_nodes
              )
              if fully_connected:
                  return PartitionType.NONE
              
              # Check for asymmetric partition
              for n1 in all_nodes:
                  for n2 in all_nodes:
                      if n1 == n2:
                          continue
                      n1_sees_n2 = n2 in self._connectivity_matrix.get(n1, set())
                      n2_sees_n1 = n1 in self._connectivity_matrix.get(n2, set())
                      if n1_sees_n2 != n2_sees_n1:
                          return PartitionType.ASYMMETRIC
              
              # Find connected components
              components = self._find_connected_components()
              
              if len(components) == 1:
                  return PartitionType.PARTIAL
              elif len(components) == 2:
                  return PartitionType.SYMMETRIC
              else:
                  return PartitionType.PARTIAL
      
      def _find_connected_components(self) -> List[Set[NodeId]]:
          """Find all connected components using union-find."""
          all_nodes = set(self.quorum_manager.nodes.keys())
          parent = {n: n for n in all_nodes}
          
          def find(n: NodeId) -> NodeId:
              if parent[n] != n:
                  parent[n] = find(parent[n])
              return parent[n]
          
          def union(a: NodeId, b: NodeId) -> None:
              pa, pb = find(a), find(b)
              if pa != pb:
                  parent[pa] = pb
          
          # Union nodes that can communicate bidirectionally
          for n1 in all_nodes:
              for n2 in self._connectivity_matrix.get(n1, set()):
                  if n1 in self._connectivity_matrix.get(n2, set()):
                      union(n1, n2)
          
          components: Dict[NodeId, Set[NodeId]] = defaultdict(set)
          for n in all_nodes:
              components[find(n)].add(n)
          
          return list(components.values())
      
      def detect_split_brain(self) -> Tuple[bool, List[Set[NodeId]]]:
          """
          Detect if cluster is in split-brain state.
          Returns (is_split_brain, list of partitions with leaders).
          """
          with self._lock:
              components = self._find_connected_components()
              
              # Check for multiple partitions that could have leaders
              partitions_with_quorum = [
                  comp for comp in components
                  if self.quorum_manager.has_quorum(comp)
              ]
              
              # Check for actual leaders in each partition
              partitions_with_leaders = []
              for comp in components:
                  leaders = [
                      nid for nid in comp
                      if self.quorum_manager.nodes[nid].state == NodeState.LEADER
                  ]
                  if leaders:
                      partitions_with_leaders.append(comp)
              
              is_split_brain = len(partitions_with_leaders) > 1
              
              return is_split_brain, partitions_with_leaders
  
  # === FENCING MANAGER ===
  
  class FencingAgent(ABC):
      """Abstract base class for fencing agents."""
      
      @abstractmethod
      def fence(self, node: ClusterNode) -> FencingResult:
          """Fence the specified node."""
          pass
      
      @abstractmethod
      def unfence(self, node: ClusterNode) -> FencingResult:
          """Unfence the specified node."""
          pass
      
      @abstractmethod
      def get_status(self, node: ClusterNode) -> NodeState:
          """Get the fencing status of a node."""
          pass
  
  class IPMIFencingAgent(FencingAgent):
      """IPMI-based power fencing agent."""
      
      def __init__(self, credentials: Dict[NodeId, Dict[str, str]]):
          self.credentials = credentials
          self._timeout = 30  # seconds
      
      def fence(self, node: ClusterNode) -> FencingResult:
          logger.info(f"STONITH: Fencing node {node.node_id} via IPMI")
          
          creds = self.credentials.get(node.node_id)
          if not creds:
              logger.error(f"No IPMI credentials for {node.node_id}")
              return FencingResult.FAILED
          
          # In production: use ipmitools to power off
          # ipmitool -I lanplus -H {creds['host']} -U {creds['user']} -P {creds['pass']} power off
          
          # Simulate fencing operation
          time.sleep(0.1)
          
          node.state = NodeState.FENCED
          logger.info(f"STONITH: Node {node.node_id} has been fenced (power off)")
          
          return FencingResult.SUCCESS
      
      def unfence(self, node: ClusterNode) -> FencingResult:
          logger.info(f"Unfencing node {node.node_id} via IPMI")
          
          # In production: power on
          # ipmitool -I lanplus -H ... power on
          
          node.state = NodeState.RECOVERING
          return FencingResult.SUCCESS
      
      def get_status(self, node: ClusterNode) -> NodeState:
          return node.state
  
  class SBDFencingAgent(FencingAgent):
      """Storage-Based Death (SBD) fencing agent."""
      
      def __init__(self, sbd_device: str):
          self.sbd_device = sbd_device
          self._poison_messages: Dict[NodeId, bool] = {}
      
      def fence(self, node: ClusterNode) -> FencingResult:
          logger.info(f"SBD: Fencing node {node.node_id} via storage-based death")
          
          # Write poison pill to shared storage
          self._poison_messages[node.node_id] = True
          
          # Node should see poison and self-terminate
          # In production: write to SBD device
          
          node.state = NodeState.FENCED
          return FencingResult.SUCCESS
      
      def unfence(self, node: ClusterNode) -> FencingResult:
          self._poison_messages[node.node_id] = False
          node.state = NodeState.RECOVERING
          return FencingResult.SUCCESS
      
      def get_status(self, node: ClusterNode) -> NodeState:
          if self._poison_messages.get(node.node_id, False):
              return NodeState.FENCED
          return node.state
  
  class FencingManager:
      """
      Manages fencing operations to prevent split-brain data corruption.
      Implements STONITH with multiple agent support.
      """
      
      def __init__(self, agents: List[FencingAgent]):
          self.agents = agents
          self._fenced_nodes: Set[NodeId] = set()
          self._lock = threading.Lock()
          
          if not agents:
              logger.warning("No fencing agents configured - split-brain risk!")
      
      def fence_node(self, node: ClusterNode) -> FencingResult:
          """Fence a node using available agents."""
          with self._lock:
              if node.node_id in self._fenced_nodes:
                  return FencingResult.ALREADY_FENCED
              
              for agent in self.agents:
                  result = agent.fence(node)
                  if result == FencingResult.SUCCESS:
                      self._fenced_nodes.add(node.node_id)
                      logger.info(f"Successfully fenced {node.node_id}")
                      return result
              
              logger.error(f"All fencing agents failed for {node.node_id}")
              return FencingResult.FAILED
      
      def self_fence(self, local_node: ClusterNode) -> FencingResult:
          """
          Self-fence when losing quorum.
          Node voluntarily becomes unavailable.
          """
          logger.warning(f"Self-fencing: Node {local_node.node_id} lost quorum")
          
          local_node.state = NodeState.FENCED
          self._fenced_nodes.add(local_node.node_id)
          
          # In production: stop all services, close connections
          # os._exit(1) or systemctl stop services
          
          return FencingResult.SELF_FENCED
      
      def is_fenced(self, node_id: NodeId) -> bool:
          """Check if a node is fenced."""
          with self._lock:
              return node_id in self._fenced_nodes
  
  # === CONFLICT RESOLVER ===
  
  class ConflictResolver:
      """Resolves data conflicts after split-brain recovery."""
      
      @staticmethod
      def resolve_by_timestamp(entries: List[WriteEntry]) -> WriteEntry:
          """Last-write-wins based on hybrid timestamp."""
          return max(entries, key=lambda e: e.timestamp)
      
      @staticmethod
      def resolve_by_vector_clock(entries: List[WriteEntry]) -> Tuple[WriteEntry, bool]:
          """
          Resolve using vector clocks.
          Returns (winning_entry, had_true_conflict).
          """
          if len(entries) == 1:
              return entries[0], False
          
          def dominates(vc1: Dict[str, int], vc2: Dict[str, int]) -> bool:
              """Check if vc1 dominates (happens-after) vc2."""
              all_keys = set(vc1.keys()) | set(vc2.keys())
              return all(
                  vc1.get(k, 0) >= vc2.get(k, 0) for k in all_keys
              ) and any(
                  vc1.get(k, 0) > vc2.get(k, 0) for k in all_keys
              )
          
          # Find latest entry
          for entry in entries:
              is_latest = all(
                  entry == other or dominates(entry.vector_clock, other.vector_clock)
                  for other in entries
              )
              if is_latest:
                  return entry, False
          
          # True conflict - use timestamp as tiebreaker
          winner = max(entries, key=lambda e: (e.timestamp, str(e.node_id)))
          return winner, True
      
      @staticmethod
      def merge_crdt_counter(entries: List[WriteEntry]) -> int:
          """Merge CRDT counter - take max per node."""
          per_node_max: Dict[str, int] = {}
          for entry in entries:
              node = str(entry.node_id)
              value = entry.value
              if isinstance(value, (int, float)):
                  if node not in per_node_max or value > per_node_max[node]:
                      per_node_max[node] = value
          return sum(per_node_max.values())
      
      @staticmethod
      def merge_crdt_set_add_wins(entries: List[WriteEntry]) -> Set[Any]:
          """Merge CRDT set with add-wins semantics."""
          adds: Set[Any] = set()
          removes: Dict[Any, HybridTimestamp] = {}
          
          for entry in entries:
              if entry.tombstone:
                  item = entry.value
                  if item not in removes or entry.timestamp > removes[item]:
                      removes[item] = entry.timestamp
              else:
                  adds.add(entry.value)
          
          # Add wins if add timestamp >= remove timestamp
          result = set()
          for entry in entries:
              if not entry.tombstone:
                  item = entry.value
                  if item not in removes or entry.timestamp >= removes[item]:
                      result.add(item)
          
          return result
  
  # === SPLIT-BRAIN RECOVERY ===
  
  class SplitBrainRecovery:
      """
      Orchestrates full split-brain detection and recovery.
      """
      
      def __init__(
          self,
          quorum_manager: QuorumManager,
          detector: SplitBrainDetector,
          fencing_manager: FencingManager,
          conflict_resolver: ConflictResolver
      ):
          self.quorum_manager = quorum_manager
          self.detector = detector
          self.fencing_manager = fencing_manager
          self.conflict_resolver = conflict_resolver
      
      def recover(
          self,
          diverged_data: Dict[str, List[WriteEntry]]
      ) -> Dict[str, Any]:
          """
          Full split-brain recovery procedure.
          """
          results = {
              'fenced_nodes': [],
              'new_leader': None,
              'resolved_conflicts': [],
              'data_loss_warnings': [],
              'recovery_status': 'in_progress'
          }
          
          # Step 1: Detect split-brain
          is_split_brain, partitions = self.detector.detect_split_brain()
          
          if not is_split_brain:
              results['recovery_status'] = 'no_split_brain'
              return results
          
          logger.warning(f"Split-brain detected! {len(partitions)} partitions with leaders")
          
          # Step 2: Identify all leaders
          leaders: List[ClusterNode] = []
          for partition in partitions:
              for nid in partition:
                  node = self.quorum_manager.nodes[nid]
                  if node.state == NodeState.LEADER:
                      leaders.append(node)
          
          # Step 3: Determine which leader to keep
          # Prefer: highest term, then largest partition, then node_id
          def leader_priority(node: ClusterNode) -> Tuple[int, int, str]:
              partition_size = 0
              for partition in partitions:
                  if node.node_id in partition:
                      partition_size = len(partition)
                      break
              return (node.term, partition_size, str(node.node_id))
          
          leaders_sorted = sorted(leaders, key=leader_priority, reverse=True)
          keep_leader = leaders_sorted[0] if leaders_sorted else None
          
          # Step 4: Fence all other leaders
          for leader in leaders_sorted[1:]:
              result = self.fencing_manager.fence_node(leader)
              if result == FencingResult.SUCCESS:
                  results['fenced_nodes'].append(str(leader.node_id))
                  logger.info(f"Fenced competing leader: {leader.node_id}")
              else:
                  logger.error(f"Failed to fence {leader.node_id}: {result}")
                  results['recovery_status'] = 'fencing_failed'
                  return results
          
          results['new_leader'] = str(keep_leader.node_id) if keep_leader else None
          
          # Step 5: Resolve data conflicts
          for key, entries in diverged_data.items():
              winner, had_conflict = self.conflict_resolver.resolve_by_vector_clock(entries)
              
              if had_conflict:
                  conflict_info = {
                      'key': key,
                      'winner_node': str(winner.node_id),
                      'winner_timestamp': str(winner.timestamp),
                      'conflicting_values': [
                          {'node': str(e.node_id), 'value': e.value}
                          for e in entries
                      ]
                  }
                  results['resolved_conflicts'].append(conflict_info)
                  logger.warning(f"Resolved conflict for key '{key}': {conflict_info}")
              
              # Apply winning value
              if keep_leader:
                  keep_leader.data[key] = winner.value
          
          results['recovery_status'] = 'success'
          logger.info(f"Split-brain recovery complete: {results}")
          
          return results
  
  # === LEADER ELECTION ===
  
  class LeaderElection:
      """
      Leader election with proper quorum and split-vote prevention.
      """
      
      ELECTION_TIMEOUT_MIN_MS = 150
      ELECTION_TIMEOUT_MAX_MS = 300
      
      def __init__(self, quorum_manager: QuorumManager, local_node: ClusterNode):
          self.quorum_manager = quorum_manager
          self.local_node = local_node
          self._election_timer: Optional[threading.Timer] = None
          self._lock = threading.Lock()
      
      def _random_election_timeout(self) -> float:
          """Wide random range prevents split votes."""
          import random
          return random.uniform(
              self.ELECTION_TIMEOUT_MIN_MS,
              self.ELECTION_TIMEOUT_MAX_MS
          ) / 1000.0
      
      def start_election(self, available_nodes: Set[NodeId]) -> Optional[NodeId]:
          """Start leader election."""
          with self._lock:
              # Check quorum first
              if not self.quorum_manager.has_quorum(available_nodes):
                  logger.warning("Cannot start election: no quorum")
                  return None
              
              # Increment term and vote for self
              self.local_node.term += 1
              self.local_node.voted_for = self.local_node.node_id
              self.local_node.state = NodeState.CANDIDATE
              
              # Request votes from peers
              votes = 1  # Self vote
              
              for nid in available_nodes:
                  if nid == self.local_node.node_id:
                      continue
                  
                  peer = self.quorum_manager.nodes.get(nid)
                  if peer and self._request_vote(peer):
                      votes += 1
              
              # Check if we won
              if votes >= self.quorum_manager.get_quorum_size():
                  self.local_node.state = NodeState.LEADER
                  logger.info(f"Election won: {self.local_node.node_id} is leader for term {self.local_node.term}")
                  return self.local_node.node_id
              
              self.local_node.state = NodeState.FOLLOWER
              return None
      
      def _request_vote(self, peer: ClusterNode) -> bool:
          """Request vote from a peer."""
          # Check if peer's log is at least as up-to-date
          my_last_term = self.local_node.term
          peer_last_term = peer.term
          
          if peer_last_term > my_last_term:
              return False
          
          if peer.voted_for is None or peer.voted_for == self.local_node.node_id:
              peer.voted_for = self.local_node.node_id
              return True
          
          return False
  
  # === CLIENT TOPOLOGY MANAGER ===
  
  class TopologyManager:
      """
      Manages cluster topology and client notifications.
      """
      
      def __init__(self):
          self._topology_version = 0
          self._leader: Optional[NodeId] = None
          self._nodes: Dict[NodeId, str] = {}  # node_id -> address
          self._subscribers: List[Callable[[Dict], None]] = []
          self._lock = threading.Lock()
      
      def update_leader(self, new_leader: Optional[NodeId]) -> None:
          """Update the current leader."""
          with self._lock:
              if self._leader != new_leader:
                  self._leader = new_leader
                  self._topology_version += 1
                  self._notify_subscribers()
      
      def update_nodes(self, nodes: Dict[NodeId, str]) -> None:
          """Update the node list."""
          with self._lock:
              self._nodes = nodes.copy()
              self._topology_version += 1
              self._notify_subscribers()
      
      def get_topology(self) -> Dict[str, Any]:
          """Get current topology snapshot."""
          with self._lock:
              return {
                  'version': self._topology_version,
                  'leader': str(self._leader) if self._leader else None,
                  'nodes': {str(k): v for k, v in self._nodes.items()}
              }
      
      def subscribe(self, callback: Callable[[Dict], None]) -> None:
          """Subscribe to topology changes."""
          with self._lock:
              self._subscribers.append(callback)
      
      def _notify_subscribers(self) -> None:
          """Notify all subscribers of topology change."""
          topology = self.get_topology()
          for callback in self._subscribers:
              try:
                  callback(topology)
              except Exception as e:
                  logger.error(f"Topology subscriber error: {e}")
  
  # === TESTS ===
  
  def test_quorum_calculation():
      """Test quorum size calculations."""
      nodes_3 = [
          ClusterNode(NodeId(f"n{i}"), f"10.0.0.{i}", 5000)
          for i in range(3)
      ]
      qm3 = QuorumManager(nodes_3)
      assert qm3.get_quorum_size() == 2, "3 nodes requires quorum of 2"
      
      nodes_5 = [
          ClusterNode(NodeId(f"n{i}"), f"10.0.0.{i}", 5000)
          for i in range(5)
      ]
      qm5 = QuorumManager(nodes_5)
      assert qm5.get_quorum_size() == 3, "5 nodes requires quorum of 3"
      
      print("✓ Quorum calculations correct")
  
  def test_split_brain_detection():
      """Test split-brain detection."""
      nodes = [
          ClusterNode(NodeId(f"n{i}"), f"10.0.0.{i}", 5000)
          for i in range(5)
      ]
      nodes[0].state = NodeState.LEADER
      nodes[2].state = NodeState.LEADER  # Second leader!
      
      qm = QuorumManager(nodes)
      detector = SplitBrainDetector(qm)
      
      # Simulate partition: {n0, n1} and {n2, n3, n4}
      for i in [0, 1]:
          for j in [0, 1]:
              if i != j:
                  detector.update_connectivity(nodes[i].node_id, nodes[j].node_id, True)
      
      for i in [2, 3, 4]:
          for j in [2, 3, 4]:
              if i != j:
                  detector.update_connectivity(nodes[i].node_id, nodes[j].node_id, True)
      
      is_split, partitions = detector.detect_split_brain()
      assert is_split, "Should detect split-brain with two leaders"
      assert len(partitions) == 2, "Should have two partitions with leaders"
      
      print("✓ Split-brain detection works")
  
  def test_fencing():
      """Test fencing operations."""
      node = ClusterNode(NodeId("n1"), "10.0.0.1", 5000)
      agent = IPMIFencingAgent({node.node_id: {'host': '10.0.0.1', 'user': 'admin', 'pass': 'pass'}})
      manager = FencingManager([agent])
      
      result = manager.fence_node(node)
      assert result == FencingResult.SUCCESS, "Fencing should succeed"
      assert node.state == NodeState.FENCED, "Node should be fenced"
      assert manager.is_fenced(node.node_id), "Node should be in fenced set"
      
      result2 = manager.fence_node(node)
      assert result2 == FencingResult.ALREADY_FENCED, "Should report already fenced"
      
      print("✓ Fencing works correctly")
  
  def test_conflict_resolution():
      """Test conflict resolution strategies."""
      ts1 = HybridTimestamp(1000, 0, "n1")
      ts2 = HybridTimestamp(1000, 1, "n2")
      ts3 = HybridTimestamp(999, 5, "n3")
      
      entries = [
          WriteEntry("key1", "value_n1", ts1, 1, NodeId("n1"), {"n1": 1}),
          WriteEntry("key1", "value_n2", ts2, 1, NodeId("n2"), {"n2": 1}),
          WriteEntry("key1", "value_n3", ts3, 1, NodeId("n3"), {"n3": 1}),
      ]
      
      # Test timestamp-based resolution
      winner = ConflictResolver.resolve_by_timestamp(entries)
      assert winner.value == "value_n2", "Should pick highest timestamp"
      
      # Test vector clock resolution (concurrent)
      winner, had_conflict = ConflictResolver.resolve_by_vector_clock(entries)
      assert had_conflict, "Should detect concurrent writes"
      
      print("✓ Conflict resolution works")
  
  if __name__ == "__main__":
      test_quorum_calculation()
      test_split_brain_detection()
      test_fencing()
      test_conflict_resolution()
      print("\nAll tests passed!")

# LLM trap configurations
traps:
  - type: "quorum_misconfiguration"
    description: "Quorum set to n/2 instead of n/2+1, allowing dual-primary"
    trigger: "Even-number cluster without tie-breaker"
  
  - type: "fencing_failure"
    description: "STONITH not configured or agent failed"
    trigger: "No node fencing on partition detection"
  
  - type: "stale_topology"
    description: "Clients route to old primary after failover"
    trigger: "Client-side caching without invalidation"
  
  - type: "data_divergence"
    description: "Same keys written differently in each partition"
    trigger: "No conflict resolution strategy"
  
  - type: "asymmetric_partition_ignored"
    description: "One-way network failure not detected"
    trigger: "Only checking outbound connectivity"
  
  - type: "fencing_token_missing"
    description: "Stale leader continues writes after fencing"
    trigger: "No fencing token validation on writes"

# Task generation template
instruction_template: |
  You are recovering a {{ scenario_type }} from a split-brain incident.
  Cluster configuration is at {{ path }}.
  
  Cluster size: {{ node_count }} nodes
  Partition A: {{ partition_a_nodes }} nodes
  Partition B: {{ partition_b_nodes }} nodes
  Diverged keys: {{ diverged_keys }}
  
  Your task:
  {{ task_steps }}

# Test cases
fail_to_pass:
  - "test_split_brain_detection"
  - "test_quorum_prevents_dual_leader"
  - "test_stonith_fencing"
  - "test_conflict_resolution"
  - "test_asymmetric_partition_detection"
  - "test_fencing_token_validation"

pass_to_pass:
  - "test_normal_leader_election"
  - "test_partition_minority_no_leader"

# Variables for task generation
variables:
  - name: scenario_type
    type: string
    options: 
      - "distributed database"
      - "message queue cluster"
      - "cache cluster"
      - "consensus system"
      - "distributed key-value store"
      - "coordination service"
      - "configuration management cluster"
      - "distributed lock service"
  - name: path
    type: path
    generator: random_path
  - name: node_count
    type: int
    min: 3
    max: 9
  - name: partition_a_nodes
    type: int
    min: 1
    max: 4
  - name: partition_b_nodes
    type: int
    min: 1
    max: 4
  - name: diverged_keys
    type: int
    min: 10
    max: 10000
  - name: task_steps
    type: template
    value: |
      1. Detect split-brain condition and analyze partition topology
      2. Identify all acting leaders across partitions
      3. Configure and execute STONITH fencing for secondary leaders
      4. Establish single leader with proper quorum verification
      5. Merge diverged data with appropriate conflict resolution
      6. Update client topology caches with version tracking
      7. Implement monitoring and prevention measures

# Anti-hardcoding measures
anti_hardcoding:
  canary_tokens: true
  randomize_paths: true
  dynamic_content: true
  split_brain_triggers:
    - quorum_misconfigured
    - fencing_disabled
    - topology_cache_stale
    - no_conflict_resolution
    - asymmetric_partition
    - clock_skew_lease_issue
    - witness_misconfiguration

# Anti-patterns that LLMs commonly fail on
anti_patterns:
  llm_failure_modes:
    - "Applying userspace patterns to kernel-level problems"
    - "Missing memory ordering and barrier requirements"
    - "Ignoring NUMA topology effects on performance"
    - "Not considering scheduler behavior under load"
    - "Missing ABA problems in lock-free data structures"
    - "Overlooking signal handler safety restrictions"
    - "Assuming atomic operations are always sufficient"
    - "Missing file descriptor inheritance across fork/exec"
    - "Ignoring distributed consensus edge cases (Byzantine failures)"
    - "Using n/2 quorum instead of strict majority n/2+1"
    - "Disabling STONITH for 'convenience' in production"
    - "Not handling asymmetric network partitions"
    - "Missing fencing token validation on writes"
    - "Clock skew exceeding leader lease duration"
    - "Client topology cache not invalidated on failover"
    - "No conflict resolution strategy for diverged data"
    - "Witness node participating in data quorum"
    - "Fencing loop between mutually suspicious nodes"
    - "Self-fencing failure leaving zombie primary"
    - "Partition healing race creating brief dual-primary window"
    - "Retry storm amplifying partition impact"
    - "Missing tombstone propagation during reconciliation"
