id: "sys-dist-consensus-failure-001"
version: "1.0.0"
category: "systems"
subcategory: "distributed-systems"

# SWE-bench_Pro style fields
problem_statement: |
  A distributed consensus system (Raft/Paxos implementation) is failing to reach 
  agreement under certain conditions. The implementation appears correct but has 
  subtle bugs that manifest under specific failure scenarios.
  
  Failure modes:
  1. Byzantine behavior from corrupted messages
  2. Split vote leading to livelock
  3. Log divergence after leader failure
  4. Membership changes during consensus
  5. Snapshot/compaction race conditions

requirements: |
  - Identify consensus protocol violations
  - Handle message corruption/duplication
  - Prevent split vote scenarios
  - Ensure log consistency after leader failure
  - Safe membership changes (joint consensus)

interface: |
  Input: Consensus implementation, failure scenarios
  Output: Fixed implementation, test scenarios
  Validation: Pass Jepsen-style testing

# terminal-bench style fields
difficulty:
  estimated: "hard"
  time_range: [3600, 10800]
  command_steps: [40, 110]

# === QUALITY REQUIREMENTS ===
quality_requirements:
  minimum_time: "120-300 minutes for senior systems engineers with kernel contribution experience"
  expertise_level: "Principal distributed systems engineer with Raft/Paxos implementation and formal verification expertise"
  validation_standard: "Must expose subtle consensus protocol violations causing safety and liveness issues"

# === MULTI-AGENT ORCHESTRATION ===
multi_agent_orchestration:
  required_agents: 10
  agent_specializations:
    - name: "raft_internals_expert"
      role: "Debug Raft protocol implementation details"
      expertise: ["leader election", "log replication", "safety proofs", "membership changes"]
    - name: "paxos_specialist"
      role: "Debug Paxos variants and Multi-Paxos optimizations"
      expertise: ["prepare-accept", "ballot numbers", "stable leader", "EPaxos"]
    - name: "log_consistency_debugger"
      role: "Debug log consistency and divergence issues"
      expertise: ["log matching", "conflict resolution", "compaction", "snapshots"]
    - name: "election_analyzer"
      role: "Debug leader election issues and split votes"
      expertise: ["randomized timeouts", "prevote", "check quorum", "term management"]
    - name: "membership_change_expert"
      role: "Debug joint consensus and configuration changes"
      expertise: ["Cnew,old", "single-server changes", "learner promotion", "leader transfer"]
    - name: "byzantine_fault_expert"
      role: "Debug Byzantine fault tolerance issues"
      expertise: ["PBFT", "HotStuff", "view changes", "cryptographic verification"]
    - name: "jepsen_tester"
      role: "Apply Jepsen-style testing and analysis"
      expertise: ["nemesis", "linearizability checking", "fault injection", "history analysis"]
    - name: "tlaplus_modeler"
      role: "Formal verification with TLA+ and model checking"
      expertise: ["safety properties", "liveness properties", "state space", "invariants"]
    - name: "network_fault_injector"
      role: "Debug behavior under network partitions"
      expertise: ["symmetric partitions", "asymmetric partitions", "message delays", "reordering"]
    - name: "storage_layer_debugger"
      role: "Debug persistence and recovery"
      expertise: ["WAL", "fsync ordering", "crash recovery", "state machine checkpoint"]
  cross_subsystem_chains:
    - "Network partition → Split vote → Term inflation → Delayed election → Liveness failure"
    - "Snapshot race → Log truncation → Missing entries → Safety violation"
    - "Membership change → Quorum miscalculation → Dual leaders → Data corruption"
    - "Clock skew → Lease invalid → Stale read → Linearizability violation"
  parallel_debugging_requirements:
    - "Correlated log traces across all consensus nodes"
    - "Simultaneous election timeout and vote analysis"
    - "Agent handoff for complex multi-node failure scenarios"

# === TRAP CONFIGURATION ===
trap_configuration:
  trap_count: "12+ deeply interacting traps across kernel/userspace/hardware boundaries"
  trap_categories:
    election_traps:
      - "Election timeout range too narrow causing persistent split votes"
      - "Missing prevote causing term inflation during partition"
      - "Vote persistence race with concurrent vote requests"
      - "Leadership transfer during network instability"
    log_replication_traps:
      - "Missing prevLogTerm check allowing log divergence"
      - "Incorrect truncation point on leader change"
      - "Commit index advancing beyond actual replication"
      - "Snapshot installed over uncommitted entries"
    membership_change_traps:
      - "Concurrent configuration changes breaking quorum"
      - "Leader removal during pending configuration change"
      - "Learner counting toward quorum incorrectly"
      - "Configuration log entry not treated specially"

# === NIGHTMARE_PLUS DIFFICULTY ===
difficulty_levels:
  nightmare_plus:
    estimated_time: [43200, 259200]  # 12-72 hours
    command_steps: [600, 2500]
    techniques_required: 18
    description: "Raft PhD thesis / Diego Ongaro dissertation-level difficulty"
    requirements:
      - "Requires understanding of formal safety proofs for consensus"
      - "Must handle all corner cases in Raft Extended paper"
      - "Involves debugging Multi-Raft and cross-shard protocols"
      - "Cross-platform behavior differences in persistence semantics"
      - "Time estimate: 12-72 hours for consensus protocol researchers"
      - "Requires synthesizing distributed systems, storage, and networking knowledge"

# === KERNEL SUBSYSTEM REQUIREMENTS ===
kernel_subsystem_requirements:
  storage_subsystem:
    - "Write-ahead logging and fsync ordering requirements"
    - "Direct I/O vs buffered I/O persistence guarantees"
    - "Block device write ordering semantics"
    - "Crash recovery and journal replay"
  networking_subsystem:
    - "TCP vs UDP tradeoffs for consensus messages"
    - "Network partition detection and handling"
    - "Message ordering guarantees"
    - "Connection timeout and retry semantics"
  process_management:
    - "Signal handling during consensus operations"
    - "Process crash and restart behavior"
    - "Memory mapping and durability"
    - "Fork safety with consensus state"
  timing_subsystem:
    - "Election timeout implementation"
    - "Heartbeat scheduling and jitter"
    - "Clock synchronization requirements"
    - "Lease timing and clock bounds"

# === HARDWARE INTERACTION ===
hardware_interaction:
  storage_hardware:
    - "NVMe vs SATA fsync latency differences"
    - "Write cache behavior and barriers"
    - "Power loss protection (PLP) requirements"
    - "RAID controller write-back effects"
  network_hardware:
    - "NIC buffer sizes and packet loss"
    - "Switch cut-through vs store-and-forward"
    - "Network partition failure modes"
    - "Jumbo frames and fragmentation"
  cpu_and_memory:
    - "Memory ordering effects on consensus state"
    - "NUMA effects on distributed state access"
    - "CPU scheduling effects on timeout accuracy"
    - "Memory pressure effects on message handling"

# === FORMAL VERIFICATION REQUIREMENTS ===
formal_verification_requirements:
  modeling_requirements:
    - "Complete TLA+ specification of consensus protocol"
    - "Express safety invariants formally"
    - "Model liveness under fairness assumptions"
    - "Verify linearizability of replicated state machine"
  proof_obligations:
    - "Prove leader uniqueness per term"
    - "Verify log matching invariant"
    - "Establish commit index monotonicity"
    - "Prove membership change safety"

# === LLM GENERATION FRAMEWORK ===
generation_framework:
  multi_conversation_workflow:
    phase_1_research: "Research consensus protocol edge cases and known bugs"
    phase_2_creation: "Create task with subtle protocol violations"
    phase_3_amplification: "Add timing windows and message reordering"
    phase_4_verification:
      description: "Validate task requires understanding of safety vs liveness"
      criteria:
        - "Has at least 5 interacting system-level traps across subsystems"
        - "Has cascading failures across process, memory, and I/O subsystems"
        - "Requires knowledge of OS kernel internals and scheduler behavior"
        - "Would take experienced systems programmers 45+ minutes"
  
  difficulty_amplifiers:
    nightmare:
      multiplier: 3.5
      description: "Kernel-level debugging complexity requiring deep systems expertise"
      requirements:
        - "7+ interacting system-level traps across subsystems"
        - "Requires understanding of kernel memory management and scheduling"
        - "Time estimate: 120+ minutes for senior systems engineers"
        - "Cross-platform behavior differences (Linux vs BSD vs Windows)"
        - "Requires synthesizing concurrency, memory, distributed systems knowledge"
  
  generation_targets:
    minimum_difficulty: "45+ minutes, requires deep kernel internals and distributed systems expertise"
  
  complexity_levels:
    level_1_basic:
      description: "Simple election timeout misconfiguration"
      elements: ["narrow_timeout_range", "split_vote", "basic_term_check"]
    level_2_intermediate:
      description: "Log consistency violations under leader failure"
      elements: ["log_divergence", "stale_read", "missing_term_check"]
    level_3_advanced:
      description: "Membership change races and snapshot issues"
      elements: ["joint_consensus", "snapshot_race", "compaction_timing"]
    level_4_expert:
      description: "Byzantine failures and message corruption"
      elements: ["message_corruption", "duplicate_handling", "byzantine_leader"]
    level_5_research:
      description: "Novel consensus bugs in production systems"
      elements: ["linearizability_violation", "stale_leader_write", "lease_timing"]

# === COMPREHENSIVE TOPIC UNIVERSE ===
topic_universe:
  # Raft Protocol Internals (40+ topics)
  raft_protocol:
    leader_election:
      - "Election timer randomization requirements"
      - "Pre-vote extension for network partition"
      - "Check quorum for liveness guarantee"
      - "Leadership transfer mechanism"
      - "Candidate state transitions"
      - "Vote request handling rules"
      - "Term increment on election start"
      - "Vote persistence requirements"
      - "Election timeout tuning for WAN"
      - "Split vote scenarios and prevention"
    
    log_replication:
      - "AppendEntries RPC semantics"
      - "Log matching property"
      - "Consistency check with prevLogIndex"
      - "Truncation on conflict"
      - "Commit index advancement rules"
      - "No-op entry on leader election"
      - "Pipeline optimization"
      - "Batching strategies"
      - "Flow control mechanisms"
      - "Backpressure handling"
    
    log_compaction:
      - "Snapshot triggering conditions"
      - "Snapshot contents requirements"
      - "InstallSnapshot RPC"
      - "Snapshot chunk transfer"
      - "Log truncation after snapshot"
      - "Concurrent snapshot and replication"
      - "Incremental snapshots"
      - "Snapshot compression"
    
    membership_changes:
      - "Joint consensus protocol"
      - "Single-server membership changes"
      - "Cnew,old configuration handling"
      - "Configuration change commit rules"
      - "Learner/non-voting member addition"
      - "Safe removal of current leader"
      - "Membership change during partition"
      - "Configuration log entry handling"
    
    optimizations:
      - "Read-only queries on leader"
      - "Lease-based reads"
      - "Follower reads with read index"
      - "Batch commit"
      - "Parallel apply"
      - "Async apply"
      - "Witness nodes"
  
  # Paxos Protocol Variants (35+ topics)
  paxos_protocol:
    classic_paxos:
      - "Proposer, Acceptor, Learner roles"
      - "Prepare phase (Phase 1)"
      - "Accept phase (Phase 2)"
      - "Promise message semantics"
      - "Highest accepted value rule"
      - "Quorum intersection property"
      - "Dueling proposers problem"
      - "Proposal number generation"
    
    multi_paxos:
      - "Stable leader optimization"
      - "Skipping prepare phase"
      - "Leader lease for reads"
      - "Log gap handling"
      - "Out-of-order commit"
      - "Instance number management"
    
    fast_paxos:
      - "Fast path with no collision"
      - "Classic path fallback"
      - "Collision detection"
      - "Quorum requirements (3f+1)"
      - "Round structure"
    
    flexible_paxos:
      - "Separate read/write quorums"
      - "Quorum intersection requirement"
      - "Trading availability for consistency"
      - "Variable quorum configurations"
    
    epaxos:
      - "Command interference"
      - "Dependency tracking"
      - "Fast path commit"
      - "Slow path recovery"
      - "Execution ordering"
  
  # Byzantine Fault Tolerance (30+ topics)
  byzantine_protocols:
    pbft_core:
      - "Pre-prepare, Prepare, Commit phases"
      - "View change protocol"
      - "Checkpoint mechanism"
      - "Garbage collection"
      - "3f+1 node requirement"
      - "Message authentication"
      - "Sequence number management"
    
    optimizations:
      - "Speculative execution"
      - "Tentative execution"
      - "Batching requests"
      - "MAC vs signatures tradeoff"
    
    hotstuff:
      - "Linear view change"
      - "Chained BFT"
      - "Pipelining phases"
      - "Threshold signatures"
    
    modern_bft:
      - "SBFT with trusted hardware"
      - "Tendermint consensus"
      - "Casper FFG"
      - "Avalanche consensus"
  
  # Consensus Safety Properties (25+ topics)
  safety_properties:
    linearizability:
      - "Real-time ordering"
      - "Single-copy semantics"
      - "Linearization points"
      - "Linearizability vs serializability"
      - "Testing linearizability (Jepsen)"
    
    consensus_safety:
      - "Agreement property"
      - "Validity property"
      - "Termination property"
      - "Integrity property"
      - "FLP impossibility"
    
    log_consistency:
      - "Log matching"
      - "Leader completeness"
      - "State machine safety"
      - "Leader append-only"
    
    membership_safety:
      - "No two leaders in same term"
      - "Configuration committed before applied"
      - "Joint configuration safety"
  
  # Liveness and Availability (20+ topics)
  liveness:
    election_liveness:
      - "Eventually elect leader"
      - "Bounded election time"
      - "Progress during partition"
      - "Livelock prevention"
    
    replication_liveness:
      - "Eventually replicate"
      - "Bounded commit latency"
      - "Progress with minority failure"
    
    availability_tradeoffs:
      - "CAP theorem implications"
      - "PACELC tradeoffs"
      - "Availability vs consistency"
      - "Stale reads for availability"
  
  # Message Handling (25+ topics)
  message_handling:
    ordering:
      - "FIFO channel assumption"
      - "Out-of-order message handling"
      - "Message sequence numbers"
      - "Duplicate detection"
      - "Idempotent operations"
    
    reliability:
      - "At-most-once delivery"
      - "At-least-once delivery"
      - "Exactly-once semantics"
      - "Message persistence"
      - "Acknowledgment handling"
    
    corruption:
      - "Checksum validation"
      - "Cryptographic verification"
      - "Byzantine message handling"
      - "Malformed message rejection"
    
    timing:
      - "Message timeout handling"
      - "Retry strategies"
      - "Exponential backoff"
      - "Heartbeat intervals"
  
  # State Machine Replication (20+ topics)
  state_machine:
    determinism:
      - "Deterministic execution requirement"
      - "Non-determinism sources"
      - "Time handling in replicas"
      - "Random number handling"
      - "External service calls"
    
    application_interface:
      - "Command representation"
      - "Read vs write operations"
      - "Batch operations"
      - "Conditional writes"
    
    recovery:
      - "State reconstruction from log"
      - "Checkpoint-based recovery"
      - "Incremental recovery"
      - "Parallel recovery"
  
  # Testing and Verification (25+ topics)
  testing:
    jepsen_testing:
      - "Nemesis fault injection"
      - "History recording"
      - "Linearizability checking"
      - "Clock skew injection"
      - "Network partition simulation"
    
    tla_plus:
      - "Formal specification"
      - "Model checking"
      - "Safety properties"
      - "Liveness properties"
      - "State space exploration"
    
    chaos_engineering:
      - "Random failure injection"
      - "Latency injection"
      - "Message drop simulation"
      - "Byzantine fault injection"
    
    property_testing:
      - "Invariant checking"
      - "State transition validation"
      - "Convergence testing"
      - "Stress testing"

# === BUG PATTERNS ===
bug_patterns:
  election_bugs:
    - pattern: "narrow_election_timeout_range"
      description: "Election timeout range too narrow, causing split votes"
      severity: "high"
      detection: "Repeated elections without leader"
      fix: "Use wide random range (e.g., 150-300ms)"
      code_example: |
        # Bug: too narrow
        timeout = random.uniform(150, 160)  # Only 10ms range!
        # Fix: wide range
        timeout = random.uniform(150, 300)  # 150ms range
    
    - pattern: "missing_prevote"
      description: "No pre-vote causes unnecessary elections during partition"
      severity: "medium"
      detection: "Term inflation during network issues"
      fix: "Implement pre-vote extension"
    
    - pattern: "vote_persistence_missing"
      description: "Vote not persisted, can vote twice after restart"
      severity: "critical"
      detection: "Two leaders in same term after crash"
      fix: "Persist votedFor before responding"
    
    - pattern: "stale_term_not_checked"
      description: "Not checking term in RPC responses"
      severity: "high"
      detection: "Stale leader continues operating"
      fix: "Always check response term, step down if higher"

  log_replication_bugs:
    - pattern: "missing_prevlogindex_check"
      description: "Not verifying prevLogIndex/prevLogTerm"
      severity: "critical"
      detection: "Log divergence after leader change"
      fix: "Always verify log consistency before append"
      code_example: |
        # Bug: missing check
        def append_entries(self, req):
            self.log.extend(req.entries)  # No consistency check!
        
        # Fix: verify consistency
        def append_entries(self, req):
            if req.prev_log_index >= len(self.log):
                return False
            if self.log[req.prev_log_index].term != req.prev_log_term:
                self.log = self.log[:req.prev_log_index]
                return False
            self.log.extend(req.entries)
    
    - pattern: "no_truncation_on_conflict"
      description: "Not truncating log on term mismatch"
      severity: "critical"
      detection: "Permanent log inconsistency"
      fix: "Truncate log entries with mismatched terms"
    
    - pattern: "commit_index_beyond_log"
      description: "Commit index set beyond log length"
      severity: "high"
      detection: "Index out of bounds in apply"
      fix: "Commit index = min(leaderCommit, lastLogIndex)"
    
    - pattern: "apply_before_commit"
      description: "Applying entries before commit"
      severity: "critical"
      detection: "Uncommitted data visible to clients"
      fix: "Only apply when commitIndex >= index"

  snapshot_bugs:
    - pattern: "snapshot_during_replication"
      description: "Snapshot created while entries being replicated"
      severity: "high"
      detection: "Lost entries after snapshot"
      fix: "Coordinate snapshot with replication state"
    
    - pattern: "stale_snapshot_index"
      description: "Snapshot index not updated atomically"
      severity: "medium"
      detection: "Log truncated too early"
      fix: "Atomic update of snapshot metadata"
    
    - pattern: "partial_snapshot_install"
      description: "Incomplete snapshot installed on failure"
      severity: "high"
      detection: "Corrupted state after restart"
      fix: "Atomic snapshot installation with verification"

  membership_change_bugs:
    - pattern: "concurrent_config_changes"
      description: "Multiple config changes in flight"
      severity: "critical"
      detection: "Split-brain during membership change"
      fix: "Only one config change at a time"
    
    - pattern: "config_not_committed"
      description: "Acting on uncommitted configuration"
      severity: "critical"
      detection: "Wrong quorum calculation"
      fix: "Wait for config to commit before applying"
    
    - pattern: "leader_removed_wrong"
      description: "Leader steps down before config committed"
      severity: "high"
      detection: "Leadership gap during removal"
      fix: "Leader stays until new config committed"

  timing_bugs:
    - pattern: "heartbeat_storm"
      description: "Too frequent heartbeats overwhelm network"
      severity: "medium"
      detection: "High CPU/network during idle"
      fix: "Rate limit heartbeats appropriately"
    
    - pattern: "timeout_too_aggressive"
      description: "Election timeout less than broadcast time"
      severity: "high"
      detection: "Constant elections in normal operation"
      fix: "Timeout > 2 * max_broadcast_time"
    
    - pattern: "clock_skew_lease_invalid"
      description: "Leader lease invalid due to clock skew"
      severity: "critical"
      detection: "Stale reads from follower"
      fix: "Use monotonic clocks, account for drift"

# === EDGE CASES ===
edge_cases:
  election_edge_cases:
    - "Election starts exactly at same time on two nodes"
    - "Vote request arrives just after voting for another"
    - "Leader elected but immediately partitioned"
    - "All nodes start election simultaneously"
    - "Term overflow (very long-running cluster)"
    - "Pre-vote rejected but election succeeds"
    - "Leadership transfer during network partition"
    - "Candidate receives stale vote response"
    - "Step down during vote counting"
    - "Tie in votes with even cluster size"

  log_replication_edge_cases:
    - "Empty log on all nodes"
    - "Single entry log"
    - "Maximum log size"
    - "Entry index overflow"
    - "Duplicate AppendEntries with same index"
    - "AppendEntries with gaps"
    - "Conflicting entries from old leader"
    - "Leader change mid-replication"
    - "Network delay longer than election timeout"
    - "Follower ahead of leader after recovery"

  snapshot_edge_cases:
    - "Snapshot during membership change"
    - "Snapshot larger than memory"
    - "Concurrent snapshots from multiple leaders"
    - "Snapshot install interrupted"
    - "Snapshot version mismatch"
    - "Snapshot from future term"
    - "Log compaction during snapshot transfer"
    - "Follower restarts during snapshot install"

  membership_edge_cases:
    - "Adding node during election"
    - "Removing leader node"
    - "Adding multiple nodes simultaneously"
    - "Membership change with pending writes"
    - "Configuration change during partition"
    - "Learner promotion timing"
    - "Rolling restart during config change"
    - "Witness node in odd cluster"

  message_edge_cases:
    - "Duplicate message from network retry"
    - "Message from previous term"
    - "Message from future term"
    - "Corrupted message content"
    - "Message with invalid checksum"
    - "Message reordering"
    - "Message delay exceeding timeout"
    - "Zero-length message"
    - "Maximum message size"

# === DIFFICULTY MULTIPLIERS ===
difficulty_multipliers:
  protocol_complexity:
    multi_raft:
      multiplier: 2.0
      description: "Multiple Raft groups with cross-group operations"
      considerations:
        - "Group routing"
        - "Cross-group transactions"
        - "Load balancing"
        - "Split-brain per group"
    
    byzantine_raft:
      multiplier: 2.5
      description: "Raft with Byzantine fault tolerance"
      considerations:
        - "Message authentication"
        - "f+1 matching responses"
        - "Cryptographic overhead"

  consistency_requirements:
    strict_linearizability:
      multiplier: 1.8
      description: "Every read must be linearizable"
      considerations:
        - "Read index protocol"
        - "Leader lease overhead"
        - "Latency impact"
    
    serializable_transactions:
      multiplier: 2.0
      description: "Multi-key transactions"
      considerations:
        - "2PC over Raft"
        - "Lock management"
        - "Deadlock prevention"

  deployment_complexity:
    geo_distributed:
      multiplier: 1.8
      description: "WAN deployment across regions"
      considerations:
        - "Timeout tuning"
        - "Leader placement"
        - "Quorum configuration"
    
    heterogeneous_nodes:
      multiplier: 1.5
      description: "Different node capabilities"
      considerations:
        - "Witness nodes"
        - "Learner nodes"
        - "Weighted voting"

# === SCENARIO TEMPLATES ===
scenario_templates:
  split_vote_livelock:
    description: "Repeated split votes preventing leader election"
    setup: |
      - 4 node cluster
      - Election timeouts very similar
      - High network latency variance
    symptoms: |
      - No leader for extended period
      - Term numbers rapidly increasing
      - All nodes cycling through candidate state
    root_cause: "Insufficient randomization in election timeout"
    fix: |
      - Increase timeout range
      - Add jitter to timeout calculation
      - Consider prevote extension

  log_divergence:
    description: "Follower log diverges from leader after partition"
    setup: |
      - 3 node cluster
      - Leader A fails after committing entry
      - B becomes leader, writes different entry
      - A recovers as follower
    symptoms: |
      - Different data on different nodes
      - Linearizability violations
      - Test failures in Jepsen
    root_cause: "Missing prevLogTerm check in AppendEntries"
    fix: |
      - Always verify log consistency
      - Truncate on conflict
      - Implement proper log matching

  snapshot_corruption:
    description: "State corrupted after snapshot restore"
    setup: |
      - Long-running cluster
      - Snapshot triggered during write
      - Follower installs snapshot
    symptoms: |
      - Missing data after recovery
      - Inconsistent state between nodes
      - Application errors
    root_cause: "Race between snapshot and log application"
    fix: |
      - Atomic snapshot creation
      - Coordinate with replication
      - Verify snapshot integrity

  membership_chaos:
    description: "Cluster instability during config change"
    setup: |
      - 5 node cluster
      - Adding new node while removing old
      - Network partition during change
    symptoms: |
      - Two leaders elected
      - Quorum calculations wrong
      - Cluster unrecoverable
    root_cause: "Multiple concurrent config changes"
    fix: |
      - Enforce one change at a time
      - Use joint consensus
      - Wait for commit before next change

# === REFERENCE SOLUTION ===
reference_solution: |
  #!/usr/bin/env python3
  """
  Comprehensive Raft consensus implementation with bug fixes.
  Handles split votes, log consistency, snapshots, and membership changes.
  """
  import random
  import time
  import threading
  import hashlib
  from typing import Dict, List, Optional, Set, Tuple, Any, Callable
  from dataclasses import dataclass, field
  from enum import Enum, auto
  from collections import defaultdict
  import logging
  import struct
  
  logging.basicConfig(level=logging.INFO)
  logger = logging.getLogger(__name__)
  
  # === CORE TYPES ===
  
  class NodeState(Enum):
      FOLLOWER = auto()
      CANDIDATE = auto()
      LEADER = auto()
      LEARNER = auto()
  
  @dataclass
  class LogEntry:
      """Single log entry with term and command."""
      term: int
      index: int
      command: bytes
      entry_type: str = "normal"  # normal, config, noop
      checksum: str = ""
      
      def __post_init__(self):
          if not self.checksum:
              data = f"{self.term}:{self.index}:{self.command}:{self.entry_type}"
              self.checksum = hashlib.sha256(data.encode()).hexdigest()[:16]
      
      def verify(self) -> bool:
          data = f"{self.term}:{self.index}:{self.command}:{self.entry_type}"
          expected = hashlib.sha256(data.encode()).hexdigest()[:16]
          return self.checksum == expected
  
  @dataclass
  class AppendEntriesRequest:
      """AppendEntries RPC request."""
      term: int
      leader_id: str
      prev_log_index: int
      prev_log_term: int
      entries: List[LogEntry]
      leader_commit: int
      message_id: str = ""  # For duplicate detection
  
  @dataclass
  class AppendEntriesResponse:
      """AppendEntries RPC response."""
      term: int
      success: bool
      match_index: int
      conflict_index: int = -1  # For fast log catchup
      conflict_term: int = -1
      node_id: str = ""
      message_id: str = ""
  
  @dataclass
  class RequestVoteRequest:
      """RequestVote RPC request."""
      term: int
      candidate_id: str
      last_log_index: int
      last_log_term: int
      is_prevote: bool = False  # Pre-vote extension
  
  @dataclass
  class RequestVoteResponse:
      """RequestVote RPC response."""
      term: int
      vote_granted: bool
      node_id: str = ""
  
  @dataclass
  class InstallSnapshotRequest:
      """InstallSnapshot RPC request."""
      term: int
      leader_id: str
      last_included_index: int
      last_included_term: int
      offset: int
      data: bytes
      done: bool
  
  @dataclass
  class InstallSnapshotResponse:
      """InstallSnapshot RPC response."""
      term: int
      success: bool
      node_id: str = ""
  
  @dataclass
  class ClusterConfig:
      """Cluster configuration for membership changes."""
      config_index: int
      voters: Set[str]
      learners: Set[str] = field(default_factory=set)
      old_voters: Optional[Set[str]] = None  # For joint consensus
      
      def is_joint(self) -> bool:
          return self.old_voters is not None
      
      def get_quorum_size(self) -> int:
          if self.is_joint():
              # Need majority from both old and new
              return max(
                  len(self.voters) // 2 + 1,
                  len(self.old_voters) // 2 + 1
              )
          return len(self.voters) // 2 + 1
      
      def has_quorum(self, responding: Set[str]) -> bool:
          if self.is_joint():
              old_quorum = len(responding & self.old_voters) >= len(self.old_voters) // 2 + 1
              new_quorum = len(responding & self.voters) >= len(self.voters) // 2 + 1
              return old_quorum and new_quorum
          return len(responding & self.voters) >= len(self.voters) // 2 + 1
  
  class RaftNode:
      """
      Raft consensus node with comprehensive bug fixes.
      """
      
      # Timing constants - wide range prevents split vote
      ELECTION_TIMEOUT_MIN_MS = 150
      ELECTION_TIMEOUT_MAX_MS = 300
      HEARTBEAT_INTERVAL_MS = 50
      
      def __init__(self, node_id: str, peers: Set[str], storage: 'RaftStorage'):
          self.node_id = node_id
          self.peers = peers
          self.storage = storage
          
          # Persistent state (must survive restart)
          self.current_term = storage.get_term()
          self.voted_for = storage.get_voted_for()
          self.log: List[LogEntry] = storage.get_log()
          
          # Volatile state
          self.state = NodeState.FOLLOWER
          self.commit_index = 0
          self.last_applied = 0
          self.leader_id: Optional[str] = None
          
          # Leader state
          self.next_index: Dict[str, int] = {}
          self.match_index: Dict[str, int] = {}
          
          # Timing
          self.last_heartbeat = time.monotonic()
          self.election_timeout = self._random_election_timeout()
          
          # Configuration
          self.config = ClusterConfig(0, peers | {node_id})
          self.pending_config_change = False
          
          # Snapshot state
          self.snapshot_index = 0
          self.snapshot_term = 0
          
          # Message deduplication
          self._processed_messages: Set[str] = set()
          
          self._lock = threading.RLock()
          self._apply_callbacks: List[Callable] = []
      
      def _random_election_timeout(self) -> float:
          """
          Generate random election timeout.
          Wide range is CRITICAL to prevent split votes.
          """
          return random.uniform(
              self.ELECTION_TIMEOUT_MIN_MS,
              self.ELECTION_TIMEOUT_MAX_MS
          ) / 1000.0
      
      def _persist_state(self):
          """Persist state before responding to RPCs."""
          self.storage.save_term(self.current_term)
          self.storage.save_voted_for(self.voted_for)
          self.storage.save_log(self.log)
      
      def _get_last_log_info(self) -> Tuple[int, int]:
          """Get last log index and term."""
          if not self.log:
              return self.snapshot_index, self.snapshot_term
          return len(self.log) - 1 + self.snapshot_index + 1, self.log[-1].term
      
      # === LEADER ELECTION ===
      
      def handle_request_vote(self, req: RequestVoteRequest) -> RequestVoteResponse:
          """
          Handle RequestVote RPC with proper term and log checks.
          """
          with self._lock:
              # Check term
              if req.term < self.current_term:
                  return RequestVoteResponse(
                      term=self.current_term,
                      vote_granted=False,
                      node_id=self.node_id
                  )
              
              # Update term if needed
              if req.term > self.current_term:
                  self.current_term = req.term
                  self.voted_for = None
                  self.state = NodeState.FOLLOWER
                  self._persist_state()
              
              # Pre-vote: don't update term or grant persistent vote
              if req.is_prevote:
                  # Check if we would vote for this candidate
                  would_vote = self._would_grant_vote(req)
                  return RequestVoteResponse(
                      term=self.current_term,
                      vote_granted=would_vote,
                      node_id=self.node_id
                  )
              
              # Check if we can vote for this candidate
              can_vote = (
                  self.voted_for is None or 
                  self.voted_for == req.candidate_id
              )
              
              if not can_vote:
                  return RequestVoteResponse(
                      term=self.current_term,
                      vote_granted=False,
                      node_id=self.node_id
                  )
              
              # Log up-to-date check (CRITICAL for safety)
              if not self._is_log_up_to_date(req.last_log_term, req.last_log_index):
                  return RequestVoteResponse(
                      term=self.current_term,
                      vote_granted=False,
                      node_id=self.node_id
                  )
              
              # Grant vote
              self.voted_for = req.candidate_id
              self.last_heartbeat = time.monotonic()
              self._persist_state()  # MUST persist before responding
              
              return RequestVoteResponse(
                  term=self.current_term,
                  vote_granted=True,
                  node_id=self.node_id
              )
      
      def _would_grant_vote(self, req: RequestVoteRequest) -> bool:
          """Check if we would grant vote (for pre-vote)."""
          can_vote = (
              self.voted_for is None or 
              self.voted_for == req.candidate_id
          )
          return can_vote and self._is_log_up_to_date(req.last_log_term, req.last_log_index)
      
      def _is_log_up_to_date(self, last_term: int, last_index: int) -> bool:
          """
          Check if candidate's log is at least as up-to-date as ours.
          Raft paper §5.4.1
          """
          my_last_index, my_last_term = self._get_last_log_info()
          
          if last_term != my_last_term:
              return last_term > my_last_term
          return last_index >= my_last_index
      
      def start_election(self, use_prevote: bool = True) -> bool:
          """
          Start leader election.
          Returns True if elected leader.
          """
          with self._lock:
              # Pre-vote phase (prevents term inflation during partition)
              if use_prevote:
                  if not self._run_prevote():
                      return False
              
              # Real election
              self.state = NodeState.CANDIDATE
              self.current_term += 1
              self.voted_for = self.node_id
              self.election_timeout = self._random_election_timeout()
              self._persist_state()
              
              last_index, last_term = self._get_last_log_info()
              
              # Request votes
              votes = {self.node_id}  # Self vote
              
              request = RequestVoteRequest(
                  term=self.current_term,
                  candidate_id=self.node_id,
                  last_log_index=last_index,
                  last_log_term=last_term,
                  is_prevote=False
              )
              
              for peer in self.peers:
                  if peer == self.node_id:
                      continue
                  response = self._send_request_vote(peer, request)
                  if response and response.vote_granted:
                      votes.add(peer)
                  elif response and response.term > self.current_term:
                      # Discovered higher term
                      self.current_term = response.term
                      self.voted_for = None
                      self.state = NodeState.FOLLOWER
                      self._persist_state()
                      return False
              
              # Check if won
              if self.config.has_quorum(votes):
                  self._become_leader()
                  return True
              
              self.state = NodeState.FOLLOWER
              return False
      
      def _run_prevote(self) -> bool:
          """Run pre-vote to check if election would succeed."""
          last_index, last_term = self._get_last_log_info()
          
          request = RequestVoteRequest(
              term=self.current_term + 1,  # Next term
              candidate_id=self.node_id,
              last_log_index=last_index,
              last_log_term=last_term,
              is_prevote=True
          )
          
          votes = {self.node_id}
          for peer in self.peers:
              if peer == self.node_id:
                  continue
              response = self._send_request_vote(peer, request)
              if response and response.vote_granted:
                  votes.add(peer)
          
          return self.config.has_quorum(votes)
      
      def _become_leader(self):
          """Transition to leader state."""
          self.state = NodeState.LEADER
          self.leader_id = self.node_id
          
          # Initialize leader state
          last_index = len(self.log) - 1 + self.snapshot_index + 1 if self.log else self.snapshot_index
          for peer in self.peers:
              self.next_index[peer] = last_index + 1
              self.match_index[peer] = 0
          
          # Append no-op entry to commit entries from previous terms
          noop = LogEntry(
              term=self.current_term,
              index=last_index + 1,
              command=b"",
              entry_type="noop"
          )
          self.log.append(noop)
          self._persist_state()
          
          logger.info(f"Node {self.node_id} became leader for term {self.current_term}")
      
      def _send_request_vote(self, peer: str, request: RequestVoteRequest) -> Optional[RequestVoteResponse]:
          """Send RequestVote RPC to peer (stub for testing)."""
          # In production: actual RPC call
          return None
      
      # === LOG REPLICATION ===
      
      def handle_append_entries(self, req: AppendEntriesRequest) -> AppendEntriesResponse:
          """
          Handle AppendEntries RPC with proper consistency checks.
          """
          with self._lock:
              # Duplicate detection
              if req.message_id and req.message_id in self._processed_messages:
                  # Return same response for duplicate
                  pass
              
              # Term check
              if req.term < self.current_term:
                  return AppendEntriesResponse(
                      term=self.current_term,
                      success=False,
                      match_index=self._get_last_log_info()[0],
                      node_id=self.node_id
                  )
              
              # Update state for valid leader
              if req.term > self.current_term:
                  self.current_term = req.term
                  self.voted_for = None
                  self._persist_state()
              
              self.state = NodeState.FOLLOWER
              self.leader_id = req.leader_id
              self.last_heartbeat = time.monotonic()
              self.election_timeout = self._random_election_timeout()
              
              # Log consistency check (CRITICAL)
              if req.prev_log_index > 0:
                  adjusted_index = req.prev_log_index - self.snapshot_index - 1
                  
                  if adjusted_index >= len(self.log):
                      # Log too short
                      return AppendEntriesResponse(
                          term=self.current_term,
                          success=False,
                          match_index=len(self.log) - 1 + self.snapshot_index + 1 if self.log else self.snapshot_index,
                          conflict_index=len(self.log) + self.snapshot_index + 1,
                          node_id=self.node_id
                      )
                  
                  if adjusted_index >= 0 and self.log[adjusted_index].term != req.prev_log_term:
                      # Term mismatch - find conflict
                      conflict_term = self.log[adjusted_index].term
                      conflict_index = adjusted_index
                      while conflict_index > 0 and self.log[conflict_index - 1].term == conflict_term:
                          conflict_index -= 1
                      
                      # MUST truncate conflicting entries
                      self.log = self.log[:adjusted_index]
                      self._persist_state()
                      
                      return AppendEntriesResponse(
                          term=self.current_term,
                          success=False,
                          match_index=adjusted_index - 1 + self.snapshot_index + 1 if adjusted_index > 0 else self.snapshot_index,
                          conflict_index=conflict_index + self.snapshot_index + 1,
                          conflict_term=conflict_term,
                          node_id=self.node_id
                      )
              
              # Append new entries
              for entry in req.entries:
                  if not entry.verify():
                      logger.warning(f"Entry checksum failed: {entry.index}")
                      continue
                  
                  adjusted_index = entry.index - self.snapshot_index - 1
                  
                  if adjusted_index < len(self.log):
                      if self.log[adjusted_index].term != entry.term:
                          # Conflict - truncate and append
                          self.log = self.log[:adjusted_index]
                          self.log.append(entry)
                  elif adjusted_index == len(self.log):
                      self.log.append(entry)
                  # else: gap - shouldn't happen with proper protocol
              
              self._persist_state()
              
              # Update commit index
              if req.leader_commit > self.commit_index:
                  last_index = len(self.log) - 1 + self.snapshot_index + 1 if self.log else self.snapshot_index
                  self.commit_index = min(req.leader_commit, last_index)
                  self._apply_committed()
              
              if req.message_id:
                  self._processed_messages.add(req.message_id)
              
              return AppendEntriesResponse(
                  term=self.current_term,
                  success=True,
                  match_index=len(self.log) - 1 + self.snapshot_index + 1 if self.log else self.snapshot_index,
                  node_id=self.node_id,
                  message_id=req.message_id
              )
      
      def _apply_committed(self):
          """Apply committed entries to state machine."""
          while self.last_applied < self.commit_index:
              self.last_applied += 1
              adjusted_index = self.last_applied - self.snapshot_index - 1
              
              if adjusted_index >= 0 and adjusted_index < len(self.log):
                  entry = self.log[adjusted_index]
                  for callback in self._apply_callbacks:
                      callback(entry)
      
      # === SNAPSHOT ===
      
      def handle_install_snapshot(self, req: InstallSnapshotRequest) -> InstallSnapshotResponse:
          """Handle InstallSnapshot RPC."""
          with self._lock:
              if req.term < self.current_term:
                  return InstallSnapshotResponse(
                      term=self.current_term,
                      success=False,
                      node_id=self.node_id
                  )
              
              if req.term > self.current_term:
                  self.current_term = req.term
                  self.voted_for = None
                  self._persist_state()
              
              self.state = NodeState.FOLLOWER
              self.leader_id = req.leader_id
              self.last_heartbeat = time.monotonic()
              
              # Handle snapshot chunks
              if req.offset == 0:
                  self.storage.begin_snapshot_receive()
              
              self.storage.write_snapshot_chunk(req.offset, req.data)
              
              if req.done:
                  # Finalize snapshot
                  if not self.storage.finalize_snapshot():
                      return InstallSnapshotResponse(
                          term=self.current_term,
                          success=False,
                          node_id=self.node_id
                      )
                  
                  # Discard log entries covered by snapshot
                  self.snapshot_index = req.last_included_index
                  self.snapshot_term = req.last_included_term
                  
                  # Truncate log
                  new_log = []
                  for entry in self.log:
                      if entry.index > req.last_included_index:
                          new_log.append(entry)
                  self.log = new_log
                  
                  # Update indices
                  if req.last_included_index > self.commit_index:
                      self.commit_index = req.last_included_index
                  if req.last_included_index > self.last_applied:
                      self.last_applied = req.last_included_index
                  
                  self._persist_state()
              
              return InstallSnapshotResponse(
                  term=self.current_term,
                  success=True,
                  node_id=self.node_id
              )
      
      def create_snapshot(self, state_data: bytes) -> bool:
          """Create snapshot of current state."""
          with self._lock:
              if self.last_applied <= self.snapshot_index:
                  return False  # Nothing new to snapshot
              
              # Snapshot must include last applied entry
              snapshot_index = self.last_applied
              adjusted_index = snapshot_index - self.snapshot_index - 1
              
              if adjusted_index < 0 or adjusted_index >= len(self.log):
                  return False
              
              snapshot_term = self.log[adjusted_index].term
              
              # Save snapshot atomically
              success = self.storage.save_snapshot(
                  snapshot_index,
                  snapshot_term,
                  state_data
              )
              
              if success:
                  # Truncate log
                  self.log = self.log[adjusted_index + 1:]
                  self.snapshot_index = snapshot_index
                  self.snapshot_term = snapshot_term
                  self._persist_state()
              
              return success
      
      # === MEMBERSHIP CHANGES ===
      
      def propose_config_change(self, new_voters: Set[str]) -> bool:
          """
          Propose membership change using joint consensus.
          """
          with self._lock:
              if self.state != NodeState.LEADER:
                  return False
              
              if self.pending_config_change:
                  return False  # Only one change at a time!
              
              # Create joint configuration
              joint_config = ClusterConfig(
                  config_index=len(self.log) + self.snapshot_index + 1,
                  voters=new_voters,
                  old_voters=self.config.voters.copy()
              )
              
              # Append joint config entry
              entry = LogEntry(
                  term=self.current_term,
                  index=len(self.log) + self.snapshot_index + 1,
                  command=self._serialize_config(joint_config),
                  entry_type="config"
              )
              self.log.append(entry)
              self.pending_config_change = True
              self._persist_state()
              
              return True
      
      def _on_config_committed(self, config: ClusterConfig):
          """Called when configuration is committed."""
          if config.is_joint():
              # Joint config committed - now commit new config
              new_config = ClusterConfig(
                  config_index=len(self.log) + self.snapshot_index + 1,
                  voters=config.voters
              )
              
              entry = LogEntry(
                  term=self.current_term,
                  index=len(self.log) + self.snapshot_index + 1,
                  command=self._serialize_config(new_config),
                  entry_type="config"
              )
              self.log.append(entry)
              self._persist_state()
          else:
              # New config committed
              self.config = config
              self.pending_config_change = False
              
              # Step down if not in new config
              if self.node_id not in config.voters:
                  self.state = NodeState.FOLLOWER
      
      def _serialize_config(self, config: ClusterConfig) -> bytes:
          """Serialize configuration."""
          import json
          data = {
              'voters': list(config.voters),
              'learners': list(config.learners),
              'old_voters': list(config.old_voters) if config.old_voters else None
          }
          return json.dumps(data).encode()
  
  class RaftStorage:
      """Persistent storage for Raft state."""
      
      def __init__(self, path: str):
          self.path = path
          self._term = 0
          self._voted_for: Optional[str] = None
          self._log: List[LogEntry] = []
      
      def get_term(self) -> int:
          return self._term
      
      def save_term(self, term: int):
          self._term = term
      
      def get_voted_for(self) -> Optional[str]:
          return self._voted_for
      
      def save_voted_for(self, voted_for: Optional[str]):
          self._voted_for = voted_for
      
      def get_log(self) -> List[LogEntry]:
          return self._log.copy()
      
      def save_log(self, log: List[LogEntry]):
          self._log = log.copy()
      
      def begin_snapshot_receive(self):
          pass
      
      def write_snapshot_chunk(self, offset: int, data: bytes):
          pass
      
      def finalize_snapshot(self) -> bool:
          return True
      
      def save_snapshot(self, index: int, term: int, data: bytes) -> bool:
          return True
  
  # === TESTS ===
  
  def test_election_timeout_randomization():
      """Test that election timeout has sufficient randomization."""
      timeouts = []
      for _ in range(1000):
          timeout = random.uniform(
              RaftNode.ELECTION_TIMEOUT_MIN_MS,
              RaftNode.ELECTION_TIMEOUT_MAX_MS
          )
          timeouts.append(timeout)
      
      min_timeout = min(timeouts)
      max_timeout = max(timeouts)
      range_ms = max_timeout - min_timeout
      
      # Range should be at least 100ms to prevent split votes
      assert range_ms >= 100, f"Timeout range too narrow: {range_ms}ms"
      print("✓ Election timeout randomization sufficient")
  
  def test_log_consistency_check():
      """Test log consistency verification."""
      storage = RaftStorage("/tmp/raft")
      node = RaftNode("node1", {"node1", "node2", "node3"}, storage)
      
      # Add some entries
      node.log = [
          LogEntry(1, 1, b"cmd1"),
          LogEntry(1, 2, b"cmd2"),
          LogEntry(2, 3, b"cmd3"),
      ]
      
      # Valid append
      req = AppendEntriesRequest(
          term=2,
          leader_id="node2",
          prev_log_index=3,
          prev_log_term=2,
          entries=[LogEntry(2, 4, b"cmd4")],
          leader_commit=0
      )
      resp = node.handle_append_entries(req)
      assert resp.success, "Should accept valid append"
      
      # Invalid prev_log_term
      node.log = [
          LogEntry(1, 1, b"cmd1"),
          LogEntry(1, 2, b"cmd2"),
          LogEntry(2, 3, b"cmd3"),
      ]
      req = AppendEntriesRequest(
          term=2,
          leader_id="node2",
          prev_log_index=3,
          prev_log_term=1,  # Wrong term!
          entries=[LogEntry(2, 4, b"cmd4")],
          leader_commit=0
      )
      resp = node.handle_append_entries(req)
      assert not resp.success, "Should reject mismatched term"
      assert len(node.log) < 3, "Should truncate conflicting entries"
      
      print("✓ Log consistency check works")
  
  def test_vote_persistence():
      """Test that votes are persisted before responding."""
      storage = RaftStorage("/tmp/raft")
      node = RaftNode("node1", {"node1", "node2", "node3"}, storage)
      
      req = RequestVoteRequest(
          term=1,
          candidate_id="node2",
          last_log_index=0,
          last_log_term=0
      )
      
      resp = node.handle_request_vote(req)
      assert resp.vote_granted, "Should grant vote"
      assert storage.get_voted_for() == "node2", "Vote should be persisted"
      
      print("✓ Vote persistence works")
  
  if __name__ == "__main__":
      test_election_timeout_randomization()
      test_log_consistency_check()
      test_vote_persistence()
      print("\nAll tests passed!")

# LLM trap configurations
traps:
  - type: "split_vote"
    description: "Randomized election timeout too narrow, causing repeated split votes"
    trigger: "Insufficient randomization in election timeout"
  
  - type: "log_divergence"
    description: "Follower accepts entries from old term after leader change"
    trigger: "Not checking term in AppendEntries"
  
  - type: "membership_race"
    description: "Configuration change committed during another change"
    trigger: "Not using joint consensus for membership changes"
  
  - type: "snapshot_race"
    description: "Log compacted while being replicated"
    trigger: "Not coordinating snapshots with replication"
  
  - type: "missing_persistence"
    description: "Vote not persisted before responding"
    trigger: "Can vote twice for different candidates after crash"
  
  - type: "no_noop_on_election"
    description: "Leader doesn't append no-op, can't commit previous term entries"
    trigger: "Entries from previous term never become committed"

# Task generation template
instruction_template: |
  You are debugging a {{ scenario_type }} consensus implementation.
  The code is at {{ path }}.
  
  Consensus failures: {{ failure_count }} in last 24 hours
  Cluster size: {{ cluster_size }} nodes
  
  Your task:
  {{ task_steps }}

# Test cases
fail_to_pass:
  - "test_split_vote_prevention"
  - "test_log_consistency_after_partition"
  - "test_membership_change_safety"
  - "test_snapshot_replication_race"
  - "test_vote_persistence"
  - "test_noop_on_election"

pass_to_pass:
  - "test_basic_consensus"
  - "test_leader_election"
  - "test_log_replication"

# Variables for task generation
variables:
  - name: scenario_type
    type: string
    options: 
      - "distributed database"
      - "configuration store"
      - "lock service"
      - "metadata service"
      - "coordination service"
      - "distributed queue"
      - "replicated state machine"
  - name: path
    type: path
    generator: random_path
  - name: failure_count
    type: int
    min: 5
    max: 500
  - name: cluster_size
    type: int
    min: 3
    max: 9
  - name: task_steps
    type: template
    value: |
      1. Analyze consensus protocol implementation for violations
      2. Verify term handling in all RPC handlers
      3. Fix split vote scenarios with proper timeout randomization
      4. Ensure log consistency guarantees with proper checks
      5. Handle membership changes safely with joint consensus
      6. Add message integrity verification and duplicate detection
      7. Implement proper state persistence before RPC responses

# Anti-hardcoding measures
anti_hardcoding:
  canary_tokens: true
  randomize_paths: true
  dynamic_content: true
  consensus_bugs:
    - narrow_election_timeout
    - missing_term_check
    - unsafe_membership_change
    - snapshot_race_condition
    - missing_vote_persistence
    - no_noop_entry

# Anti-patterns that LLMs commonly fail on
anti_patterns:
  llm_failure_modes:
    - "Applying userspace patterns to kernel-level problems"
    - "Missing memory ordering and barrier requirements"
    - "Ignoring NUMA topology effects on performance"
    - "Not considering scheduler behavior under load"
    - "Missing ABA problems in lock-free data structures"
    - "Overlooking signal handler safety restrictions"
    - "Assuming atomic operations are always sufficient"
    - "Missing file descriptor inheritance across fork/exec"
    - "Ignoring distributed consensus edge cases (Byzantine failures)"
    - "Insufficient randomization in election timeout causing split votes"
    - "Not persisting vote before responding to vote request"
    - "Missing prevLogTerm check in AppendEntries RPC"
    - "Not appending no-op entry on leader election"
    - "Allowing concurrent configuration changes"
    - "Incorrect commit index calculation with minority"
    - "Missing term check in RPC response handling"
    - "Not handling message reordering and duplicates"
    - "Snapshot installed without verifying log consistency"
    - "Leader lease not accounting for clock skew"
    - "Not using read index for linearizable reads"
    - "Missing checksum verification on log entries"
    - "Ignoring fsync failures in persistence layer"
