id: "sys-mem-fragmentation-001"
version: "1.0.0"
category: "systems"
subcategory: "memory-management"

# SWE-bench_Pro style fields
problem_statement: |
  A long-running service is experiencing memory allocation failures despite having 
  significant free memory reported by the system. The issue is memory fragmentation.
  
  Hidden causes:
  1. External fragmentation from varied allocation sizes
  2. Internal fragmentation from fixed-size allocators
  3. Memory pools not returning memory to OS
  4. Allocator metadata overhead grows over time
  5. Large allocations fail while small ones succeed

requirements: |
  - Diagnose fragmentation type (internal vs external)
  - Implement memory pool with defragmentation
  - Use slab allocator for common object sizes
  - Configure allocator to return memory to OS
  - Add fragmentation metrics and monitoring

interface: |
  Input: Memory allocation patterns, allocation sizes
  Output: Optimized memory allocator, fragmentation report
  Metrics: Fragmentation ratio, allocation success rate

# terminal-bench style fields
difficulty:
  estimated: "hard"
  time_range: [2700, 7200]
  command_steps: [30, 90]

# === QUALITY REQUIREMENTS ===
quality_requirements:
  minimum_time: "120-300 minutes for senior systems engineers with kernel contribution experience"
  expertise_level: "Principal systems engineer with memory allocator internals and NUMA expertise"
  validation_standard: "Must expose subtle allocator behavior, fragmentation patterns, and kernel memory subsystem interactions"

# === MULTI-AGENT ORCHESTRATION ===
multi_agent_orchestration:
  required_agents: 9
  agent_specializations:
    - name: "kernel_slab_expert"
      role: "Debug kernel slab allocator (SLUB/SLAB) issues"
      expertise: ["slab caches", "object caching", "per-cpu arrays", "slab coloring"]
    - name: "buddy_allocator_specialist"
      role: "Debug page allocator and buddy system"
      expertise: ["buddy algorithm", "zone allocators", "watermarks", "compaction"]
    - name: "userspace_allocator_expert"
      role: "Debug jemalloc/tcmalloc/glibc malloc"
      expertise: ["arena management", "thread caches", "size classes", "decay policy"]
    - name: "huge_page_debugger"
      role: "Debug transparent huge pages and explicit huge pages"
      expertise: ["THP", "khugepaged", "huge page defragmentation", "hugetlbfs"]
    - name: "numa_memory_analyst"
      role: "Debug NUMA memory allocation and balancing"
      expertise: ["memory policies", "numa_balancing", "interleaving", "local allocation"]
    - name: "memory_compaction_expert"
      role: "Debug memory compaction and defragmentation"
      expertise: ["compact_zone", "migration scanner", "free scanner", "mobility groups"]
    - name: "reclaim_specialist"
      role: "Debug memory reclaim paths"
      expertise: ["direct reclaim", "kswapd", "OOM killer", "vmscan"]
    - name: "perf_memory_tracer"
      role: "Profile memory allocation patterns"
      expertise: ["perf mem", "eBPF tracing", "memleak", "memory flamegraphs"]
    - name: "application_memory_analyzer"
      role: "Analyze application memory usage patterns"
      expertise: ["allocation profiling", "memory access patterns", "working set", "RSS tracking"]
  cross_subsystem_chains:
    - "High allocation rate → Arena fragmentation → mmap threshold → VMA fragmentation → THP disabled"
    - "Mixed workload → SLUB fragmentation → Page allocator pressure → OOM despite free memory"
    - "NUMA imbalance → Remote access → Cache pollution → Performance degradation"
    - "THP enabled → Internal fragmentation → Memory pressure → Compaction failure"
  parallel_debugging_requirements:
    - "Correlated allocation traces with fragmentation metrics"
    - "Simultaneous userspace and kernel allocator analysis"
    - "Agent handoff for end-to-end memory lifecycle debugging"

# === TRAP CONFIGURATION ===
trap_configuration:
  trap_count: "12+ deeply interacting traps across kernel/userspace/hardware boundaries"
  trap_categories:
    external_fragmentation_traps:
      - "Buddy allocator fragmentation preventing huge page allocation"
      - "VMA fragmentation blocking contiguous mmap"
      - "Arena retention causing RSS growth without actual use"
      - "Memory not returned to OS despite allocator free"
    internal_fragmentation_traps:
      - "Power-of-two size classes wasting 50% on small allocations"
      - "Slab internal fragmentation from object size mismatch"
      - "THP internal fragmentation with sparse access"
      - "Alignment padding overhead in NUMA allocations"
    compaction_traps:
      - "Unmovable pages preventing compaction success"
      - "CMA contention with regular allocations"
      - "Compaction latency spike during allocation"
      - "Page pinning blocking defragmentation"

# === NIGHTMARE_PLUS DIFFICULTY ===
difficulty_levels:
  nightmare_plus:
    estimated_time: [43200, 259200]  # 12-72 hours
    command_steps: [600, 2500]
    techniques_required: 18
    description: "Linux mm subsystem maintainer-level difficulty requiring deep internals knowledge"
    requirements:
      - "Requires understanding of complete memory allocator hierarchy"
      - "Must handle fragmentation across kernel and userspace allocators"
      - "Involves NUMA-aware allocation and cross-node effects"
      - "Cross-platform behavior differences (Linux vs BSD vs Windows)"
      - "Time estimate: 12-72 hours for memory subsystem experts"
      - "Requires synthesizing allocator, VM, and hardware knowledge"

# === KERNEL SUBSYSTEM REQUIREMENTS ===
kernel_subsystem_requirements:
  page_allocator:
    - "Buddy algorithm and order-N allocation"
    - "Zone allocators (DMA, DMA32, Normal, Movable)"
    - "Watermarks and allocation flags (GFP_*)"
    - "Page mobility and migration types"
  slab_allocator:
    - "SLUB freelist management and partial slabs"
    - "Per-CPU slab caches and batch refill"
    - "Object constructor/destructor and SLAB_POISON"
    - "Slab merging and debugging options"
  memory_compaction:
    - "Compaction daemon (kcompactd)"
    - "Synchronous vs asynchronous compaction"
    - "Migration scanner and free scanner algorithm"
    - "Compaction deferrals and order-0 fallback"
  huge_pages:
    - "THP collapse and split operations"
    - "khugepaged scanning and collapse decisions"
    - "Huge page reservation and pool management"
    - "Direct allocation vs background collapse"

# === HARDWARE INTERACTION ===
hardware_interaction:
  memory_controller:
    - "DDR interleaving and bank conflicts"
    - "Memory channel bandwidth effects"
    - "Row buffer hit rate and access patterns"
    - "Memory controller queue depth effects"
  cache_hierarchy:
    - "Cache line size alignment requirements"
    - "Cache coloring to reduce conflicts"
    - "LLC (Last Level Cache) partitioning"
    - "Prefetcher effects on allocation patterns"
  numa_topology:
    - "Node distance and interconnect bandwidth"
    - "Local vs remote memory access latency"
    - "Memory interleaving across nodes"
    - "Per-node memory pressure differences"

# === FORMAL VERIFICATION REQUIREMENTS ===
formal_verification_requirements:
  modeling_requirements:
    - "Model allocator state machine in TLA+"
    - "Express coalescing correctness properties"
    - "Verify bounded fragmentation under workload"
    - "Model memory return policy"
  proof_obligations:
    - "Prove coalescing maintains heap invariants"
    - "Verify size class coverage minimizes waste"
    - "Establish bounded external fragmentation"
    - "Prove compaction progress guarantees"

# === LLM GENERATION FRAMEWORK ===
generation_framework:
  multi_conversation_workflow:
    phase_1_research: "Research allocator designs and fragmentation causes"
    phase_2_creation: "Create task with subtle fragmentation patterns"
    phase_3_amplification: "Add long-running scenarios with allocation patterns"
    phase_4_verification:
      description: "Validate task requires understanding of memory allocators"
      criteria:
        - "Has at least 5 interacting system-level traps across subsystems"
        - "Has cascading failures across process, memory, and I/O subsystems"
        - "Requires knowledge of OS kernel internals and scheduler behavior"
        - "Would take experienced systems programmers 45+ minutes"
  
  difficulty_amplifiers:
    nightmare:
      multiplier: 3.5
      description: "Kernel-level debugging complexity requiring deep systems expertise"
      requirements:
        - "7+ interacting system-level traps across subsystems"
        - "Requires understanding of kernel memory management and scheduling"
        - "Time estimate: 120+ minutes for senior systems engineers"
        - "Cross-platform behavior differences (Linux vs BSD vs Windows)"
        - "Requires synthesizing concurrency, memory, distributed systems knowledge"
  
  generation_targets:
    minimum_difficulty: "45+ minutes, requires deep kernel internals and distributed systems expertise"
  
  complexity_levels:
    level_1_basic:
      description: "Simple external fragmentation from random allocations"
      elements: ["varied_sizes", "no_coalescing"]
    level_2_intermediate:
      description: "Internal fragmentation with size class allocators"
      elements: ["power_of_two_classes", "poor_fit"]
    level_3_advanced:
      description: "Memory not returned to OS"
      elements: ["allocator_retention", "mmap_threshold"]
    level_4_expert:
      description: "NUMA-aware allocation and huge page fragmentation"
      elements: ["numa_imbalance", "thp_fragmentation"]
    level_5_research:
      description: "Novel fragmentation in modern allocators"
      elements: ["jemalloc_tuning", "tcmalloc_optimization"]

# === COMPREHENSIVE TOPIC UNIVERSE ===
topic_universe:
  # Allocator Designs (40+ topics)
  allocator_designs:
    classic_allocators:
      - "First-fit allocator"
      - "Best-fit allocator"
      - "Worst-fit allocator"
      - "Next-fit allocator"
      - "Segregated free lists"
      - "Buddy allocator"
      - "Bitmap allocator"
    
    modern_allocators:
      - "jemalloc architecture"
      - "tcmalloc (Google)"
      - "mimalloc (Microsoft)"
      - "Hoard allocator"
      - "ptmalloc2 (glibc)"
      - "musl allocator"
      - "dlmalloc (Doug Lea)"
    
    specialized_allocators:
      - "Slab allocator (kernel)"
      - "SLUB allocator"
      - "SLOB allocator (embedded)"
      - "Arena allocator"
      - "Pool allocator"
      - "Stack allocator (bump)"
      - "Object caches"
    
    language_runtime:
      - "Go GC and allocator"
      - "JVM heap management"
      - ".NET GC generations"
      - "Python object allocator"
      - "Rust global allocator"
  
  # Fragmentation Types (25+ topics)
  fragmentation_types:
    external_fragmentation:
      - "Free memory scattered"
      - "Coalescing adjacent free blocks"
      - "Compaction techniques"
      - "Memory shuffling"
      - "Large allocation failures"
    
    internal_fragmentation:
      - "Fixed-size block waste"
      - "Power-of-two size classes"
      - "Alignment padding"
      - "Metadata overhead"
      - "Guard pages overhead"
    
    virtual_memory_fragmentation:
      - "Virtual address space fragmentation"
      - "Page table fragmentation"
      - "THP fragmentation"
      - "Memory-mapped file fragmentation"
  
  # Memory Allocation Strategies (30+ topics)
  allocation_strategies:
    size_classes:
      - "Power-of-two classes"
      - "Fibonacci classes"
      - "Custom size classes"
      - "Thread-local size classes"
      - "Per-CPU caches"
    
    coalescing:
      - "Immediate coalescing"
      - "Deferred coalescing"
      - "Boundary tags"
      - "Footer coalescing"
      - "Binning strategies"
    
    splitting:
      - "Block splitting policies"
      - "Minimum split size"
      - "Split avoidance"
      - "Exact-fit preference"
    
    caching:
      - "Thread caches"
      - "Central cache"
      - "Transfer batches"
      - "Magazine caches"
      - "Object caching"
  
  # System Memory Management (35+ topics)
  system_memory:
    virtual_memory:
      - "Page allocation"
      - "Demand paging"
      - "Copy-on-write"
      - "Memory overcommit"
      - "OOM killer"
    
    huge_pages:
      - "Explicit huge pages"
      - "Transparent huge pages (THP)"
      - "Huge page fragmentation"
      - "khugepaged"
      - "madvise(MADV_HUGEPAGE)"
    
    numa:
      - "NUMA topology"
      - "Local vs remote allocation"
      - "NUMA balancing"
      - "Memory policies"
      - "numactl"
    
    memory_return:
      - "madvise(MADV_DONTNEED)"
      - "madvise(MADV_FREE)"
      - "MADV_PURGE_PAGES"
      - "Decommit pages"
      - "Trim threshold"
  
  # Measurement and Monitoring (25+ topics)
  measurement:
    metrics:
      - "Fragmentation ratio"
      - "Largest free block"
      - "Free block distribution"
      - "Allocation success rate"
      - "Memory utilization"
    
    tools:
      - "/proc/meminfo"
      - "/proc/buddyinfo"
      - "/proc/pagetypeinfo"
      - "/proc/slabinfo"
      - "slabtop"
      - "vmstat"
      - "pmap"
    
    profilers:
      - "Valgrind massif"
      - "heaptrack"
      - "jemalloc profiling"
      - "tcmalloc profiling"
      - "eBPF memory tracing"
  
  # Mitigation Techniques (30+ topics)
  mitigation:
    allocator_tuning:
      - "Arena configuration"
      - "Size class tuning"
      - "Thread cache size"
      - "Return-to-OS threshold"
      - "Decay time configuration"
    
    application_patterns:
      - "Object pooling"
      - "Allocation batching"
      - "Pre-allocation"
      - "Reuse patterns"
      - "Avoid mixed sizes"
    
    defragmentation:
      - "Memory compaction"
      - "Copying GC"
      - "Mark-compact GC"
      - "Incremental compaction"
      - "Concurrent compaction"

# === BUG PATTERNS ===
bug_patterns:
  external_fragmentation:
    - pattern: "no_coalescing"
      description: "Freed blocks not merged with adjacent free blocks"
      severity: "high"
      detection: "Many small free blocks, large allocation fails"
      fix: "Implement immediate coalescing with boundary tags"
      code_example: |
        # Bug: no coalescing
        def free(block):
            free_list.append(block)  # Just add to list
        
        # Fix: coalesce adjacent blocks
        def free(block):
            # Check if previous block is free
            prev = get_prev_block(block)
            if prev and prev.is_free:
                prev.size += block.size
                block = prev
            # Check if next block is free
            next = get_next_block(block)
            if next and next.is_free:
                block.size += next.size
                free_list.remove(next)
            free_list.append(block)
    
    - pattern: "first_fit_fragmentation"
      description: "First-fit always allocates from front, fragmenting"
      severity: "medium"
      detection: "Fragmentation worse at start of heap"
      fix: "Use address-ordered best-fit or segregated lists"

  internal_fragmentation:
    - pattern: "power_of_two_waste"
      description: "33-byte allocation uses 64-byte block (48% waste)"
      severity: "medium"
      detection: "High internal fragmentation ratio"
      fix: "Use finer-grained size classes"
      code_example: |
        # Bug: power-of-two only
        SIZE_CLASSES = [8, 16, 32, 64, 128, 256, 512]
        # 33-byte object → 64-byte class (48% waste)
        
        # Fix: finer granularity
        SIZE_CLASSES = [8, 16, 24, 32, 48, 64, 80, 96, 128, ...]
        # 33-byte object → 48-byte class (31% waste)
    
    - pattern: "alignment_overhead"
      description: "Every allocation padded to 16 bytes, small allocations waste"
      severity: "low"
      detection: "High overhead for small objects"
      fix: "Separate allocation path for small objects"

  memory_retention:
    - pattern: "no_return_to_os"
      description: "Freed memory kept by allocator, not returned to OS"
      severity: "high"
      detection: "Process RSS stays high after freeing"
      fix: "Configure allocator to return memory, use madvise"
      code_example: |
        # jemalloc: enable memory returning
        export MALLOC_CONF="dirty_decay_ms:1000,muzzy_decay_ms:1000"
        
        # tcmalloc: aggressive release
        MallocExtension::instance()->ReleaseFreeMemory();
        
        # glibc: trim
        malloc_trim(0);
    
    - pattern: "mmap_threshold_too_high"
      description: "Large allocations use brk, can't be returned independently"
      severity: "medium"
      detection: "Large blocks can't be returned"
      fix: "Lower mmap threshold"

  allocator_bugs:
    - pattern: "arena_imbalance"
      description: "Threads stuck on overloaded arena"
      severity: "medium"
      detection: "Lock contention on allocation"
      fix: "More arenas, better thread-arena mapping"
    
    - pattern: "metadata_bloat"
      description: "Per-allocation metadata grows unbounded"
      severity: "high"
      detection: "Metadata uses significant memory"
      fix: "Compact metadata, periodic cleanup"

# === EDGE CASES ===
edge_cases:
  allocation_pattern_edge_cases:
    - "Millions of tiny allocations"
    - "Few very large allocations"
    - "Alternating large and small"
    - "LIFO allocation/free pattern"
    - "FIFO allocation/free pattern"
    - "Random allocation/free pattern"
    - "Phase-based allocation bursts"
    - "Long-lived vs short-lived mixed"

  size_edge_cases:
    - "1-byte allocation"
    - "Exact size class boundary"
    - "Just over size class boundary"
    - "Maximum allocation size"
    - "Zero-size allocation"
    - "Negative size (error handling)"

  system_edge_cases:
    - "Near memory limit"
    - "Swap pressure"
    - "NUMA imbalance"
    - "Huge page availability"
    - "Address space exhaustion"
    - "Fork with large heap"

  timing_edge_cases:
    - "Allocation during GC"
    - "Concurrent allocation from many threads"
    - "Rapid allocation/free cycles"
    - "Long gaps between allocations"

# === DIFFICULTY MULTIPLIERS ===
difficulty_multipliers:
  allocator_complexity:
    custom_allocator:
      multiplier: 2.0
      description: "Must implement custom allocator"
      considerations:
        - "Thread safety"
        - "Coalescing"
        - "Size classes"
    
    tune_existing:
      multiplier: 1.5
      description: "Tune jemalloc/tcmalloc"
      considerations:
        - "Configuration options"
        - "Runtime adjustment"
        - "Profiling"

  system_requirements:
    numa_aware:
      multiplier: 1.8
      description: "Must handle NUMA topology"
      considerations:
        - "Local allocation"
        - "Migration policies"
        - "Balancing"
    
    real_time:
      multiplier: 2.0
      description: "Bounded allocation time"
      considerations:
        - "No blocking"
        - "Pre-allocation"
        - "Pool sizing"

# === SCENARIO TEMPLATES ===
scenario_templates:
  external_fragmentation_classic:
    description: "Free memory exists but large allocation fails"
    setup: |
      - Allocate many varied-size objects
      - Free every other object
      - Try to allocate large contiguous block
    symptoms: |
      - malloc returns NULL for large request
      - Plenty of free memory reported
      - Many small free blocks
    root_cause: "External fragmentation, no compaction"
    fix: |
      - Enable coalescing
      - Use memory pool for common sizes
      - Pre-allocate large blocks

  internal_fragmentation_waste:
    description: "High memory usage due to size class waste"
    setup: |
      - Many 33-byte allocations
      - Using power-of-two allocator
      - Each uses 64-byte slot
    symptoms: |
      - ~50% memory wasted
      - High RSS for data size
    root_cause: "Poor size class fit"
    fix: |
      - Add intermediate size classes
      - Use slab for this size
      - Custom allocator

  memory_not_returned:
    description: "Process memory doesn't decrease after freeing"
    setup: |
      - Allocate 1GB of objects
      - Free all objects
      - RSS still shows 1GB
    symptoms: |
      - Process hogging memory
      - OOM despite free
    root_cause: "Allocator retaining memory"
    fix: |
      - malloc_trim()
      - Configure decay time
      - madvise(MADV_DONTNEED)

# === REFERENCE SOLUTION ===
reference_solution: |
  #!/usr/bin/env python3
  """
  Comprehensive memory fragmentation analysis and mitigation.
  Implements slab allocator, buddy allocator, and memory pool with defragmentation.
  """
  import mmap
  import ctypes
  import threading
  from typing import Dict, List, Optional, Tuple, Set, Any
  from dataclasses import dataclass, field
  from collections import defaultdict
  from abc import ABC, abstractmethod
  import logging
  import math
  
  logging.basicConfig(level=logging.INFO)
  logger = logging.getLogger(__name__)
  
  # === ALLOCATION STATS ===
  
  @dataclass
  class AllocationStats:
      """Statistics about allocator state."""
      total_allocated: int = 0
      total_freed: int = 0
      current_in_use: int = 0
      allocation_count: int = 0
      free_count: int = 0
      failed_allocations: int = 0
      external_fragmentation: float = 0.0
      internal_fragmentation: float = 0.0
      largest_free_block: int = 0
  
  # === SLAB ALLOCATOR ===
  
  class SlabAllocator:
      """
      Slab allocator for fixed-size objects.
      Eliminates external fragmentation for objects of this size.
      Minimizes internal fragmentation by exact-fit allocation.
      """
      
      def __init__(self, object_size: int, objects_per_slab: int = 64):
          self.object_size = max(object_size, 8)  # Minimum 8 bytes
          self.objects_per_slab = objects_per_slab
          self.slab_size = self.object_size * objects_per_slab
          
          self._slabs: List[memoryview] = []
          self._slab_maps: List[mmap.mmap] = []  # Keep references
          self._free_objects: List[Tuple[int, int]] = []  # (slab_idx, obj_idx)
          self._lock = threading.Lock()
          self._stats = AllocationStats()
          
          # Pre-allocate first slab
          self._allocate_slab()
      
      def _allocate_slab(self) -> int:
          """Allocate a new slab, returns slab index."""
          try:
              # Use mmap for page-aligned allocation
              slab_map = mmap.mmap(-1, self.slab_size)
              slab_idx = len(self._slabs)
              self._slab_maps.append(slab_map)
              self._slabs.append(memoryview(slab_map))
              
              # Add all objects to free list
              for i in range(self.objects_per_slab):
                  self._free_objects.append((slab_idx, i))
              
              return slab_idx
          except Exception as e:
              logger.error(f"Failed to allocate slab: {e}")
              return -1
      
      def allocate(self) -> Optional[Tuple[int, int, memoryview]]:
          """
          Allocate an object from the slab.
          Returns (slab_idx, obj_idx, memoryview) or None if failed.
          """
          with self._lock:
              if not self._free_objects:
                  if self._allocate_slab() < 0:
                      self._stats.failed_allocations += 1
                      return None
              
              slab_idx, obj_idx = self._free_objects.pop()
              offset = obj_idx * self.object_size
              obj_view = self._slabs[slab_idx][offset:offset + self.object_size]
              
              self._stats.allocation_count += 1
              self._stats.total_allocated += self.object_size
              self._stats.current_in_use += self.object_size
              
              return (slab_idx, obj_idx, obj_view)
      
      def free(self, slab_idx: int, obj_idx: int) -> bool:
          """Return object to slab."""
          with self._lock:
              if slab_idx >= len(self._slabs):
                  return False
              if obj_idx >= self.objects_per_slab:
                  return False
              
              # Check for double-free (optional, for debugging)
              if (slab_idx, obj_idx) in self._free_objects:
                  logger.warning(f"Double free detected: slab={slab_idx}, obj={obj_idx}")
                  return False
              
              self._free_objects.append((slab_idx, obj_idx))
              
              self._stats.free_count += 1
              self._stats.total_freed += self.object_size
              self._stats.current_in_use -= self.object_size
              
              return True
      
      def get_fragmentation(self) -> Tuple[float, float]:
          """
          Calculate fragmentation ratios.
          Returns (external_fragmentation, internal_fragmentation).
          """
          with self._lock:
              total_capacity = len(self._slabs) * self.objects_per_slab * self.object_size
              in_use = self._stats.current_in_use
              
              if total_capacity == 0:
                  return (0.0, 0.0)
              
              # External fragmentation: minimal for slab allocator
              # (free objects are always usable for this size)
              free_objects = len(self._free_objects)
              external = 0.0  # Slab eliminates external fragmentation
              
              # Internal fragmentation: depends on actual object size vs slot size
              # If user objects are smaller than slot, that's internal fragmentation
              internal = 0.0  # We assume exact fit
              
              return (external, internal)
      
      def get_stats(self) -> AllocationStats:
          """Get current allocation statistics."""
          with self._lock:
              stats = AllocationStats(
                  total_allocated=self._stats.total_allocated,
                  total_freed=self._stats.total_freed,
                  current_in_use=self._stats.current_in_use,
                  allocation_count=self._stats.allocation_count,
                  free_count=self._stats.free_count,
                  failed_allocations=self._stats.failed_allocations
              )
              ext, int_ = self.get_fragmentation()
              stats.external_fragmentation = ext
              stats.internal_fragmentation = int_
              stats.largest_free_block = self.object_size if self._free_objects else 0
              return stats
  
  # === BUDDY ALLOCATOR ===
  
  class BuddyAllocator:
      """
      Buddy allocator to reduce external fragmentation.
      Allows efficient coalescing of adjacent free blocks.
      """
      
      def __init__(self, total_size: int, min_block_size: int = 64):
          # Round up to power of 2
          self.total_size = 1 << (total_size - 1).bit_length()
          self.min_block = min_block_size
          self.max_order = (self.total_size // self.min_block).bit_length() - 1
          
          # Free lists for each order (size = min_block * 2^order)
          self._free_lists: Dict[int, List[int]] = defaultdict(list)
          
          # Block metadata: offset -> (order, is_free)
          self._blocks: Dict[int, Tuple[int, bool]] = {}
          
          # Initial single block of max order
          self._free_lists[self.max_order].append(0)
          self._blocks[0] = (self.max_order, True)
          
          # Backing memory
          self._memory = mmap.mmap(-1, self.total_size)
          self._lock = threading.Lock()
          self._stats = AllocationStats()
      
      def _order_for_size(self, size: int) -> int:
          """Find minimum order that can hold requested size."""
          size = max(size, self.min_block)
          # Find order where min_block * 2^order >= size
          blocks_needed = (size + self.min_block - 1) // self.min_block
          order = max(0, (blocks_needed - 1).bit_length())
          return min(order, self.max_order)
      
      def _size_for_order(self, order: int) -> int:
          """Get block size for given order."""
          return self.min_block << order
      
      def _buddy_offset(self, offset: int, order: int) -> int:
          """Calculate buddy's offset."""
          return offset ^ self._size_for_order(order)
      
      def allocate(self, size: int) -> Optional[int]:
          """
          Allocate block of at least 'size' bytes.
          Returns offset or None if failed.
          """
          with self._lock:
              target_order = self._order_for_size(size)
              
              # Find smallest sufficient free block
              for order in range(target_order, self.max_order + 1):
                  if self._free_lists[order]:
                      offset = self._free_lists[order].pop()
                      
                      # Split larger blocks down to target size
                      while order > target_order:
                          order -= 1
                          buddy_offset = offset + self._size_for_order(order)
                          self._free_lists[order].append(buddy_offset)
                          self._blocks[buddy_offset] = (order, True)
                      
                      self._blocks[offset] = (target_order, False)
                      
                      actual_size = self._size_for_order(target_order)
                      self._stats.allocation_count += 1
                      self._stats.total_allocated += actual_size
                      self._stats.current_in_use += actual_size
                      
                      return offset
              
              self._stats.failed_allocations += 1
              return None
      
      def free(self, offset: int) -> bool:
          """Free block and coalesce with buddy if possible."""
          with self._lock:
              if offset not in self._blocks:
                  return False
              
              order, is_free = self._blocks[offset]
              if is_free:
                  logger.warning(f"Double free at offset {offset}")
                  return False
              
              actual_size = self._size_for_order(order)
              self._stats.free_count += 1
              self._stats.total_freed += actual_size
              self._stats.current_in_use -= actual_size
              
              # Coalesce with buddy
              while order < self.max_order:
                  buddy_offset = self._buddy_offset(offset, order)
                  
                  if buddy_offset not in self._blocks:
                      break
                  
                  buddy_order, buddy_free = self._blocks[buddy_offset]
                  if not buddy_free or buddy_order != order:
                      break
                  
                  # Remove buddy from free list
                  if buddy_offset in self._free_lists[order]:
                      self._free_lists[order].remove(buddy_offset)
                  
                  # Remove buddy's metadata
                  del self._blocks[buddy_offset]
                  
                  # Merge into larger block (use lower address)
                  offset = min(offset, buddy_offset)
                  order += 1
              
              self._blocks[offset] = (order, True)
              self._free_lists[order].append(offset)
              
              return True
      
      def get_memory_view(self, offset: int) -> Optional[memoryview]:
          """Get memoryview for allocated block."""
          if offset not in self._blocks:
              return None
          order, is_free = self._blocks[offset]
          if is_free:
              return None
          size = self._size_for_order(order)
          return memoryview(self._memory)[offset:offset + size]
      
      def get_fragmentation(self) -> Tuple[float, float]:
          """Calculate external and internal fragmentation."""
          with self._lock:
              free_memory = 0
              largest_free = 0
              
              for order in range(self.max_order + 1):
                  block_size = self._size_for_order(order)
                  free_count = len(self._free_lists[order])
                  free_memory += free_count * block_size
                  if free_count > 0:
                      largest_free = max(largest_free, block_size)
              
              # External fragmentation: 1 - (largest_free / total_free)
              external = 1.0 - (largest_free / free_memory) if free_memory > 0 else 0.0
              
              # Internal fragmentation would need tracking requested vs actual sizes
              internal = 0.0  # Not tracked without additional metadata
              
              return (external, internal)
      
      def get_stats(self) -> AllocationStats:
          """Get allocation statistics."""
          with self._lock:
              stats = AllocationStats(
                  total_allocated=self._stats.total_allocated,
                  total_freed=self._stats.total_freed,
                  current_in_use=self._stats.current_in_use,
                  allocation_count=self._stats.allocation_count,
                  free_count=self._stats.free_count,
                  failed_allocations=self._stats.failed_allocations
              )
              ext, int_ = self.get_fragmentation()
              stats.external_fragmentation = ext
              stats.internal_fragmentation = int_
              
              # Find largest free block
              for order in range(self.max_order, -1, -1):
                  if self._free_lists[order]:
                      stats.largest_free_block = self._size_for_order(order)
                      break
              
              return stats
  
  # === MEMORY POOL WITH DEFRAGMENTATION ===
  
  class MemoryPoolWithDefrag:
      """
      Memory pool that supports defragmentation.
      Moves allocations to compact free space.
      """
      
      def __init__(self, pool_size: int):
          self.pool_size = pool_size
          self._memory = bytearray(pool_size)
          
          # Allocation table: id -> (offset, size, callback for relocation)
          self._allocations: Dict[int, Tuple[int, int, Optional[callable]]] = {}
          self._next_id = 0
          
          # Free list: list of (offset, size) sorted by offset
          self._free_blocks: List[Tuple[int, int]] = [(0, pool_size)]
          self._lock = threading.Lock()
      
      def allocate(self, size: int, relocation_callback: callable = None) -> Optional[int]:
          """
          Allocate memory and return allocation ID.
          relocation_callback(old_view, new_view) called during defrag.
          """
          with self._lock:
              # First-fit allocation
              for i, (offset, block_size) in enumerate(self._free_blocks):
                  if block_size >= size:
                      alloc_id = self._next_id
                      self._next_id += 1
                      
                      self._allocations[alloc_id] = (offset, size, relocation_callback)
                      
                      # Update free list
                      if block_size > size:
                          self._free_blocks[i] = (offset + size, block_size - size)
                      else:
                          del self._free_blocks[i]
                      
                      # Coalesce adjacent free blocks
                      self._coalesce_free_blocks()
                      
                      return alloc_id
              
              return None  # Allocation failed
      
      def free(self, alloc_id: int) -> bool:
          """Free allocation by ID."""
          with self._lock:
              if alloc_id not in self._allocations:
                  return False
              
              offset, size, _ = self._allocations[alloc_id]
              del self._allocations[alloc_id]
              
              # Add to free list in sorted order
              inserted = False
              for i, (free_offset, free_size) in enumerate(self._free_blocks):
                  if offset < free_offset:
                      self._free_blocks.insert(i, (offset, size))
                      inserted = True
                      break
              if not inserted:
                  self._free_blocks.append((offset, size))
              
              self._coalesce_free_blocks()
              return True
      
      def _coalesce_free_blocks(self) -> None:
          """Merge adjacent free blocks."""
          if len(self._free_blocks) < 2:
              return
          
          i = 0
          while i < len(self._free_blocks) - 1:
              offset1, size1 = self._free_blocks[i]
              offset2, size2 = self._free_blocks[i + 1]
              
              if offset1 + size1 == offset2:
                  # Merge
                  self._free_blocks[i] = (offset1, size1 + size2)
                  del self._free_blocks[i + 1]
              else:
                  i += 1
      
      def get_view(self, alloc_id: int) -> Optional[memoryview]:
          """Get memoryview for allocation."""
          with self._lock:
              if alloc_id not in self._allocations:
                  return None
              offset, size, _ = self._allocations[alloc_id]
              return memoryview(self._memory)[offset:offset + size]
      
      def defragment(self) -> int:
          """
          Compact memory by moving allocations.
          Returns number of allocations moved.
          """
          with self._lock:
              if not self._allocations:
                  return 0
              
              # Sort allocations by offset
              sorted_allocs = sorted(
                  self._allocations.items(),
                  key=lambda x: x[1][0]
              )
              
              current_offset = 0
              moves = 0
              
              for alloc_id, (old_offset, size, callback) in sorted_allocs:
                  if old_offset != current_offset:
                      # Need to move this allocation
                      old_view = memoryview(self._memory)[old_offset:old_offset + size]
                      new_view = memoryview(self._memory)[current_offset:current_offset + size]
                      
                      # Copy data
                      new_view[:] = old_view[:]
                      
                      # Update allocation record
                      self._allocations[alloc_id] = (current_offset, size, callback)
                      
                      # Notify via callback if provided
                      if callback:
                          try:
                              callback(old_view, new_view)
                          except Exception as e:
                              logger.error(f"Relocation callback error: {e}")
                      
                      moves += 1
                  
                  current_offset += size
              
              # Update free list - single contiguous block at end
              free_size = self.pool_size - current_offset
              self._free_blocks = [(current_offset, free_size)] if free_size > 0 else []
              
              return moves
      
      def get_fragmentation_ratio(self) -> float:
          """Calculate fragmentation as 1 - (largest_free / total_free)."""
          with self._lock:
              if not self._free_blocks:
                  return 0.0
              
              total_free = sum(size for _, size in self._free_blocks)
              largest_free = max(size for _, size in self._free_blocks)
              
              if total_free == 0:
                  return 0.0
              
              return 1.0 - (largest_free / total_free)
  
  # === SIZE CLASS ALLOCATOR ===
  
  class SizeClassAllocator:
      """
      Allocator with multiple size classes to reduce internal fragmentation.
      Uses slab allocators for common sizes.
      """
      
      # Size classes: powers of 2 plus intermediate sizes
      SIZE_CLASSES = [
          8, 16, 24, 32, 48, 64, 80, 96, 128,
          192, 256, 384, 512, 768, 1024,
          1536, 2048, 3072, 4096
      ]
      
      def __init__(self):
          self._slabs: Dict[int, SlabAllocator] = {}
          for size in self.SIZE_CLASSES:
              self._slabs[size] = SlabAllocator(size, objects_per_slab=64)
          
          # For larger sizes, use buddy allocator
          self._large_allocator = BuddyAllocator(1024 * 1024, min_block_size=4096)
          self._lock = threading.Lock()
      
      def _find_size_class(self, size: int) -> Optional[int]:
          """Find smallest size class that fits the request."""
          for sc in self.SIZE_CLASSES:
              if sc >= size:
                  return sc
          return None
      
      def allocate(self, size: int) -> Optional[Tuple[Any, memoryview]]:
          """
          Allocate memory of given size.
          Returns (handle, memoryview) or None if failed.
          """
          size_class = self._find_size_class(size)
          
          if size_class is not None:
              result = self._slabs[size_class].allocate()
              if result:
                  slab_idx, obj_idx, view = result
                  return (('slab', size_class, slab_idx, obj_idx), view[:size])
          else:
              offset = self._large_allocator.allocate(size)
              if offset is not None:
                  view = self._large_allocator.get_memory_view(offset)
                  if view:
                      return (('large', offset), view[:size])
          
          return None
      
      def free(self, handle: Tuple) -> bool:
          """Free memory by handle."""
          if handle[0] == 'slab':
              _, size_class, slab_idx, obj_idx = handle
              return self._slabs[size_class].free(slab_idx, obj_idx)
          elif handle[0] == 'large':
              _, offset = handle
              return self._large_allocator.free(offset)
          return False
      
      def get_internal_fragmentation(self, requested_size: int) -> float:
          """Calculate internal fragmentation for a given request size."""
          size_class = self._find_size_class(requested_size)
          if size_class is None:
              return 0.0
          return 1.0 - (requested_size / size_class)
  
  # === TESTS ===
  
  def test_slab_allocator():
      """Test slab allocator eliminates fragmentation for fixed size."""
      slab = SlabAllocator(32, objects_per_slab=16)
      
      # Allocate all slots
      handles = []
      for _ in range(16):
          result = slab.allocate()
          assert result is not None
          handles.append(result[:2])
      
      # Free every other slot
      for i in range(0, 16, 2):
          slab.free(*handles[i])
      
      # Should still be able to allocate
      for _ in range(8):
          result = slab.allocate()
          assert result is not None
      
      stats = slab.get_stats()
      assert stats.external_fragmentation == 0.0
      print("✓ Slab allocator works")
  
  def test_buddy_allocator():
      """Test buddy allocator coalesces free blocks."""
      buddy = BuddyAllocator(1024, min_block_size=64)
      
      # Allocate several blocks
      offsets = []
      for size in [64, 128, 64, 256]:
          offset = buddy.allocate(size)
          assert offset is not None
          offsets.append(offset)
      
      # Free middle blocks
      buddy.free(offsets[1])
      buddy.free(offsets[2])
      
      # Should coalesce and allow 256-byte allocation
      offset = buddy.allocate(192)
      assert offset is not None
      
      print("✓ Buddy allocator coalescing works")
  
  def test_defragmentation():
      """Test memory pool defragmentation."""
      pool = MemoryPoolWithDefrag(1024)
      
      # Allocate several blocks
      ids = []
      for size in [100, 200, 100, 200, 100]:
          alloc_id = pool.allocate(size)
          assert alloc_id is not None
          ids.append(alloc_id)
      
      # Free alternating blocks
      pool.free(ids[1])
      pool.free(ids[3])
      
      # Check fragmentation exists
      frag_before = pool.get_fragmentation_ratio()
      assert frag_before > 0
      
      # Defragment
      moves = pool.defragment()
      assert moves > 0
      
      # Fragmentation should be zero now
      frag_after = pool.get_fragmentation_ratio()
      assert frag_after == 0.0
      
      print("✓ Defragmentation works")
  
  if __name__ == "__main__":
      test_slab_allocator()
      test_buddy_allocator()
      test_defragmentation()
      print("\nAll tests passed!")

# LLM trap configurations
traps:
  - type: "external_fragmentation"
    description: "Free memory exists but not contiguous for large allocation"
    trigger: "Allocating varied sizes without compaction strategy"
  
  - type: "internal_fragmentation"
    description: "Fixed-size blocks waste space for smaller objects"
    trigger: "Using power-of-2 size classes without fitting"
  
  - type: "allocator_metadata"
    description: "Allocator overhead grows with allocation count"
    trigger: "Not considering per-allocation metadata cost"
  
  - type: "memory_not_returned"
    description: "Free'd memory held by allocator, not returned to OS"
    trigger: "Using allocator that doesn't release pages"
  
  - type: "numa_imbalance"
    description: "Memory allocated on wrong NUMA node"
    trigger: "Not considering NUMA topology"

# Task generation template
instruction_template: |
  You are debugging a {{ scenario_type }} with memory fragmentation issues.
  The code is at {{ path }}.
  
  System memory: {{ total_memory_mb }} MB
  Process RSS: {{ rss_mb }} MB
  Allocation failures: {{ allocation_failures }} per hour
  
  Your task:
  {{ task_steps }}

# Test cases
fail_to_pass:
  - "test_external_fragmentation_reduction"
  - "test_buddy_coalescing"
  - "test_slab_internal_fragmentation"
  - "test_defragmentation"
  - "test_memory_return_to_os"

pass_to_pass:
  - "test_basic_allocation"
  - "test_free_and_reuse"

# Variables for task generation
variables:
  - name: scenario_type
    type: string
    options: 
      - "application server"
      - "database engine"
      - "game server"
      - "embedded system"
      - "cache server"
      - "message broker"
  - name: path
    type: path
    generator: random_path
  - name: total_memory_mb
    type: int
    min: 1024
    max: 65536
  - name: rss_mb
    type: int
    min: 512
    max: 32768
  - name: allocation_failures
    type: int
    min: 1
    max: 10000
  - name: task_steps
    type: template
    value: |
      1. Diagnose fragmentation type (internal vs external)
      2. Analyze allocation size distribution
      3. Implement appropriate allocator (slab, buddy, or pool)
      4. Add memory coalescing/compaction
      5. Configure allocator to return memory to OS
      6. Add fragmentation metrics and monitoring
      7. Validate with stress testing

# Anti-hardcoding measures
anti_hardcoding:
  canary_tokens: true
  randomize_paths: true
  dynamic_content: true
  fragmentation_causes:
    - varied_allocation_sizes
    - no_coalescing
    - memory_not_returned
    - allocator_overhead
    - poor_size_class_fit

# Anti-patterns that LLMs commonly fail on
anti_patterns:
  llm_failure_modes:
    - "Applying userspace patterns to kernel-level problems"
    - "Missing memory ordering and barrier requirements"
    - "Ignoring NUMA topology effects on performance"
    - "Not considering scheduler behavior under load"
    - "Missing ABA problems in lock-free data structures"
    - "Overlooking signal handler safety restrictions"
    - "Assuming atomic operations are always sufficient"
    - "Missing file descriptor inheritance across fork/exec"
    - "Ignoring distributed consensus edge cases (Byzantine failures)"
    - "Not understanding power-of-two size class waste"
    - "Missing immediate coalescing for adjacent free blocks"
    - "Ignoring mmap threshold effects on memory return"
    - "Not configuring allocator decay time for timely return"
    - "Missing THP internal fragmentation with sparse access"
    - "Ignoring NUMA-aware allocation for performance"
    - "Not handling unmovable pages blocking compaction"
    - "Assuming malloc_trim always returns memory"
    - "Missing slab cache internal fragmentation"
    - "Ignoring per-CPU cache retention causing RSS growth"
    - "Not understanding arena imbalance across threads"
    - "Missing buddy allocator order-N allocation failures"
    - "Ignoring zone watermark effects on allocation"
