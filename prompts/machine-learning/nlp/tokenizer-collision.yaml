id: "ml-nlp-tokenizer-collision-001"
version: "2.0.0"
category: "machine-learning"
subcategory: "nlp"
complexity: "expert"

# === LLM GENERATION FRAMEWORK ===
generation_framework:
  multi_conversation_workflow:
    phase_1_research: "Research obscure tokenization vulnerabilities and NLP attacks"
    phase_2_creation: "Create task with subtle Unicode and encoding bugs"
    phase_3_amplification: "Add security and correctness requirements"
    phase_4_verification:
      description: "Validate task requires deep understanding of tokenization"
      validation_criteria:
        - "Has at least 5 interacting hidden traps"
        - "Has cascading numerical instability issues"
        - "Requires knowledge of framework internals (PyTorch/TensorFlow compilation)"
        - "Would take experienced ML engineers 45+ minutes"
        - "Combines mathematical theory with systems-level debugging"
  
  task_generation_strategy:
    min_unique_tasks: 10000
    complexity_levels: ["medium", "hard", "expert", "research"]
    require_security_analysis: true
    require_unicode_knowledge: true
    require_determinism_proofs: true
    time_range: [1800, 7200]
    command_steps: [30, 90]
  
  quality_requirements:
    minimum_time: "120-240 minutes for ML researchers with PhD-level expertise"
    depth: "Requires deep understanding of tokenization algorithms, Unicode encoding, and NLP security"
    traps: "10+ deeply interacting traps across encoding, normalization, security, and cross-framework domains"
  
  generation_targets:
    minimum_difficulty: "120-240 minutes, requires PhD-level NLP expertise, Unicode standards knowledge, and security research experience"
    target_audience: "Senior NLP security researchers with publication track record in adversarial NLP"
    failure_mode_count: 10
  
  multi_agent_orchestration:
    required_agents:
      - agent: "unicode_specialist"
        role: "Analyzes Unicode normalization, encoding attacks, and character-level vulnerabilities"
        expertise: ["unicode_normalization_forms", "homoglyph_detection", "bidirectional_text"]
      - agent: "tokenizer_architect"
        role: "Designs and validates tokenization algorithms for security and consistency"
        expertise: ["bpe_algorithms", "vocabulary_optimization", "determinism_guarantees"]
      - agent: "framework_debugger"
        role: "Debugs cross-framework tokenizer compatibility and serialization issues"
        expertise: ["huggingface_tokenizers", "sentencepiece_internals", "tiktoken_encoding"]
      - agent: "security_analyst"
        role: "Identifies and mitigates tokenizer-based attack vectors"
        expertise: ["prompt_injection", "token_collision", "adversarial_inputs"]
      - agent: "multilingual_expert"
        role: "Validates tokenization consistency across languages and scripts"
        expertise: ["cjk_tokenization", "rtl_languages", "morphologically_rich_languages"]
      - agent: "performance_optimizer"
        role: "Optimizes tokenizer performance and memory efficiency"
        expertise: ["rust_tokenizers", "parallel_tokenization", "cache_optimization"]
      - agent: "privacy_auditor"
        role: "Analyzes information leakage through tokenization patterns"
        expertise: ["membership_inference", "vocabulary_probing", "training_data_extraction"]
    cross_framework_attack_chains:
      - chain: "huggingface_to_sentencepiece_collision"
        description: "Exploit tokenizer differences between HuggingFace and SentencePiece for prompt injection"
        steps:
          - "Identify normalization differences between tokenizer implementations"
          - "Craft input that tokenizes differently across frameworks"
          - "Exploit token boundary differences for injection"
          - "Bypass safety filters through tokenization discrepancies"
      - chain: "tiktoken_to_vocabulary_extraction"
        description: "Use tiktoken encoding patterns to extract training vocabulary information"
        steps:
          - "Analyze merge rule ordering in tiktoken BPE"
          - "Probe vocabulary boundaries through systematic queries"
          - "Extract information about training corpus composition"
          - "Infer sensitive tokens from vocabulary structure"
    parallel_analysis_requirements:
      - "Coordinate Unicode normalization validation across multiple tokenizer versions"
      - "Share homoglyph detection patterns between security-focused agents"
      - "Maintain consistent vocabulary state across framework migrations"
      - "Synchronize adversarial input testing across multilingual contexts"
    agent_handoff_protocols:
      - protocol: "attack_discovery_handoff"
        trigger: "Token collision vulnerability discovered"
        from_agent: "security_analyst"
        to_agent: "unicode_specialist"
        context_transfer: ["collision_patterns", "normalization_bypass", "affected_encodings"]
      - protocol: "multilingual_validation_handoff"
        trigger: "Language-specific tokenization issue found"
        from_agent: "multilingual_expert"
        to_agent: "tokenizer_architect"
        context_transfer: ["script_boundaries", "fertility_analysis", "vocabulary_coverage"]
  
  difficulty_amplifiers:
    nightmare:
      multiplier: 3.5
      description: "Research-level difficulty requiring deep NLP theory and security expertise"
      requirements:
        - "10+ interacting traps across Unicode, encoding, security, and framework domains"
        - "Requires understanding of tokenizer internals across HuggingFace, SentencePiece, and tiktoken"
        - "Time estimate: 120+ minutes for senior NLP security researchers"
        - "Cross-framework compatibility issues causing security vulnerabilities"
        - "Requires synthesizing Unicode standards, information theory, and adversarial ML"
    nightmare_plus:
      multiplier: 10.0
      estimated_time: [43200, 172800]  # 12-48 hours
      command_steps: [500, 2000]
      techniques_required: 15
      description: "Publication-grade difficulty requiring novel tokenization security research and cross-framework analysis"
      requirements:
        - "15+ deeply coupled traps requiring simultaneous resolution across encoding, security, and framework domains"
        - "Novel tokenization attack discovery and defense mechanism design"
        - "Cross-framework attack chains spanning HuggingFace, SentencePiece, tiktoken, and custom implementations"
        - "Formal security proofs for tokenization determinism and collision resistance"
        - "Information-theoretic analysis of vocabulary information leakage"
        - "Production-grade implementation handling billions of tokens per day"
        - "Adversarial robustness against sophisticated prompt injection attacks"
        - "Multilingual tokenization fairness across 100+ languages"
  
  theoretical_foundations:
    information_theoretic_bounds:
      - theorem: "vocabulary_entropy_bound"
        statement: "H(V) ≤ log|V| with equality for uniform distribution; actual H(V) reveals corpus statistics"
        proof_requirements: ["maximum_entropy_principle", "kullback_leibler_divergence", "fisher_information"]
        application: "Bound information leaked through vocabulary probing"
      - theorem: "tokenization_compression_optimality"
        statement: "BPE achieves H(X) + O(log n / n) bits per character for n-length inputs"
        proof_requirements: ["source_coding_theorem", "lempel_ziv_bounds", "universal_compression"]
        application: "Derive optimal vocabulary size for compression-privacy tradeoff"
    differential_geometry:
      - concept: "embedding_manifold_structure"
        description: "Analyze token embedding space geometry for collision detection"
        requirements: ["principal_curvatures", "intrinsic_dimension", "geodesic_distances"]
      - concept: "vocabulary_space_topology"
        description: "Use topological methods to detect vocabulary structure anomalies"
        requirements: ["persistent_homology", "betti_numbers", "simplicial_complexes"]
    category_theory:
      - concept: "tokenization_functor"
        description: "Model tokenization as functor from text category to token category"
        requirements: ["forgetful_functors", "free_constructions", "adjoint_pairs"]
      - concept: "normalization_natural_transformation"
        description: "Formalize Unicode normalization as natural transformation"
        requirements: ["functor_composition", "naturality_squares", "coherence_conditions"]
    measure_theory:
      - concept: "token_distribution_analysis"
        description: "Analyze token frequency distributions using measure-theoretic methods"
        requirements: ["zipf_law_derivation", "heavy_tail_analysis", "concentration_inequalities"]
      - concept: "collision_probability_bounds"
        description: "Derive tight bounds on token collision probability"
        requirements: ["birthday_paradox", "balls_into_bins", "occupancy_problems"]
  
  cross_framework_requirements:
    huggingface_expertise:
      - "Fast vs slow tokenizer implementation differences"
      - "PreTrainedTokenizer customization and extension"
      - "Tokenizer serialization and vocabulary management"
      - "Special token handling and template processing"
      - "Batch encoding edge cases and padding strategies"
    sentencepiece_expertise:
      - "Unigram language model training and inference"
      - "BPE model customization and vocabulary control"
      - "Byte-fallback mechanism implementation"
      - "Normalization rule configuration"
      - "Cross-platform serialization compatibility"
    tiktoken_expertise:
      - "Regex-based pretokenization patterns"
      - "Encoding registration and management"
      - "Special token allowlisting and handling"
      - "Rust binding performance optimization"
      - "Custom encoding creation and validation"
    custom_implementation_expertise:
      - "Deterministic BPE merge ordering algorithms"
      - "Unicode normalization library selection and configuration"
      - "Memory-efficient vocabulary storage"
      - "Parallel tokenization for throughput"
      - "Streaming tokenization for long documents"
  
  anti_patterns:
    llm_failure_modes:
      - "Applying textbook solutions without considering numerical stability"
      - "Missing gradient flow issues in deep architectures"
      - "Ignoring framework-specific automatic differentiation behavior"
      - "Not considering mixed precision training edge cases"
      - "Missing hardware-specific numerical behavior (GPU vs CPU)"
      - "Overlooking data pipeline bottlenecks masking as model issues"
      - "Assuming standard initialization works for all architectures"
      - "Missing tokenizer edge cases with unicode and special characters"
      - "Ignoring batch normalization mode (train vs eval) effects"
      - "Assuming Unicode normalization is always applied consistently"
      - "Missing zero-width character injection vulnerabilities"
      - "Ignoring bidirectional text override attacks"
      - "Assuming BPE merge order is deterministic without verification"
      - "Missing homoglyph confusion between Cyrillic and Latin"
      - "Ignoring tokenizer train/inference normalization mismatches"
      - "Assuming special tokens are always protected from injection"
      - "Missing vocabulary collision between similar subwords"
      - "Ignoring encoding detection failures for non-UTF8 input"
      - "Assuming whitespace handling is consistent across frameworks"
      - "Missing surrogate pair handling for supplementary plane characters"
      - "Ignoring timing side channels in tokenization"
      - "Assuming vocabulary is collision-free without verification"
      - "Missing prompt injection through tokenizer preprocessing"
      - "Ignoring multilingual fertility imbalance in shared vocabularies"
  
  publication_requirements:
    venue_tier: "top_nlp_security"
    target_venues: ["ACL", "EMNLP", "NAACL", "USENIX Security", "S&P", "CCS"]
    novel_contributions:
      - contribution: "new_attack_vector"
        description: "Discovery of previously unknown tokenizer-based attack exploiting cross-framework inconsistencies"
        requirements: ["empirical_validation", "responsible_disclosure", "defense_recommendation"]
      - contribution: "secure_tokenizer_design"
        description: "Novel tokenizer architecture with formal security guarantees"
        requirements: ["formal_verification", "determinism_proof", "collision_resistance"]
      - contribution: "multilingual_fairness"
        description: "Tokenization approach achieving balanced fertility across languages"
        requirements: ["fairness_metrics", "cross_lingual_evaluation", "computational_efficiency"]
    evaluation_requirements:
      - "Comparison against 5+ state-of-the-art tokenizers"
      - "Evaluation on 10+ languages across different scripts"
      - "Security testing against known attack vectors"
      - "Performance benchmarking on large-scale corpora"
      - "Ablation studies for all security components"

# === COMPREHENSIVE TOPIC UNIVERSE ===
topic_universe:
  tokenization_algorithms:
    - "byte_pair_encoding_bpe"
    - "wordpiece_tokenization"
    - "unigram_language_model"
    - "sentencepiece_unified"
    - "character_level_tokenization"
    - "word_level_tokenization"
    - "subword_regularization"
    - "bpe_dropout"
    - "unigram_sampling"
    - "byte_level_bpe"
    - "tiktoken_openai"
    - "huggingface_tokenizers"
    - "spacy_tokenization"
    - "nltk_tokenization"
    - "moses_tokenization"
    - "mecab_tokenization"
    - "jieba_tokenization"
    - "sudachi_tokenization"
    - "kytea_tokenization"
    - "polyglot_tokenization"
    - "stanza_tokenization"
    - "morfessor_tokenization"
    - "fastbpe_tokenization"
    - "youtokentome_tokenization"
    
  vocabulary_management:
    - "vocabulary_construction"
    - "vocabulary_pruning"
    - "vocabulary_merging"
    - "vocabulary_expansion"
    - "subword_frequency_analysis"
    - "rare_word_handling"
    - "unknown_token_strategy"
    - "special_token_design"
    - "reserved_token_allocation"
    - "vocabulary_size_optimization"
    - "cross_lingual_vocabulary"
    - "shared_vocabulary_construction"
    - "language_specific_tokens"
    - "domain_specific_vocabulary"
    - "vocabulary_adaptation"
    - "dynamic_vocabulary"
    - "vocabulary_factorization"
    - "hierarchical_vocabulary"
    - "compositional_vocabulary"
    - "morphological_vocabulary"
    
  unicode_handling:
    - "unicode_normalization_forms"
    - "nfc_normalization"
    - "nfd_normalization"
    - "nfkc_normalization"
    - "nfkd_normalization"
    - "unicode_equivalence"
    - "canonical_equivalence"
    - "compatibility_equivalence"
    - "unicode_categories"
    - "unicode_blocks"
    - "unicode_scripts"
    - "combining_characters"
    - "precomposed_characters"
    - "decomposed_characters"
    - "compatibility_characters"
    - "variation_selectors"
    - "emoji_handling"
    - "emoji_zwj_sequences"
    - "skin_tone_modifiers"
    - "regional_indicators"
    - "unicode_bidirectional"
    - "right_to_left_handling"
    - "bidirectional_override"
    - "unicode_control_characters"
    - "zero_width_characters"
    - "byte_order_marks"
    - "replacement_characters"
    - "private_use_areas"
    - "surrogate_pairs"
    - "utf8_encoding"
    - "utf16_encoding"
    - "utf32_encoding"
    
  security_vulnerabilities:
    - "token_collision_attacks"
    - "adversarial_token_injection"
    - "prompt_injection_tokenization"
    - "unicode_homoglyph_attacks"
    - "invisible_character_injection"
    - "control_character_exploitation"
    - "bidirectional_text_attacks"
    - "normalization_bypass"
    - "encoding_confusion_attacks"
    - "vocabulary_probing"
    - "token_boundary_manipulation"
    - "special_token_injection"
    - "reserved_token_collision"
    - "training_data_extraction"
    - "membership_inference_tokenizer"
    - "model_inversion_via_oov"
    - "differential_tokenization_leak"
    - "timing_side_channel"
    - "cache_side_channel"
    - "error_message_leak"
    
  nlp_specific_issues:
    - "out_of_vocabulary_handling"
    - "rare_word_embedding"
    - "word_boundary_detection"
    - "sentence_boundary_detection"
    - "paragraph_segmentation"
    - "document_chunking"
    - "context_window_truncation"
    - "long_sequence_handling"
    - "sliding_window_tokenization"
    - "hierarchical_tokenization"
    - "multi_granularity_tokenization"
    - "code_tokenization"
    - "markup_tokenization"
    - "latex_tokenization"
    - "math_tokenization"
    - "chemical_formula_tokenization"
    - "musical_notation_tokenization"
    - "dna_sequence_tokenization"
    - "protein_sequence_tokenization"
    - "time_series_tokenization"
    
  multilingual_challenges:
    - "cross_lingual_tokenization"
    - "language_detection"
    - "script_detection"
    - "code_switching_handling"
    - "transliteration"
    - "romanization"
    - "diacritics_handling"
    - "tone_marks_handling"
    - "ideographic_tokenization"
    - "syllabic_tokenization"
    - "agglutinative_language_handling"
    - "morphologically_rich_languages"
    - "low_resource_languages"
    - "language_specific_preprocessing"
    - "culturally_sensitive_tokenization"
    - "numeral_system_handling"
    - "date_format_normalization"
    - "name_transliteration"
    - "address_normalization"
    - "currency_handling"
    
  preprocessing_pipeline:
    - "text_cleaning"
    - "html_stripping"
    - "url_handling"
    - "email_handling"
    - "mention_handling"
    - "hashtag_handling"
    - "number_normalization"
    - "date_normalization"
    - "time_normalization"
    - "currency_normalization"
    - "unit_normalization"
    - "contraction_expansion"
    - "abbreviation_expansion"
    - "acronym_handling"
    - "case_normalization"
    - "accent_removal"
    - "punctuation_handling"
    - "whitespace_normalization"
    - "newline_handling"
    - "tab_handling"
    - "spell_correction"
    - "grammar_correction"
    - "profanity_filtering"
    - "pii_redaction"
    - "text_anonymization"
    
  special_sequences:
    - "padding_token_handling"
    - "cls_token_placement"
    - "sep_token_placement"
    - "mask_token_handling"
    - "bos_token_placement"
    - "eos_token_placement"
    - "pad_token_placement"
    - "unk_token_replacement"
    - "custom_special_tokens"
    - "task_specific_tokens"
    - "speaker_tokens"
    - "language_tokens"
    - "sentiment_tokens"
    - "entity_tokens"
    - "relation_tokens"
    - "structural_tokens"
    - "positional_tokens"
    - "segment_tokens"
    - "type_tokens"
    - "instruction_tokens"
    
  evaluation_metrics:
    - "tokenization_consistency"
    - "vocabulary_coverage"
    - "fertility_rate"
    - "continuation_rate"
    - "sequence_length_distribution"
    - "token_frequency_distribution"
    - "unknown_rate"
    - "out_of_vocabulary_rate"
    - "subword_fragmentation"
    - "morphological_alignment"
    - "cross_lingual_alignment"
    - "tokenization_speed"
    - "memory_efficiency"
    - "compression_ratio"
    - "reconstruction_accuracy"
    - "semantic_preservation"
    - "syntactic_preservation"
    - "entity_preservation"
    - "boundary_accuracy"

# === BUG PATTERNS AND ANTI-PATTERNS ===
bug_patterns:
  unicode_issues:
    - pattern: "inconsistent_normalization"
      description: "Different normalization forms used in train vs inference"
      symptom: "Same text produces different tokens"
      fix: "Apply consistent normalization (NFKC) at input"
      severity: "critical"
      
    - pattern: "combining_character_split"
      description: "Combining characters separated from base characters"
      symptom: "Accented characters become multiple tokens"
      fix: "Normalize to NFC before tokenization"
      severity: "high"
      
    - pattern: "homoglyph_confusion"
      description: "Visually identical characters treated differently"
      symptom: "Cyrillic 'а' vs Latin 'a' are different tokens"
      fix: "Implement homoglyph normalization for security"
      severity: "high"
      
    - pattern: "zero_width_injection"
      description: "Zero-width characters manipulate tokenization"
      symptom: "Invisible characters change token boundaries"
      fix: "Strip zero-width characters before tokenization"
      severity: "critical"
      
    - pattern: "bidirectional_override_attack"
      description: "Bidi override characters hide malicious content"
      symptom: "Text appears different than it tokenizes"
      fix: "Detect and neutralize bidi overrides"
      severity: "critical"
      
    - pattern: "surrogate_pair_handling"
      description: "Supplementary plane characters mishandled"
      symptom: "Emoji or rare scripts broken into invalid surrogates"
      fix: "Use UTF-8 internally, validate surrogate pairs"
      severity: "high"
      
    - pattern: "byte_order_mark_in_middle"
      description: "BOM characters appear mid-text"
      symptom: "Unexpected token boundaries at BOM positions"
      fix: "Strip BOM characters from input"
      severity: "medium"
      
    - pattern: "private_use_area_handling"
      description: "Private use area characters undefined behavior"
      symptom: "Custom characters become unknown tokens"
      fix: "Define policy for PUA handling"
      severity: "medium"
      
  bpe_algorithm_issues:
    - pattern: "merge_order_nondeterminism"
      description: "BPE merge order varies between runs"
      symptom: "Same text tokenizes differently"
      fix: "Sort by merge rank, then alphabetically for ties"
      severity: "critical"
      
    - pattern: "greedy_vs_optimal_mismatch"
      description: "Greedy BPE differs from optimal segmentation"
      symptom: "Suboptimal token boundaries"
      fix: "Use dynamic programming for optimal BPE"
      severity: "medium"
      
    - pattern: "vocabulary_collision"
      description: "Different strings map to same token"
      symptom: "Loss of information during tokenization"
      fix: "Check vocabulary for collisions"
      severity: "high"
      
    - pattern: "special_token_merge"
      description: "Special tokens participate in BPE merges"
      symptom: "Special tokens fragmented or merged"
      fix: "Protect special tokens from merging"
      severity: "critical"
      
    - pattern: "whitespace_handling_inconsistency"
      description: "Leading/trailing whitespace treated inconsistently"
      symptom: "' hello' vs 'hello' tokenize differently"
      fix: "Explicit whitespace token policy"
      severity: "high"
      
    - pattern: "case_sensitivity_mismatch"
      description: "Training vs inference case handling differs"
      symptom: "'Hello' vs 'hello' unexpected tokenization"
      fix: "Consistent case policy, document behavior"
      severity: "high"
      
  vocabulary_issues:
    - pattern: "unknown_token_explosion"
      description: "Too many tokens map to UNK"
      symptom: "High UNK rate, poor model performance"
      fix: "Increase vocabulary or use byte-level fallback"
      severity: "high"
      
    - pattern: "vocabulary_size_mismatch"
      description: "Model expects different vocabulary size"
      symptom: "Index out of bounds errors"
      fix: "Validate vocabulary size matches model config"
      severity: "critical"
      
    - pattern: "token_id_collision"
      description: "Multiple tokens share same ID"
      symptom: "Decode produces wrong tokens"
      fix: "Ensure vocabulary is bijective"
      severity: "critical"
      
    - pattern: "reserved_id_used"
      description: "Reserved IDs used for regular tokens"
      symptom: "Conflict with special tokens"
      fix: "Reserve ID ranges for special purposes"
      severity: "high"
      
    - pattern: "vocabulary_not_loaded"
      description: "Tokenizer used before vocabulary load"
      symptom: "All tokens become UNK"
      fix: "Lazy load or validate vocabulary on init"
      severity: "critical"
      
  encoding_issues:
    - pattern: "encoding_detection_failure"
      description: "Wrong encoding assumed for input"
      symptom: "Garbled text, mojibake"
      fix: "Explicit encoding specification or detection"
      severity: "high"
      
    - pattern: "mixed_encoding_input"
      description: "Input contains multiple encodings"
      symptom: "Partial garbled text"
      fix: "Transcode to UTF-8 with error handling"
      severity: "high"
      
    - pattern: "invalid_utf8_sequence"
      description: "Input contains invalid UTF-8 bytes"
      symptom: "Decoding errors or replacement characters"
      fix: "Validate and sanitize UTF-8 input"
      severity: "high"
      
    - pattern: "null_byte_in_input"
      description: "Null bytes terminate string early"
      symptom: "Truncated tokenization"
      fix: "Remove or escape null bytes"
      severity: "critical"
      
  pipeline_issues:
    - pattern: "tokenizer_train_test_mismatch"
      description: "Different tokenizer config train vs test"
      symptom: "Test performance much worse than train"
      fix: "Serialize and load exact tokenizer config"
      severity: "critical"
      
    - pattern: "preprocessing_not_applied"
      description: "Preprocessing steps skipped at inference"
      symptom: "Token distribution shift"
      fix: "Encapsulate all preprocessing in tokenizer"
      severity: "high"
      
    - pattern: "truncation_boundary_incorrect"
      description: "Truncation cuts mid-token or mid-word"
      symptom: "Incomplete tokens at sequence boundary"
      fix: "Truncate at token boundaries"
      severity: "medium"
      
    - pattern: "padding_position_wrong"
      description: "Left vs right padding inconsistent"
      symptom: "Position embeddings misaligned"
      fix: "Document and enforce padding direction"
      severity: "high"
      
    - pattern: "attention_mask_mismatch"
      description: "Attention mask doesn't match padding"
      symptom: "Model attends to padding tokens"
      fix: "Generate attention mask with tokenization"
      severity: "critical"

# === MATHEMATICAL RIGOR REQUIREMENTS ===
mathematical_requirements:
  compression_theory:
    - requirement: "analyze_bpe_compression_ratio"
      description: "Compute compression ratio of BPE encoding"
      formula: "CR = original_length / encoded_length"
      topics: ["compression", "entropy", "information_theory"]
      
    - requirement: "optimal_vocabulary_size"
      description: "Derive optimal vocabulary size given corpus"
      formula: "|V|* = argmin |V| H(P) + λ|V| where H is entropy"
      topics: ["vocabulary_optimization", "mdl", "rate_distortion"]
      
    - requirement: "subword_entropy_analysis"
      description: "Compute entropy of subword distribution"
      formula: "H(S) = -Σ p(s) log p(s) for subwords s"
      topics: ["entropy", "probability", "information"]
      
    - requirement: "fertility_analysis"
      description: "Analyze tokens per word across languages"
      formula: "Fertility = Σ|tokens(w)| / |words| averaged over corpus"
      topics: ["cross_lingual", "morphology", "segmentation"]
      
  algorithm_analysis:
    - requirement: "bpe_complexity_analysis"
      description: "Derive time complexity of BPE encoding"
      formula: "O(n log n) for n characters with proper data structures"
      topics: ["complexity", "algorithms", "optimization"]
      
    - requirement: "vocabulary_construction_complexity"
      description: "Analyze vocabulary learning complexity"
      formula: "O(C * |V| * log|V|) for corpus size C"
      topics: ["learning", "merging", "frequency"]
      
    - requirement: "decode_complexity_analysis"
      description: "Analyze decoding time complexity"
      formula: "O(n) for n tokens with hash table lookup"
      topics: ["decoding", "lookup", "efficiency"]
      
    - requirement: "memory_complexity_analysis"
      description: "Derive memory requirements for tokenizer"
      formula: "O(|V| * avg_token_length + merge_rules)"
      topics: ["memory", "storage", "vocabulary"]
      
  security_analysis:
    - requirement: "collision_probability_bound"
      description: "Bound probability of token collision"
      formula: "P(collision) < 1/|V| for random strings"
      topics: ["security", "probability", "hash"]
      
    - requirement: "adversarial_robustness_analysis"
      description: "Analyze robustness to adversarial perturbations"
      formula: "δ-robustness: ||t(x) - t(x')|| < ε for ||x - x'|| < δ"
      topics: ["adversarial", "robustness", "perturbation"]
      
    - requirement: "information_leakage_bounds"
      description: "Bound information leaked through tokenization"
      formula: "I(training_data; tokenizer) ≤ |V| * log|V|"
      topics: ["privacy", "information", "leakage"]
      
  linguistic_analysis:
    - requirement: "morphological_alignment"
      description: "Measure alignment with morpheme boundaries"
      formula: "Alignment = |matched_boundaries| / |total_morphemes|"
      topics: ["morphology", "linguistics", "evaluation"]
      
    - requirement: "cross_lingual_consistency"
      description: "Measure tokenization consistency across languages"
      formula: "Consistency = 1 - variance(fertility) across languages"
      topics: ["multilingual", "fairness", "consistency"]
      
    - requirement: "semantic_preservation"
      description: "Prove semantic information preserved"
      formula: "Information bottleneck: I(X;T) ≥ I(X;Y) for task Y"
      topics: ["semantics", "information", "compression"]

# === FRAMEWORK-SPECIFIC ISSUES ===
framework_issues:
  huggingface_tokenizers:
    issues:
      - issue: "fast_vs_slow_mismatch"
        description: "Fast tokenizer produces different results than slow"
        symptom: "Tokenization differs between implementations"
        fix: "Use consistent tokenizer type, test both"
        
      - issue: "add_special_tokens_default"
        description: "add_special_tokens default varies by method"
        symptom: "Missing or extra special tokens"
        fix: "Always explicitly specify add_special_tokens"
        
      - issue: "truncation_strategy_mismatch"
        description: "Truncation strategy differs from expected"
        symptom: "Wrong tokens truncated"
        fix: "Explicitly set truncation_strategy"
        
      - issue: "padding_side_ignored"
        description: "Padding side setting not respected"
        symptom: "Padding on wrong side"
        fix: "Set padding_side on tokenizer, not in call"
        
      - issue: "return_tensors_none"
        description: "return_tensors not set returns lists"
        symptom: "Type errors in model forward"
        fix: "Always specify return_tensors"
        
      - issue: "max_length_exceeded_silently"
        description: "Sequences silently truncated"
        symptom: "Information loss without warning"
        fix: "Enable verbose truncation warnings"
        
      - issue: "tokenizer_parallelism_warning"
        description: "Tokenizer parallelism causes deadlock"
        symptom: "Hangs in multiprocessing"
        fix: "Set TOKENIZERS_PARALLELISM=false"
        
  sentencepiece:
    issues:
      - issue: "model_file_not_found"
        description: "Model file path incorrect"
        symptom: "FileNotFoundError on load"
        fix: "Validate path before loading"
        
      - issue: "byte_fallback_disabled"
        description: "Unknown bytes not handled"
        symptom: "UnicodeDecodeError on rare characters"
        fix: "Enable byte_fallback in training"
        
      - issue: "add_dummy_prefix_inconsistent"
        description: "Whitespace prefix handling varies"
        symptom: "First word tokenized differently"
        fix: "Match training and inference settings"
        
      - issue: "encode_as_pieces_vs_ids"
        description: "Wrong method used for encoding"
        symptom: "Strings instead of IDs or vice versa"
        fix: "Use correct method for use case"
        
  tiktoken:
    issues:
      - issue: "encoding_not_found"
        description: "Encoding name not recognized"
        symptom: "ValueError on get_encoding"
        fix: "Use valid encoding name or custom"
        
      - issue: "special_tokens_not_allowed"
        description: "Special tokens in disallowed set"
        symptom: "ValueError on encode"
        fix: "Use allowed_special parameter"
        
      - issue: "encoding_for_model_mismatch"
        description: "Wrong encoding for model"
        symptom: "Token count mismatch"
        fix: "Use encoding_for_model() helper"
        
  spacy:
    issues:
      - issue: "model_not_installed"
        description: "Language model not downloaded"
        symptom: "OSError on load"
        fix: "Run python -m spacy download"
        
      - issue: "custom_tokenizer_rules"
        description: "Custom rules not applied"
        symptom: "Unexpected tokenization"
        fix: "Add rules to tokenizer exceptions"
        
      - issue: "tokenizer_factory_config"
        description: "Factory config not matching"
        symptom: "Different behavior after reload"
        fix: "Configure factory consistently"

# === SCENARIO TEMPLATES ===
scenario_templates:
  multilingual_model:
    description: "Tokenizer for 100+ language model"
    issues:
      - "Vocabulary dominated by high-resource languages"
      - "Fertility imbalance across scripts"
      - "Transliteration inconsistency"
    required_fixes:
      - "Temperature-based vocabulary sampling"
      - "Script-aware token allocation"
      - "Consistent transliteration policy"
      
  code_generation:
    description: "Tokenizer for code generation model"
    issues:
      - "Whitespace significance lost"
      - "Identifier splitting inconsistent"
      - "Special syntax characters as UNK"
    required_fixes:
      - "Preserve significant whitespace"
      - "CamelCase/snake_case aware splitting"
      - "Programming language-aware vocabulary"
      
  retrieval_system:
    description: "Tokenizer for semantic search"
    issues:
      - "Query and document tokenization mismatch"
      - "Exact match degradation"
      - "Entity handling inconsistent"
    required_fixes:
      - "Consistent tokenization policy"
      - "Preserve entity tokens"
      - "Query-document alignment"
      
  chat_application:
    description: "Tokenizer for conversational AI"
    issues:
      - "Prompt injection through special tokens"
      - "User input sanitization gaps"
      - "System/user message boundary"
    required_fixes:
      - "Input sanitization pipeline"
      - "Special token escaping"
      - "Clear message boundaries"
      
  classification_system:
    description: "Tokenizer for text classification"
    issues:
      - "Label words tokenization varies"
      - "Truncation loses key information"
      - "Class imbalance in tokenization"
    required_fixes:
      - "Consistent label tokenization"
      - "Smart truncation strategy"
      - "Length-aware sampling"
      
  translation_system:
    description: "Tokenizer for machine translation"
    issues:
      - "Source-target vocabulary imbalance"
      - "Alignment information lost"
      - "Named entity inconsistency"
    required_fixes:
      - "Shared vocabulary design"
      - "Subword alignment preservation"
      - "Named entity handling"
      
  document_processing:
    description: "Tokenizer for long document understanding"
    issues:
      - "Context window fragmentation"
      - "Section boundary handling"
      - "Table/figure tokenization"
    required_fixes:
      - "Hierarchical tokenization"
      - "Structure-aware segmentation"
      - "Multimodal token handling"
      
  privacy_preserving:
    description: "Tokenizer with privacy requirements"
    issues:
      - "Training data extractable via probing"
      - "Membership inference vulnerability"
      - "PII leakage through OOV"
    required_fixes:
      - "Differential privacy in vocabulary"
      - "Obfuscated OOV handling"
      - "PII-aware tokenization"
      
  streaming_inference:
    description: "Tokenizer for real-time applications"
    issues:
      - "Incremental tokenization inconsistent"
      - "Latency variation"
      - "Token boundary uncertainty"
    required_fixes:
      - "Streaming-aware tokenization"
      - "Latency bounds"
      - "Partial token handling"
      
  api_security:
    description: "Tokenizer for public API"
    issues:
      - "Denial of service via complex input"
      - "Token count manipulation"
      - "Encoding attack vectors"
    required_fixes:
      - "Input length limits"
      - "Complexity bounds"
      - "Encoding validation"

# === REFERENCE SOLUTIONS ===
reference_solutions:
  secure_tokenizer:
    code: |
      """
      Secure tokenizer with comprehensive input validation and normalization.
      """
      
      import unicodedata
      import re
      from typing import List, Dict, Optional, Tuple, Set
      import hashlib
      from collections import OrderedDict
      
      # Dangerous Unicode categories
      DANGEROUS_CATEGORIES: Set[str] = {
          'Cc',  # Control characters (except whitespace)
          'Cf',  # Format characters (includes ZWJ, etc.)
          'Co',  # Private use (unless explicitly allowed)
          'Cs',  # Surrogates (should not appear in valid UTF-8)
      }
      
      # Allowed control characters (whitespace)
      ALLOWED_CONTROL: Set[str] = {'\n', '\r', '\t', ' '}
      
      # Dangerous specific characters
      DANGEROUS_CHARS: Set[str] = {
          '\u200b',  # Zero-width space
          '\u200c',  # Zero-width non-joiner
          '\u200d',  # Zero-width joiner
          '\u200e',  # Left-to-right mark
          '\u200f',  # Right-to-left mark
          '\u202a',  # Left-to-right embedding
          '\u202b',  # Right-to-left embedding
          '\u202c',  # Pop directional formatting
          '\u202d',  # Left-to-right override
          '\u202e',  # Right-to-left override
          '\u2060',  # Word joiner
          '\u2061',  # Function application
          '\u2062',  # Invisible times
          '\u2063',  # Invisible separator
          '\u2064',  # Invisible plus
          '\u2066',  # Left-to-right isolate
          '\u2067',  # Right-to-left isolate
          '\u2068',  # First strong isolate
          '\u2069',  # Pop directional isolate
          '\ufeff',  # BOM / Zero-width no-break space
          '\ufffe',  # Invalid
          '\uffff',  # Invalid
      }
      
      class SecureTokenizer:
          """
          Tokenizer with security hardening against:
          - Unicode normalization attacks
          - Control character injection
          - Homoglyph attacks
          - Token collision attacks
          - Bidirectional text attacks
          """
          
          def __init__(
              self,
              vocab: Dict[str, int],
              merges: List[Tuple[str, str]],
              normalization: str = 'NFKC',
              lowercase: bool = False,
              strip_accents: bool = False,
              max_input_length: int = 1_000_000,
              max_token_length: int = 1000,
          ):
              self.vocab = vocab
              self.vocab_size = len(vocab)
              self.merges = merges
              self.merge_ranks = {merge: i for i, merge in enumerate(merges)}
              self.normalization = normalization
              self.lowercase = lowercase
              self.strip_accents = strip_accents
              self.max_input_length = max_input_length
              self.max_token_length = max_token_length
              
              # Reverse vocabulary for decoding
              self.id_to_token = {v: k for k, v in vocab.items()}
              
              # Special tokens
              self.unk_token = '<unk>'
              self.unk_id = vocab.get(self.unk_token, 0)
              self.pad_token = '<pad>'
              self.pad_id = vocab.get(self.pad_token, 1)
              self.bos_token = '<s>'
              self.bos_id = vocab.get(self.bos_token, 2)
              self.eos_token = '</s>'
              self.eos_id = vocab.get(self.eos_token, 3)
              
              # Validate vocabulary integrity
              self._validate_vocabulary()
              
              # Cache for determinism verification
              self._cache: OrderedDict[str, List[int]] = OrderedDict()
              self._cache_max_size = 10000
          
          def _validate_vocabulary(self):
              """Validate vocabulary for integrity issues."""
              # Check for duplicate IDs
              id_to_tokens: Dict[int, List[str]] = {}
              for token, id_ in self.vocab.items():
                  if id_ not in id_to_tokens:
                      id_to_tokens[id_] = []
                  id_to_tokens[id_].append(token)
              
              duplicates = {id_: tokens for id_, tokens in id_to_tokens.items() 
                           if len(tokens) > 1}
              if duplicates:
                  raise ValueError(f"Vocabulary has duplicate IDs: {duplicates}")
              
              # Check for ID gaps
              max_id = max(self.vocab.values())
              if max_id >= self.vocab_size:
                  raise ValueError(
                      f"Vocabulary size {self.vocab_size} but max ID is {max_id}"
                  )
          
          def sanitize(self, text: str) -> str:
              """
              Sanitize input text for security.
              
              Steps:
              1. Length validation
              2. Remove dangerous characters
              3. Normalize Unicode
              4. Normalize whitespace
              """
              # Length check
              if len(text) > self.max_input_length:
                  raise ValueError(
                      f"Input exceeds max length: {len(text)} > {self.max_input_length}"
                  )
              
              # Remove null bytes
              text = text.replace('\x00', '')
              
              # Remove dangerous characters
              cleaned_chars = []
              for char in text:
                  if char in ALLOWED_CONTROL:
                      cleaned_chars.append(char)
                  elif char in DANGEROUS_CHARS:
                      continue  # Skip dangerous characters
                  elif unicodedata.category(char) in DANGEROUS_CATEGORIES:
                      continue  # Skip dangerous categories
                  else:
                      cleaned_chars.append(char)
              
              text = ''.join(cleaned_chars)
              
              # Unicode normalization
              if self.normalization:
                  text = unicodedata.normalize(self.normalization, text)
              
              # Optional: lowercase
              if self.lowercase:
                  text = text.lower()
              
              # Optional: strip accents
              if self.strip_accents:
                  text = self._strip_accents(text)
              
              # Normalize whitespace
              text = re.sub(r'[\t\r\n]+', ' ', text)
              text = re.sub(r' +', ' ', text)
              text = text.strip()
              
              return text
          
          def _strip_accents(self, text: str) -> str:
              """Remove accents from text."""
              nfd = unicodedata.normalize('NFD', text)
              return ''.join(
                  char for char in nfd
                  if unicodedata.category(char) != 'Mn'
              )
          
          def _get_pairs(self, word: List[str]) -> List[Tuple[str, str]]:
              """Get all adjacent pairs in word, preserving order."""
              pairs = []
              prev = word[0]
              for char in word[1:]:
                  pairs.append((prev, char))
                  prev = char
              return pairs
          
          def _bpe(self, word: str) -> List[str]:
              """
              Apply BPE with deterministic merge order.
              """
              if len(word) == 0:
                  return []
              if len(word) == 1:
                  return [word]
              
              # Split into characters
              word_list = list(word)
              
              while True:
                  # Get all pairs
                  pairs = self._get_pairs(word_list)
                  if not pairs:
                      break
                  
                  # Find best merge (lowest rank)
                  best_pair = None
                  best_rank = float('inf')
                  
                  for pair in pairs:
                      rank = self.merge_ranks.get(pair, float('inf'))
                      if rank < best_rank:
                          best_rank = rank
                          best_pair = pair
                  
                  if best_pair is None or best_rank == float('inf'):
                      break
                  
                  # Apply merge
                  first, second = best_pair
                  merged = first + second
                  
                  new_word = []
                  i = 0
                  while i < len(word_list):
                      if (i < len(word_list) - 1 and
                          word_list[i] == first and
                          word_list[i + 1] == second):
                          new_word.append(merged)
                          i += 2
                      else:
                          new_word.append(word_list[i])
                          i += 1
                  
                  word_list = new_word
              
              return word_list
          
          def tokenize(
              self,
              text: str,
              add_special_tokens: bool = True,
              max_length: Optional[int] = None,
          ) -> List[int]:
              """
              Tokenize text with full security checks.
              """
              # Sanitize input
              text = self.sanitize(text)
              
              # Check cache for determinism
              cache_key = hashlib.sha256(text.encode()).hexdigest()[:32]
              if cache_key in self._cache:
                  cached = self._cache[cache_key]
                  # Move to end (LRU)
                  self._cache.move_to_end(cache_key)
                  return self._add_special(cached, add_special_tokens, max_length)
              
              # Tokenize
              tokens = []
              for word in text.split(' '):
                  if not word:
                      continue
                  
                  # Apply BPE
                  subwords = self._bpe(word)
                  
                  for subword in subwords:
                      if len(subword) > self.max_token_length:
                          # Handle extremely long tokens
                          tokens.append(self.unk_id)
                      elif subword in self.vocab:
                          tokens.append(self.vocab[subword])
                      else:
                          tokens.append(self.unk_id)
              
              # Cache result
              self._cache[cache_key] = tokens.copy()
              if len(self._cache) > self._cache_max_size:
                  self._cache.popitem(last=False)
              
              return self._add_special(tokens, add_special_tokens, max_length)
          
          def _add_special(
              self,
              tokens: List[int],
              add_special_tokens: bool,
              max_length: Optional[int],
          ) -> List[int]:
              """Add special tokens and handle truncation."""
              if add_special_tokens:
                  tokens = [self.bos_id] + tokens + [self.eos_id]
              
              if max_length is not None and len(tokens) > max_length:
                  if add_special_tokens:
                      # Preserve special tokens
                      tokens = tokens[:max_length - 1] + [self.eos_id]
                  else:
                      tokens = tokens[:max_length]
              
              return tokens
          
          def decode(
              self,
              token_ids: List[int],
              skip_special_tokens: bool = True,
          ) -> str:
              """
              Decode token IDs back to text.
              """
              special_ids = {self.pad_id, self.bos_id, self.eos_id, self.unk_id}
              
              tokens = []
              for id_ in token_ids:
                  if skip_special_tokens and id_ in special_ids:
                      continue
                  
                  if id_ in self.id_to_token:
                      tokens.append(self.id_to_token[id_])
                  else:
                      # Unknown ID - don't leak info about vocabulary
                      tokens.append(self.unk_token)
              
              return ' '.join(tokens)
          
          def verify_determinism(self, text: str, iterations: int = 10) -> bool:
              """Verify tokenization is deterministic."""
              results = []
              for _ in range(iterations):
                  # Clear cache to force re-tokenization
                  self._cache.clear()
                  result = self.tokenize(text, add_special_tokens=False)
                  results.append(tuple(result))
              
              return len(set(results)) == 1

  unicode_normalizer:
    code: |
      """
      Comprehensive Unicode normalization utilities.
      """
      
      import unicodedata
      import re
      from typing import Dict, List, Set, Optional
      
      # Homoglyph mappings (common confusables)
      HOMOGLYPHS: Dict[str, str] = {
          # Cyrillic -> Latin
          '\u0430': 'a',  # а -> a
          '\u0435': 'e',  # е -> e
          '\u043e': 'o',  # о -> o
          '\u0440': 'p',  # р -> p
          '\u0441': 'c',  # с -> c
          '\u0443': 'y',  # у -> y
          '\u0445': 'x',  # х -> x
          '\u0410': 'A',  # А -> A
          '\u0412': 'B',  # В -> B
          '\u0415': 'E',  # Е -> E
          '\u041a': 'K',  # К -> K
          '\u041c': 'M',  # М -> M
          '\u041d': 'H',  # Н -> H
          '\u041e': 'O',  # О -> O
          '\u0420': 'P',  # Р -> P
          '\u0421': 'C',  # С -> C
          '\u0422': 'T',  # Т -> T
          '\u0425': 'X',  # Х -> X
          # Greek -> Latin
          '\u0391': 'A',  # Α -> A
          '\u0392': 'B',  # Β -> B
          '\u0395': 'E',  # Ε -> E
          '\u0397': 'H',  # Η -> H
          '\u0399': 'I',  # Ι -> I
          '\u039a': 'K',  # Κ -> K
          '\u039c': 'M',  # Μ -> M
          '\u039d': 'N',  # Ν -> N
          '\u039f': 'O',  # Ο -> O
          '\u03a1': 'P',  # Ρ -> P
          '\u03a4': 'T',  # Τ -> T
          '\u03a5': 'Y',  # Υ -> Y
          '\u03a7': 'X',  # Χ -> X
          '\u03a9': 'Ω',  # Ω -> (keep as is, distinct)
          '\u03b1': 'a',  # α -> a (keep? depends on context)
          '\u03bf': 'o',  # ο -> o
          # Fullwidth -> ASCII
          '０': '0', '１': '1', '２': '2', '３': '3', '４': '4',
          '５': '5', '６': '6', '７': '7', '８': '8', '９': '9',
          'Ａ': 'A', 'Ｂ': 'B', 'Ｃ': 'C', 'Ｄ': 'D', 'Ｅ': 'E',
          'Ｆ': 'F', 'Ｇ': 'G', 'Ｈ': 'H', 'Ｉ': 'I', 'Ｊ': 'J',
          'Ｋ': 'K', 'Ｌ': 'L', 'Ｍ': 'M', 'Ｎ': 'N', 'Ｏ': 'O',
          'Ｐ': 'P', 'Ｑ': 'Q', 'Ｒ': 'R', 'Ｓ': 'S', 'Ｔ': 'T',
          'Ｕ': 'U', 'Ｖ': 'V', 'Ｗ': 'W', 'Ｘ': 'X', 'Ｙ': 'Y',
          'Ｚ': 'Z',
          'ａ': 'a', 'ｂ': 'b', 'ｃ': 'c', 'ｄ': 'd', 'ｅ': 'e',
          'ｆ': 'f', 'ｇ': 'g', 'ｈ': 'h', 'ｉ': 'i', 'ｊ': 'j',
          'ｋ': 'k', 'ｌ': 'l', 'ｍ': 'm', 'ｎ': 'n', 'ｏ': 'o',
          'ｐ': 'p', 'ｑ': 'q', 'ｒ': 'r', 'ｓ': 's', 'ｔ': 't',
          'ｕ': 'u', 'ｖ': 'v', 'ｗ': 'w', 'ｘ': 'x', 'ｙ': 'y',
          'ｚ': 'z',
      }
      
      class UnicodeNormalizer:
          """
          Comprehensive Unicode normalization for NLP.
          """
          
          def __init__(
              self,
              form: str = 'NFKC',
              normalize_homoglyphs: bool = True,
              strip_control: bool = True,
              normalize_whitespace: bool = True,
          ):
              self.form = form
              self.normalize_homoglyphs = normalize_homoglyphs
              self.strip_control = strip_control
              self.normalize_whitespace = normalize_whitespace
          
          def normalize(self, text: str) -> str:
              """Apply all normalizations."""
              # Unicode normalization form
              text = unicodedata.normalize(self.form, text)
              
              # Homoglyph normalization
              if self.normalize_homoglyphs:
                  text = self._normalize_homoglyphs(text)
              
              # Strip control characters
              if self.strip_control:
                  text = self._strip_control(text)
              
              # Normalize whitespace
              if self.normalize_whitespace:
                  text = self._normalize_whitespace(text)
              
              return text
          
          def _normalize_homoglyphs(self, text: str) -> str:
              """Replace homoglyphs with canonical forms."""
              return ''.join(HOMOGLYPHS.get(c, c) for c in text)
          
          def _strip_control(self, text: str) -> str:
              """Remove control characters except whitespace."""
              result = []
              for char in text:
                  cat = unicodedata.category(char)
                  if cat == 'Cc':  # Control character
                      if char in '\t\n\r':
                          result.append(char)
                      # Skip other control chars
                  elif cat == 'Cf':  # Format character
                      continue  # Skip format chars (ZWJ, etc.)
                  else:
                      result.append(char)
              return ''.join(result)
          
          def _normalize_whitespace(self, text: str) -> str:
              """Normalize all whitespace to single spaces."""
              # Replace various whitespace with space
              text = re.sub(r'[\s\xa0\u2000-\u200a\u2028\u2029\u202f\u205f\u3000]+', ' ', text)
              return text.strip()
          
          def detect_suspicious(self, text: str) -> List[Dict]:
              """Detect suspicious Unicode patterns."""
              issues = []
              
              # Check for mixed scripts
              scripts = set()
              for char in text:
                  try:
                      script = unicodedata.name(char).split()[0]
                      if script in {'LATIN', 'CYRILLIC', 'GREEK'}:
                          scripts.add(script)
                  except ValueError:
                      pass
              
              if len(scripts) > 1:
                  issues.append({
                      'type': 'mixed_scripts',
                      'scripts': list(scripts),
                      'severity': 'medium',
                  })
              
              # Check for homoglyphs
              homoglyphs_found = []
              for i, char in enumerate(text):
                  if char in HOMOGLYPHS:
                      homoglyphs_found.append({
                          'position': i,
                          'char': char,
                          'replacement': HOMOGLYPHS[char],
                      })
              
              if homoglyphs_found:
                  issues.append({
                      'type': 'homoglyphs',
                      'instances': homoglyphs_found,
                      'severity': 'high',
                  })
              
              # Check for bidirectional override
              bidi_chars = ['\u202a', '\u202b', '\u202c', '\u202d', '\u202e',
                          '\u2066', '\u2067', '\u2068', '\u2069']
              bidi_found = [i for i, c in enumerate(text) if c in bidi_chars]
              
              if bidi_found:
                  issues.append({
                      'type': 'bidi_override',
                      'positions': bidi_found,
                      'severity': 'critical',
                  })
              
              # Check for zero-width characters
              zw_chars = ['\u200b', '\u200c', '\u200d', '\u2060', '\ufeff']
              zw_found = [i for i, c in enumerate(text) if c in zw_chars]
              
              if zw_found:
                  issues.append({
                      'type': 'zero_width',
                      'positions': zw_found,
                      'severity': 'high',
                  })
              
              return issues

  vocabulary_analyzer:
    code: |
      """
      Vocabulary analysis and optimization tools.
      """
      
      from typing import Dict, List, Tuple, Counter
      from collections import Counter
      import math
      
      class VocabularyAnalyzer:
          """
          Analyze vocabulary for optimization and security.
          """
          
          def __init__(self, vocab: Dict[str, int]):
              self.vocab = vocab
              self.id_to_token = {v: k for k, v in vocab.items()}
          
          def compute_statistics(self) -> Dict:
              """Compute vocabulary statistics."""
              tokens = list(self.vocab.keys())
              
              # Length distribution
              lengths = [len(t) for t in tokens]
              
              # Character coverage
              all_chars = set()
              for token in tokens:
                  all_chars.update(token)
              
              # Prefix/suffix analysis
              prefixes = Counter()
              suffixes = Counter()
              for token in tokens:
                  if len(token) >= 2:
                      prefixes[token[:2]] += 1
                      suffixes[token[-2:]] += 1
              
              return {
                  'vocab_size': len(self.vocab),
                  'avg_token_length': sum(lengths) / len(lengths),
                  'max_token_length': max(lengths),
                  'min_token_length': min(lengths),
                  'unique_characters': len(all_chars),
                  'top_prefixes': prefixes.most_common(10),
                  'top_suffixes': suffixes.most_common(10),
              }
          
          def find_collisions(self) -> List[Tuple[str, str]]:
              """Find tokens that might collide after normalization."""
              import unicodedata
              
              normalized: Dict[str, List[str]] = {}
              
              for token in self.vocab.keys():
                  # Normalize
                  norm = unicodedata.normalize('NFKC', token.lower())
                  if norm not in normalized:
                      normalized[norm] = []
                  normalized[norm].append(token)
              
              # Find collisions
              collisions = []
              for norm, tokens in normalized.items():
                  if len(tokens) > 1:
                      for i, t1 in enumerate(tokens):
                          for t2 in tokens[i+1:]:
                              collisions.append((t1, t2))
              
              return collisions
          
          def compute_entropy(self, corpus: List[str]) -> float:
              """Compute entropy of token distribution on corpus."""
              token_counts = Counter()
              total = 0
              
              for text in corpus:
                  # Simple whitespace tokenization for analysis
                  for word in text.split():
                      if word in self.vocab:
                          token_counts[word] += 1
                          total += 1
              
              if total == 0:
                  return 0.0
              
              entropy = 0.0
              for count in token_counts.values():
                  p = count / total
                  if p > 0:
                      entropy -= p * math.log2(p)
              
              return entropy
          
          def compute_coverage(self, corpus: List[str]) -> Dict:
              """Compute vocabulary coverage on corpus."""
              total_words = 0
              covered_words = 0
              oov_words = Counter()
              
              for text in corpus:
                  for word in text.split():
                      total_words += 1
                      if word in self.vocab:
                          covered_words += 1
                      else:
                          oov_words[word] += 1
              
              return {
                  'coverage': covered_words / total_words if total_words > 0 else 0,
                  'total_words': total_words,
                  'covered_words': covered_words,
                  'oov_count': len(oov_words),
                  'top_oov': oov_words.most_common(20),
              }

# === TEST CASES ===
test_cases:
  fail_to_pass:
    - "test_unicode_normalization_consistency"
    - "test_homoglyph_detection"
    - "test_control_character_filtering"
    - "test_bidi_attack_prevention"
    - "test_deterministic_tokenization"
    - "test_vocabulary_collision_detection"
    - "test_token_boundary_security"
    - "test_special_token_protection"
    - "test_encoding_validation"
    - "test_adversarial_input_handling"
    
  pass_to_pass:
    - "test_basic_tokenization"
    - "test_decode_roundtrip"
    - "test_special_tokens_added"
    - "test_truncation_correct"
    - "test_padding_correct"

# === COMPREHENSIVE VARIABLES ===
variables:
  scenario_type:
    type: string
    options:
      - "text_classification"
      - "language_modeling"
      - "machine_translation"
      - "question_answering"
      - "named_entity_recognition"
      - "sentiment_analysis"
      - "text_generation"
      - "summarization"
      - "dialogue_system"
      - "code_generation"
      - "semantic_search"
      - "information_retrieval"
      - "text_embedding"
      - "document_classification"
      - "spam_detection"
      - "toxicity_detection"
      - "language_identification"
      - "part_of_speech_tagging"
      - "dependency_parsing"
      - "coreference_resolution"
      
  tokenizer_type:
    type: string
    options:
      - "bpe"
      - "wordpiece"
      - "unigram"
      - "sentencepiece"
      - "tiktoken"
      - "character"
      - "word"
      - "byte"
      
  normalization_form:
    type: string
    options:
      - "NFC"
      - "NFD"
      - "NFKC"
      - "NFKD"
      - "none"
      
  language:
    type: string
    options:
      - "english"
      - "chinese"
      - "japanese"
      - "korean"
      - "arabic"
      - "russian"
      - "german"
      - "french"
      - "spanish"
      - "portuguese"
      - "hindi"
      - "thai"
      - "vietnamese"
      - "multilingual"
      
  attack_type:
    type: string
    options:
      - "homoglyph"
      - "zero_width"
      - "bidi_override"
      - "normalization_bypass"
      - "token_collision"
      - "prompt_injection"
      - "encoding_confusion"
      - "dos_input"

# === PROBLEM STATEMENT TEMPLATE ===
problem_statement: |
  An NLP model's tokenizer has edge cases causing security and correctness issues:
  
  1. Different Unicode representations tokenize inconsistently
     - "café" (precomposed) vs "cafe\u0301" (decomposed) produce different tokens
     - NFKC normalization not applied consistently
  
  2. Adversarial inputs exploit tokenization vulnerabilities
     - Homoglyph attacks using Cyrillic/Greek characters
     - Zero-width characters manipulating token boundaries
     - Bidirectional override hiding malicious content
  
  3. BPE algorithm produces non-deterministic results
     - Merge order varies between runs
     - Same input produces different token sequences
  
  4. Security vulnerabilities in tokenizer
     - Control characters not filtered
     - Special tokens injectable via user input
     - Token collision enables prompt injection
  
  5. Performance and correctness issues
     - OOV handling leaks training data patterns
     - Vocabulary collision rate: {{ collision_rate }}%
     - Maximum input causes DoS ({{ max_input_time }}ms)
  
  The tokenizer is used for:
  - Vocabulary size: {{ vocab_size }} tokens
  - Languages: {{ languages }}
  - Use case: {{ scenario_type }}

requirements: |
  - Normalize all Unicode input consistently ({{ normalization_form }})
  - Handle adversarial token collision attempts
  - Ensure 100% deterministic tokenization
  - Safely handle control and bidirectional characters
  - Prevent training data extraction via OOV analysis
  - Maintain tokenization speed < {{ max_latency }}ms per 1K tokens
  - Support all target languages with balanced coverage

interface: |
  Input: Raw text string (UTF-8)
  Output: List of token IDs
  Vocabulary: {{ vocab_size }} tokens + special tokens
  
  Security requirements:
  - Input sanitization
  - Attack detection
  - Audit logging

# === INSTRUCTION TEMPLATE ===
instruction_template: |
  You are fixing tokenizer security issues in a {{ scenario_type }} NLP system.
  The tokenizer code is at {{ code_path }}.
  
  Tokenizer type: {{ tokenizer_type }}
  Vocabulary size: {{ vocab_size }}
  Languages: {{ languages }}
  
  Discovered vulnerabilities:
  {{ vulnerabilities }}
  
  Attack vectors to defend against:
  {{ attack_vectors }}
  
  Your task:
  {{ task_steps }}
  
  Security requirements:
  {{ security_requirements }}
  
  Performance requirements:
  {{ performance_requirements }}

# === TASK STEP TEMPLATES ===
task_step_templates:
  basic:
    - "Implement consistent Unicode normalization ({{ normalization_form }})"
    - "Filter dangerous control and zero-width characters"
    - "Ensure deterministic BPE merge ordering"
    - "Add special token protection"
    - "Implement input sanitization pipeline"
    - "Verify with security test suite"
    
  advanced:
    - "Analyze vocabulary for collision vulnerabilities"
    - "Implement homoglyph detection and normalization"
    - "Design bidirectional text attack prevention"
    - "Create adversarial input detection system"
    - "Implement timing-safe tokenization"
    - "Design privacy-preserving OOV handling"
    
  expert:
    - "Prove tokenization determinism mathematically"
    - "Derive bounds on collision probability"
    - "Analyze information leakage through tokenization"
    - "Design differential privacy for vocabulary"
    - "Implement formal verification of security properties"
    - "Create comprehensive adversarial test suite"

# === ANTI-HARDCODING MEASURES ===
anti_hardcoding:
  canary_tokens: true
  randomize_paths: true
  dynamic_content: true
  vocabulary_variations: true
  language_combinations: true
  attack_vector_variations: true
  
  parameter_ranges:
    vocab_size: [1000, 8000, 32000, 50000, 100000, 250000]
    max_length: [128, 256, 512, 1024, 2048, 4096, 8192]
    max_input_length: [10000, 100000, 1000000]
    max_latency_ms: [1, 5, 10, 50, 100]

# === EVALUATION CRITERIA ===
evaluation_criteria:
  correctness:
    - "Tokenization is 100% deterministic"
    - "Unicode normalization applied consistently"
    - "All security tests pass"
    - "Decode(encode(text)) preserves meaning"
    
  security:
    - "No control character injection possible"
    - "Homoglyph attacks detected or normalized"
    - "Bidirectional attacks neutralized"
    - "Special tokens protected from injection"
    - "No information leakage through OOV"
    
  performance:
    - "Tokenization latency within bounds"
    - "Memory usage acceptable"
    - "DoS resistance verified"
    
  code_quality:
    - "Comprehensive input validation"
    - "Proper error handling"
    - "Well-documented security measures"
    - "Testable and modular design"

# === METADATA ===
metadata:
  created: "2024-01-01"
  updated: "2024-12-01"
  author: "NLP Security Team"
  tags:
    - "tokenization"
    - "unicode"
    - "security"
    - "nlp"
    - "adversarial"
  difficulty_levels:
    medium: "Basic normalization and sanitization"
    hard: "Comprehensive security hardening"
    expert: "Formal verification and privacy guarantees"
    research: "Novel attack detection and prevention"
