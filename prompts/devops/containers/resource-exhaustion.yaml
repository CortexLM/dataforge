id: "devops-container-resource-001"
version: "2.0.0"
category: "devops"
subcategory: "containers"

# === LLM GENERATION FRAMEWORK ===
generation_framework:
  multi_conversation_workflow:
    phase_1_research: "Research obscure resource exhaustion vectors and kernel limits"
    phase_2_creation: "Create complex multi-container resource contention scenarios"
    phase_3_amplification: "Add cascading failures with recovery and scaling requirements"
    phase_4_verification: "Validate task requires production-level resource management expertise"
  
  complexity_targets:
    minimum_unique_tasks: 10000
    difficulty_range: ["medium", "hard", "expert", "nightmare"]
    domain_expertise_required: ["resource_management", "kernel_tuning", "capacity_planning", "observability"]
  
  task_generation_rules:
    - "Combine 3+ resource types per task"
    - "Include real-world failure case studies"
    - "Require performance profiling skills"
    - "Add cost optimization constraints"
    - "Include multi-tenant isolation scenarios"
  
  phase_4_verification:
    requirements:
      - "Has at least 5 interacting hidden traps"
      - "Has cascading failures across infrastructure layers (network, compute, storage)"
      - "Requires knowledge of cloud provider internals and Kubernetes scheduler"
      - "Would take experienced SREs 30+ minutes"

# SWE-bench_Pro style fields
problem_statement: |
  Containerized applications are experiencing resource issues:
  
  1. OOM kills due to no memory limits
  2. CPU starvation from noisy neighbors
  3. Disk exhaustion from log files
  4. PID exhaustion from fork bombs
  5. Network bandwidth saturation
  6. File descriptor exhaustion
  7. Inode depletion on shared filesystems
  8. Ephemeral storage overflow
  9. GPU memory fragmentation
  10. NUMA node imbalance

requirements: |
  - Set appropriate resource limits
  - Implement resource requests for scheduling
  - Configure proper health checks
  - Set up resource monitoring
  - Handle graceful degradation
  - Implement capacity planning
  - Configure autoscaling policies
  - Establish resource quotas

interface: |
  Input: Container deployments with resource issues
  Output: Properly resource-limited configuration with monitoring
  Validation: No OOM kills, fair resource sharing, cost optimized

# === COMPREHENSIVE TOPIC UNIVERSE ===
# 100+ detailed topics for resource exhaustion scenarios
topic_universe:
  # Memory Management (30 topics)
  memory_management:
    linux_memory_subsystem:
      - virtual_memory: |
          Virtual memory management:
          - vm.overcommit_memory settings (0, 1, 2)
          - vm.overcommit_ratio tuning
          - RLIMIT_AS vs RLIMIT_DATA
          - mmap() behavior and limits
          - Memory-mapped file handling
      
      - memory_cgroup: |
          Memory cgroup deep dive:
          - memory.limit_in_bytes (v1)
          - memory.max (v2)
          - memory.high (soft limit)
          - memory.low (protection)
          - memory.min (hard protection)
          - memory.swap.max
          - memory.oom.group
      
      - oom_killer: |
          OOM killer mechanics:
          - oom_score_adj (-1000 to 1000)
          - /proc/[pid]/oom_score
          - oom_kill_disable cgroup parameter
          - OOM priority inheritance
          - Memory reclaim before OOM
      
      - memory_pressure: |
          Memory pressure handling:
          - memory.pressure (PSI metrics)
          - PGSCAN/PGSTEAL rates
          - Swap thrashing detection
          - Memory compaction
          - Direct reclaim
      
      - memory_accounting: |
          Memory accounting nuances:
          - RSS vs VSZ vs PSS
          - Shared memory accounting
          - Kernel memory (kmem) limits
          - TCP buffer memory
          - File cache accounting
      
      - huge_pages: |
          Huge page management:
          - Transparent Huge Pages (THP)
          - Explicit huge pages (hugetlbfs)
          - NUMA huge page allocation
          - Huge page fragmentation
          - Kubernetes hugepages-* resources
    
    container_memory:
      - java_memory: |
          JVM memory in containers:
          - -XX:MaxRAMPercentage
          - UseContainerSupport flag
          - Off-heap memory accounting
          - Metaspace sizing
          - GC selection for containers
          - Native memory tracking
      
      - go_memory: |
          Go runtime memory:
          - GOMEMLIMIT (Go 1.19+)
          - GOGC tuning
          - Memory ballast pattern
          - Goroutine stack sizes
          - GC pacer behavior
      
      - python_memory: |
          Python memory management:
          - Reference counting limitations
          - Garbage collector generations
          - memory_profiler usage
          - Tracemalloc debugging
          - numpy/pandas memory patterns
      
      - nodejs_memory: |
          Node.js memory limits:
          - --max-old-space-size
          - V8 heap snapshots
          - Buffer vs ArrayBuffer
          - Stream backpressure
          - Memory leak detection
      
      - rust_memory: |
          Rust memory in containers:
          - Jemalloc vs system allocator
          - Memory fragmentation
          - Arena allocators
          - Stack vs heap trade-offs
    
    memory_troubleshooting:
      - memory_leak_detection: |
          Memory leak identification:
          - smem for proportional memory
          - pmap for detailed mapping
          - /proc/[pid]/smaps analysis
          - heaptrack for allocation tracking
          - Continuous profiling tools
      
      - cache_memory: |
          Page cache behavior:
          - Reclaimable vs non-reclaimable
          - drop_caches safety
          - File-backed vs anonymous
          - MADVISE hints
      
      - swap_management: |
          Swap in containerized environments:
          - Kubernetes swap support (1.28+)
          - vm.swappiness tuning
          - Swap priority
          - ZRAM vs traditional swap
          - Swap accounting in cgroups

  # CPU Management (25 topics)
  cpu_management:
    cpu_scheduling:
      - cfs_scheduler: |
          Completely Fair Scheduler:
          - cpu.shares (cgroup v1)
          - cpu.weight (cgroup v2)
          - cpu.max (quota/period)
          - cpu.cfs_quota_us
          - cpu.cfs_period_us
          - Throttling mechanics
      
      - cpu_throttling: |
          CPU throttling deep dive:
          - container_cpu_cfs_throttled_periods_total
          - container_cpu_cfs_throttled_seconds_total
          - Burst vs sustained workloads
          - Multi-core throttling behavior
          - Period tuning (100ms default)
      
      - cpu_pinning: |
          CPU affinity and cpusets:
          - cpuset.cpus configuration
          - cpuset.mems for NUMA
          - Isolating CPUs (isolcpus)
          - CPU manager policies (static/none)
          - Exclusive CPU allocation
      
      - real_time_scheduling: |
          Real-time scheduling:
          - SCHED_FIFO and SCHED_RR
          - rt_runtime_us limits
          - Priority inversion
          - Latency-sensitive workloads
      
      - numa_awareness: |
          NUMA topology:
          - numactl usage
          - Topology manager policies
          - Memory locality
          - Cross-NUMA penalties
          - NUMA balancing
    
    cpu_troubleshooting:
      - cpu_profiling: |
          CPU profiling techniques:
          - perf top/record/report
          - flamegraph generation
          - BPF-based profiling (bcc)
          - Continuous profiling (Parca)
          - Language-specific profilers
      
      - runqueue_analysis: |
          Run queue analysis:
          - /proc/schedstat
          - runqlat BPF tool
          - Context switch overhead
          - Migration costs
      
      - interrupt_handling: |
          Interrupt processing:
          - SoftIRQ overhead
          - IRQ balance
          - RPS/RFS configuration
          - NAPI polling
      
      - cpu_frequency: |
          CPU frequency management:
          - CPUFreq governors
          - P-states vs C-states
          - Turbo boost impact
          - Power capping (RAPL)
    
    container_cpu:
      - guaranteed_cpu: |
          Guaranteed CPU resources:
          - request = limit pattern
          - Static CPU manager
          - CPU allocation visibility
          - Guaranteed QoS class
      
      - burstable_cpu: |
          Burstable CPU patterns:
          - Request << limit design
          - Burst utilization tracking
          - Noisy neighbor mitigation
          - Scheduler latency impact

  # Storage and I/O (25 topics)
  storage_io:
    disk_resources:
      - ephemeral_storage: |
          Ephemeral storage management:
          - ephemeral-storage requests/limits
          - emptyDir sizeLimit
          - Container writable layer
          - Log file accounting
          - Eviction thresholds
      
      - persistent_volumes: |
          Persistent volume resource management:
          - Storage class provisioners
          - Volume expansion
          - Quota enforcement
          - Thin vs thick provisioning
          - WaitForFirstConsumer binding
      
      - inode_exhaustion: |
          Inode management:
          - df -i monitoring
          - Filesystem inode density
          - Small file workloads
          - Inode cache pressure
          - Per-container inode limits
      
      - storage_drivers: |
          Container storage drivers:
          - overlay2 performance
          - devicemapper direct-lvm
          - btrfs subvolumes
          - ZFS zvols
          - VirtioFS for VMs
    
    io_scheduling:
      - blkio_cgroup: |
          Block I/O cgroup controls:
          - io.weight (cgroup v2)
          - io.max (bandwidth/IOPS limits)
          - blkio.throttle (v1)
          - BFQ vs mq-deadline
          - io.latency targets
      
      - io_priority: |
          I/O priority classes:
          - IOPRIO_CLASS_RT
          - IOPRIO_CLASS_BE
          - IOPRIO_CLASS_IDLE
          - ionice command
          - CFQ vs BFQ differences
      
      - direct_io: |
          Direct I/O considerations:
          - O_DIRECT bypass
          - Page cache contention
          - Alignment requirements
          - SSD vs HDD patterns
      
      - async_io: |
          Asynchronous I/O:
          - io_uring performance
          - libaio usage
          - POSIX AIO limitations
          - Kernel queue depth
    
    io_troubleshooting:
      - iowait_analysis: |
          I/O wait debugging:
          - iostat -xz interpretation
          - await vs svctm
          - Queue depth analysis
          - %util limitations on SSDs
      
      - latency_tracing: |
          I/O latency tracing:
          - biolatency BPF tool
          - blktrace analysis
          - fio benchmarking
          - Storage fabric latency
      
      - log_management: |
          Container log management:
          - Log rotation configuration
          - json-file driver limits
          - journald integration
          - Fluentd/Fluentbit patterns

  # Network Resources (20 topics)
  network_resources:
    bandwidth_management:
      - network_cgroup: |
          Network bandwidth controls:
          - net_cls cgroup
          - tc (traffic control) rules
          - Bandwidth plugin (Kubernetes)
          - CNI bandwidth limiting
          - Ingress vs egress shaping
      
      - rate_limiting: |
          Network rate limiting:
          - Token bucket filters
          - HTB qdisc
          - Policing vs shaping
          - Burst allowance
          - Fair queuing
      
      - qos_classes: |
          Network QoS classes:
          - DSCP marking
          - Priority queuing
          - WFQ implementation
          - Traffic classification
    
    connection_resources:
      - connection_limits: |
          Connection resource limits:
          - RLIMIT_NOFILE
          - net.core.somaxconn
          - tcp_max_syn_backlog
          - Connection tracking (conntrack)
          - nf_conntrack_max tuning
      
      - socket_buffers: |
          Socket buffer management:
          - net.core.rmem_max/wmem_max
          - tcp_rmem/tcp_wmem
          - UDP buffer sizing
          - Memory pressure handling
      
      - port_exhaustion: |
          Port exhaustion:
          - Ephemeral port range
          - TIME_WAIT accumulation
          - tcp_tw_reuse
          - Source port randomization
    
    network_troubleshooting:
      - network_profiling: |
          Network performance analysis:
          - ss -s statistics
          - netstat vs ss
          - eBPF network tracing
          - tcpdump patterns
      
      - dns_performance: |
          DNS resolution performance:
          - ndots:5 impact
          - search domain expansion
          - DNS cache tuning
          - CoreDNS performance

  # Process and File Descriptor Resources (15 topics)
  process_resources:
    pid_management:
      - pid_limits: |
          PID resource management:
          - pids.max cgroup control
          - kernel.pid_max
          - Per-user PID limits
          - Fork bomb protection
          - Thread vs process counting
      
      - zombie_processes: |
          Zombie process handling:
          - Init process in containers
          - tini/dumb-init usage
          - PID 1 signal handling
          - Orphan process reaping
      
      - process_tree: |
          Process hierarchy:
          - cgroup.procs management
          - Namespace process visibility
          - systemd-cgtop monitoring
          - Process tree limits
    
    file_descriptors:
      - fd_limits: |
          File descriptor limits:
          - RLIMIT_NOFILE per process
          - fs.file-max system-wide
          - fs.nr_open maximum
          - /proc/sys/fs/file-nr monitoring
      
      - fd_leaks: |
          File descriptor leak detection:
          - lsof analysis
          - /proc/[pid]/fd enumeration
          - fdleak tools
          - Automatic FD close patterns
      
      - epoll_scaling: |
          Epoll scalability:
          - EPOLLEXCLUSIVE
          - Level vs edge triggering
          - Event loop sizing
          - Wakeup thundering herd
    
    ipc_resources:
      - shared_memory: |
          Shared memory limits:
          - kernel.shmmax
          - kernel.shmall
          - POSIX shm_open limits
          - SysV shared memory
      
      - semaphores: |
          Semaphore limits:
          - kernel.sem settings
          - SEMMSL/SEMMNS/SEMOPM/SEMMNI
          - Database semaphore requirements
      
      - message_queues: |
          Message queue limits:
          - kernel.msgmax
          - kernel.msgmnb
          - POSIX mq_* limits
          - mqueue filesystem

  # GPU and Accelerator Resources (10 topics)
  accelerator_resources:
    nvidia_gpu:
      - gpu_memory: |
          GPU memory management:
          - nvidia-smi monitoring
          - MIG (Multi-Instance GPU)
          - GPU memory limits
          - CUDA memory pools
          - Unified memory
      
      - gpu_compute: |
          GPU compute resources:
          - SM utilization
          - GPU time-slicing
          - MPS (Multi-Process Service)
          - CUDA streams
      
      - gpu_sharing: |
          GPU sharing patterns:
          - Single GPU, multiple pods
          - Time-slicing configuration
          - MIG partitioning
          - GPU affinity
    
    other_accelerators:
      - tpu_resources: |
          TPU resource management:
          - TPU chip allocation
          - TPU memory hierarchy
          - TPU pod configurations
      
      - fpga_resources: |
          FPGA resource management:
          - Bitstream management
          - Partial reconfiguration
          - Resource isolation

# === FAILURE SCENARIOS ===
# 50+ detailed failure scenarios
failure_scenarios:
  memory_failures:
    - scenario: "OOM cascade across pods"
      description: |
        1. Memory-intensive job starts
        2. Consumes all available memory
        3. Kernel OOM killer activates
        4. Critical system pods killed
        5. Node becomes NotReady
        6. Pods rescheduled, cascade continues
      detection:
        - "dmesg | grep -i oom"
        - "kubectl get events --field-selector reason=OOMKilling"
        - "container_oom_events_total metric"
      recovery_steps:
        - "Cordon affected nodes"
        - "Investigate memory consumption"
        - "Set appropriate memory limits"
        - "Enable memory QoS (cgroup v2)"
    
    - scenario: "Memory leak causes gradual degradation"
      description: |
        1. Application leaks 100MB/hour
        2. Memory usage grows slowly
        3. Increased GC pressure
        4. Response times degrade
        5. Eventually OOM after 2 days
      detection:
        - "container_memory_working_set_bytes trend"
        - "GC pause time metrics"
        - "Heap profiler snapshots"
    
    - scenario: "JVM container memory misconfiguration"
      description: |
        1. JVM unaware of cgroup limits
        2. Default heap = 1/4 physical RAM
        3. Container limit = 512Mi, heap = 4Gi
        4. Immediate OOM on startup
      fix: |
        Use -XX:MaxRAMPercentage=75 or explicit -Xmx
    
    - scenario: "Swap exhaustion with memory pressure"
      description: |
        1. Swap enabled in cluster (K8s 1.28+)
        2. Memory.high breached, swap activated
        3. I/O bottleneck from swap
        4. Severe latency impact
        5. System becomes unresponsive
  
  cpu_failures:
    - scenario: "CPU throttling causes latency spikes"
      description: |
        1. Container uses 200m CPU limit
        2. Burst workload needs 1000m briefly
        3. Throttled for 80ms every 100ms period
        4. Request latency increases 10x
        5. Health checks fail
        6. Pod restarted, exacerbating issue
      detection:
        - "container_cpu_cfs_throttled_periods_total"
        - "Latency percentile increase"
      fix: |
        - Increase CPU limit
        - Use cpu.cfs_burst_us (kernel 5.14+)
        - Consider burstable QoS
    
    - scenario: "Noisy neighbor CPU starvation"
      description: |
        1. Pod A: requests=100m, limit=4000m
        2. Pod B: requests=100m, limit=100m
        3. Pod A bursts to 3900m
        4. Pod B starved despite request
        5. Scheduler sees both as equal priority
      mitigation:
        - "Use static CPU manager for critical pods"
        - "Set limits closer to requests"
        - "Implement pod priority classes"
    
    - scenario: "NUMA imbalance causing latency"
      description: |
        1. Container allocated CPUs on node 0
        2. Memory allocated on node 1
        3. Cross-NUMA memory access penalty
        4. 30% latency increase
      fix: |
        Enable topology manager with single-numa-node policy
  
  storage_failures:
    - scenario: "Ephemeral storage exhaustion"
      description: |
        1. Container writes temp files
        2. No ephemeral-storage limit set
        3. Node disk fills up
        4. Kubelet evicts ALL pods on node
        5. Workloads disrupted cluster-wide
      prevention:
        - "Set ephemeral-storage limits"
        - "Configure kubelet eviction thresholds"
        - "Monitor disk usage"
    
    - scenario: "Log flood fills container layer"
      description: |
        1. Application error loop
        2. 100GB logs generated in 1 hour
        3. Container writable layer exhausted
        4. Container becomes unresponsive
        5. No space for recovery actions
      prevention:
        - "Configure log rotation"
        - "Set max-size in logging driver"
        - "Use external log aggregation"
    
    - scenario: "Inode exhaustion from small files"
      description: |
        1. Application creates 10M small files
        2. Filesystem inodes exhausted
        3. "No space left on device" (misleading)
        4. df shows space available
        5. df -i shows 100% inode usage
    
    - scenario: "Storage class throttling"
      description: |
        1. Cloud storage IOPS limit reached
        2. I/O operations queue
        3. Application timeouts
        4. Database corruption risk
  
  network_failures:
    - scenario: "Connection tracking table exhaustion"
      description: |
        1. High connection rate (10K/sec)
        2. nf_conntrack table fills
        3. New connections dropped
        4. "nf_conntrack: table full" in dmesg
        5. Service completely unavailable
      fix:
        - "Increase net.netfilter.nf_conntrack_max"
        - "Tune timeout values"
        - "Use IPVS mode in kube-proxy"
    
    - scenario: "Port exhaustion for outbound connections"
      description: |
        1. Many short-lived outbound connections
        2. Ephemeral ports exhausted
        3. TIME_WAIT accumulation
        4. Cannot establish new connections
      fix:
        - "Connection pooling"
        - "Increase ephemeral port range"
        - "Enable tcp_tw_reuse"
    
    - scenario: "DNS query storm"
      description: |
        1. Each DNS query generates 6+ lookups
        2. High ndots value (default 5)
        3. CoreDNS overwhelmed
        4. DNS resolution timeouts
        5. Application failures cascade
  
  pid_failures:
    - scenario: "Fork bomb escapes limits"
      description: |
        1. Malicious/buggy code forks rapidly
        2. PID limit per container reached
        3. But system PIDs still available
        4. Child processes escape to host
        5. Node becomes unresponsive
      prevention:
        - "Set pids-limit in runtime"
        - "Enable kernel.pid_max protection"
        - "Monitor process count"
    
    - scenario: "Zombie process accumulation"
      description: |
        1. Container lacks proper init
        2. Child processes not reaped
        3. Zombie processes accumulate
        4. PID namespace fills with zombies
        5. No PIDs for new processes
      fix: |
        Use proper init system (tini, dumb-init)
  
  cascading_failures:
    - scenario: "Resource starvation cascade"
      description: |
        1. Node A experiences memory pressure
        2. Pods evicted, rescheduled to Node B
        3. Node B now overloaded
        4. Node B pods evicted to Node C
        5. Cascade continues until cluster-wide outage
      prevention:
        - "Pod disruption budgets"
        - "Anti-affinity rules"
        - "Resource quota per namespace"
        - "Cluster autoscaler properly configured"

# === CLOUD-SPECIFIC ISSUES ===
# 100+ cloud-specific resource management issues
cloud_specific_issues:
  aws:
    eks:
      - instance_type_resources: |
          EKS instance resource considerations:
          - vCPU to ECU mapping
          - Burstable instances (t3) behavior
          - Credit balance exhaustion
          - Network bandwidth limits per instance type
          - ENI and IP limits
          - EBS bandwidth sharing with network
      
      - eks_resource_management: |
          EKS-specific resource features:
          - Cluster autoscaler IAM requirements
          - Karpenter provisioner constraints
          - Spot instance interruptions
          - Reserved capacity planning
          - Compute Savings Plans
      
      - ebs_performance: |
          EBS volume performance:
          - GP3 IOPS/throughput provisioning
          - io2 Block Express
          - Burst balance (gp2)
          - Volume-instance bandwidth limits
          - Multi-attach considerations
      
      - eni_limits: |
          ENI and IP address limits:
          - ENIs per instance type
          - IPs per ENI
          - Prefix delegation mode
          - WARM_ENI_TARGET tuning
          - Custom networking mode
    
    ecs:
      - fargate_resources: |
          Fargate resource configuration:
          - CPU/memory combinations
          - 4 vCPU maximum
          - Ephemeral storage (20-200 GiB)
          - No GPU support
          - Network bandwidth limits
      
      - ec2_capacity_provider: |
          EC2 capacity provider:
          - Managed scaling configuration
          - Target capacity percentage
          - Warm pool configuration
          - Instance weight
    
    lambda:
      - lambda_resources: |
          Lambda resource allocation:
          - Memory proportional to CPU
          - 10GB maximum memory
          - 15 minute timeout
          - Provisioned concurrency
          - Reserved concurrency
          - Ephemeral storage (512MB-10GB)
  
  gcp:
    gke:
      - machine_type_resources: |
          GKE machine type considerations:
          - Predefined vs custom machine types
          - Local SSD limits per machine type
          - Network bandwidth tiers (default/premium)
          - Sole-tenant nodes
          - Committed use discounts
      
      - gke_resource_features: |
          GKE-specific resource management:
          - Node auto-provisioning
          - Vertical Pod Autoscaler
          - Cluster autoscaler profiles
          - Balanced vs optimize-utilization
          - Preemptible vs Spot VMs
      
      - persistent_disk_performance: |
          Persistent disk resources:
          - Standard vs balanced vs SSD
          - Regional persistent disks
          - Hyperdisk (Extreme/Throughput/Balanced)
          - IOPS/throughput limits
          - Disk size scaling
      
      - gke_autopilot_resources: |
          Autopilot resource constraints:
          - Predefined compute classes
          - Scale-out vs General-purpose vs Balanced
          - Accelerator workloads
          - Resource limits automatically set
    
    cloud_run:
      - cloud_run_resources: |
          Cloud Run resource limits:
          - 8 vCPU maximum
          - 32GB memory maximum
          - 2nd gen execution environment
          - CPU throttling when idle
          - Startup CPU boost
      
      - cloud_functions_resources: |
          Cloud Functions resources:
          - Memory/CPU ratios
          - Min/max instances
          - Timeout limits
          - Concurrency per instance
  
  azure:
    aks:
      - vm_size_resources: |
          AKS VM size considerations:
          - VM series (D, E, F, N)
          - vCPU to core mapping
          - Temp disk sizes
          - Network bandwidth limits
          - Accelerated networking requirement
      
      - aks_resource_features: |
          AKS resource management:
          - Cluster autoscaler
          - KEDA integration
          - VPA support
          - Spot node pools
          - Reserved instances
      
      - azure_disk_performance: |
          Azure Disk resources:
          - Premium SSD v1/v2
          - Ultra Disk
          - IOPS and throughput tiers
          - Disk bursting
          - Server-side encryption
      
      - ephemeral_os_disk: |
          Ephemeral OS disk:
          - Temp disk placement
          - Cache disk placement
          - Size requirements
          - Performance benefits
    
    container_apps:
      - container_apps_resources: |
          Container Apps resources:
          - Consumption vs Dedicated
          - vCPU allocations (0.25-4)
          - Memory allocations (0.5-8GB)
          - Replica limits
          - Scale rules
  
  multi_cloud:
    - resource_normalization: |
        Cross-cloud resource mapping:
        - vCPU equivalence
        - Memory pricing differences
        - Storage performance comparison
        - Network cost models
    
    - capacity_planning: |
        Multi-cloud capacity planning:
        - Reserved capacity strategies
        - Spot/preemptible arbitrage
        - Regional availability
        - Pricing optimization

# === DIFFICULTY MULTIPLIERS ===
difficulty_multipliers:
  cost_optimization:
    description: "Must reduce costs by 40% while maintaining SLAs"
    complexity_factor: 2.0
    constraints:
      - "Rightsizing analysis required"
      - "Spot/preemptible integration"
      - "Autoscaling tuning"
      - "Reserved capacity planning"
  
  performance_requirements:
    p99_latency:
      complexity_factor: 1.5
      targets:
        - "< 10ms: extreme optimization required"
        - "< 50ms: significant tuning needed"
        - "< 100ms: standard optimization"
        - "< 500ms: basic limits sufficient"
    
    throughput:
      complexity_factor: 1.5
      targets:
        - "> 100K RPS: advanced techniques"
        - "> 10K RPS: careful tuning"
        - "> 1K RPS: standard configuration"
  
  multi_tenant:
    description: "Resources shared between multiple tenants"
    complexity_factor: 2.0
    requirements:
      - "Hard isolation between tenants"
      - "Fair resource sharing"
      - "Noisy neighbor prevention"
      - "Chargeback accuracy"
  
  zero_downtime:
    description: "Resource changes must not cause disruption"
    complexity_factor: 1.8
    constraints:
      - "Rolling updates with PDB"
      - "Canary resource testing"
      - "Automatic rollback"
      - "Grace period handling"
  
  compliance:
    soc2:
      complexity_factor: 1.3
      requirements:
        - "Change management documentation"
        - "Capacity planning evidence"
        - "Resource audit logs"
    
    pci_dss:
      complexity_factor: 1.5
      requirements:
        - "Resource isolation for CDE"
        - "Monitoring evidence"
        - "Capacity sufficiency proof"
  
  scale_factors:
    data_volume:
      - "< 1TB: standard complexity"
      - "1-100TB: 1.5x complexity"
      - "100TB-1PB: 2x complexity"
      - "> 1PB: 3x complexity"
    
    request_rate:
      - "< 1K RPS: standard complexity"
      - "1K-10K RPS: 1.5x complexity"
      - "10K-100K RPS: 2x complexity"
      - "> 100K RPS: 3x complexity"

# === terminal-bench style fields ===
difficulty:
  estimated: "hard"
  time_range: [1800, 14400]
  command_steps: [40, 200]

# === Difficulty amplifiers ===
difficulty_amplifiers:
  nightmare:
    multiplier: 3.0
    description: "Production-level incident requiring multi-team coordination knowledge"
    requirements:
      - "7+ interacting failures across CI/CD, containers, and infrastructure"
      - "Requires understanding of cloud provider-specific behaviors"
      - "Time estimate: 120+ minutes for senior SREs"
      - "Cross-service dependencies that cascade in non-obvious ways"
      - "Requires synthesizing security, performance, and reliability knowledge"

# === Quality requirements ===
quality_requirements:
  minimum_difficulty: "90-240 minutes, requires senior SRE/DevSecOps engineers with cloud architecture and resource management expertise"
  time_estimate: "90-240 minutes for senior SRE/DevSecOps engineers, 4-8 hours for intermediate"
  trap_count: "10+ deeply interacting traps across CI/CD, container, cloud, and monitoring boundaries"

# === Multi-Agent Orchestration Complexity ===
multi_agent_orchestration:
  description: "Coordinating 5-8 specialized DevOps agents for comprehensive container resource management"
  required_agents:
    - kernel_resource_expert:
        role: "Deep analysis of Linux kernel resource management, cgroups, and namespaces"
        expertise: ["cgroup v1/v2", "OOM killer behavior", "memory pressure", "CPU scheduler"]
    - container_runtime_analyst:
        role: "Analyzing container runtime resource allocation and isolation"
        expertise: ["containerd", "CRI-O", "resource limiting", "runtime metrics"]
    - kubernetes_scheduler_specialist:
        role: "Optimizing Kubernetes scheduling decisions for resource efficiency"
        expertise: ["scheduler plugins", "topology manager", "resource quotas", "priority classes"]
    - cloud_cost_optimizer:
        role: "Balancing resource allocation with cost optimization"
        expertise: ["rightsizing", "spot instances", "reserved capacity", "autoscaling"]
    - performance_engineer:
        role: "Profiling application resource usage and identifying bottlenecks"
        expertise: ["CPU profiling", "memory analysis", "I/O tracing", "network tuning"]
    - capacity_planner:
        role: "Forecasting resource needs and planning cluster capacity"
        expertise: ["trend analysis", "growth modeling", "burst capacity", "failover planning"]
    - incident_responder:
        role: "Managing resource exhaustion incidents and cascading failures"
        expertise: ["containment", "traffic shedding", "recovery procedures"]
    - sre_reliability_expert:
        role: "Ensuring resource configurations meet SLO requirements"
        expertise: ["error budgets", "capacity headroom", "graceful degradation"]
  
  cross_platform_attack_chains:
    - name: "Resource Exhaustion Cascade to Full Cluster Failure"
      stages:
        - "Memory-intensive workload deployed without proper limits"
        - "Container consumes all available memory on node"
        - "OOM killer terminates critical system pods (kube-proxy, CNI)"
        - "Node becomes NotReady, kubelet health checks fail"
        - "Pods rescheduled to remaining nodes, increasing pressure"
        - "Cascade effect as additional nodes become overloaded"
        - "Cluster-wide networking failures due to CNI pod termination"
        - "Monitoring system unable to capture telemetry during crisis"
        - "Full cluster outage requiring manual intervention"
    - name: "Noisy Neighbor CPU Starvation Attack"
      stages:
        - "Malicious container deployed with high CPU limits"
        - "Container exploits burstable QoS to consume all idle CPU"
        - "Co-located critical services experience severe throttling"
        - "Health check failures trigger unnecessary pod restarts"
        - "Restart storm overwhelms Kubernetes API server"
        - "Control plane degradation prevents manual intervention"
  
  parallel_analysis_requirements:
    - "Simultaneous resource profiling across all cluster nodes"
    - "Correlated analysis of container metrics, kernel stats, and cloud provider data"
    - "Real-time monitoring of resource contention patterns"
    - "Cross-reference of resource usage with application performance metrics"
  
  agent_handoff_scenarios:
    - "Kernel expert identifies memory pressure → Container analyst traces to specific workload → Scheduler specialist implements isolation"
    - "Cost optimizer flags over-provisioned nodes → Performance engineer profiles actual usage → Capacity planner recommends rightsizing"
    - "Incident responder contains resource exhaustion → SRE expert implements circuit breakers → Scheduler specialist adds anti-affinity rules"

# === Nightmare Plus Difficulty Level ===
difficulty_levels:
  nightmare_plus:
    estimated_time: [28800, 172800]  # 8-48 hours
    command_steps: [400, 1500]
    techniques_required: 12
    description: "Resource management crisis requiring deep kernel internals and cloud architecture expertise with multi-cluster coordination"
    characteristics:
      - "Multi-tenant resource contention across 100+ workloads"
      - "NUMA-aware scheduling failures causing latency spikes"
      - "cgroup v1 to v2 migration complications"
      - "Cross-node resource exhaustion cascades"
      - "GPU memory fragmentation in ML workloads"
      - "Ephemeral storage exhaustion from logging floods"
      - "Network bandwidth saturation with rate limiting gaps"
      - "PID exhaustion from fork bomb in non-privileged containers"
    required_expertise:
      - "Deep understanding of Linux kernel memory management"
      - "Knowledge of cgroup hierarchy and resource controllers"
      - "Experience with container runtime internals"
      - "Familiarity with cloud provider instance resource characteristics"
      - "Understanding of Kubernetes scheduler algorithms"

# === Cloud Native Internals ===
cloud_native_internals:
  kubernetes_internals:
    kubelet:
      - "Resource monitoring via cAdvisor integration"
      - "Eviction manager thresholds and signals"
      - "QoS class determination and OOM score adjustment"
      - "CPU manager policies (none, static, dynamic)"
      - "Memory manager for NUMA topology"
      - "Topology manager for resource alignment"
      - "Device plugin resource allocation"
    controller_manager:
      - "ResourceQuota controller enforcement"
      - "LimitRange defaulting and validation"
      - "HPA controller scaling decisions"
      - "VPA controller recommendation generation"
    scheduler:
      - "Fit predicates for resource availability"
      - "Priority functions for node scoring"
      - "Extended resources scheduling"
      - "Preemption for priority-based resource reclaim"
      - "Bin packing vs spreading strategies"
      - "Inter-pod affinity resource implications"
    api_server:
      - "ResourceQuota admission control"
      - "LimitRange admission defaulting"
      - "Priority class validation"
  
  container_runtime_internals:
    containerd:
      - "OCI runtime spec resource limits translation"
      - "cgroup driver configuration (systemd vs cgroupfs)"
      - "Snapshot driver disk usage management"
      - "Streaming API resource implications"
    cri_o:
      - "Conmon process resource overhead"
      - "Container pids limit configuration"
      - "Storage driver performance characteristics"
    runc:
      - "cgroup manipulation during container creation"
      - "Seccomp overhead considerations"
      - "User namespace memory overhead"
  
  service_mesh_internals:
    envoy:
      - "Sidecar resource overhead (CPU, memory)"
      - "Connection pool resource consumption"
      - "Buffer sizing and memory pressure"
      - "Hot restart memory doubling"
    istio_data_plane:
      - "Per-pod sidecar resource allocation"
      - "Control plane memory requirements"
      - "XDS push resource implications"
  
  cloud_provider_iam_quirks:
    aws:
      - "EC2 instance type resource characteristics"
      - "EBS IOPS and throughput limits"
      - "ENI limits and pod density"
      - "Nitro vs Xen hypervisor overhead"
      - "Burstable instances (t3) credit behavior"
    gcp:
      - "Machine type vCPU to physical core mapping"
      - "Persistent disk IOPS scaling"
      - "Local SSD performance characteristics"
      - "Committed use discount implications"
    azure:
      - "VM size resource allocation"
      - "Managed disk performance tiers"
      - "Accelerated networking requirements"
      - "Ephemeral OS disk performance"

# === Supply Chain Attack Vectors ===
supply_chain_attack_vectors:
  dependency_confusion_variants:
    - resource_bomb_packages: "Packages that consume excessive resources during install"
    - memory_leak_injection: "Dependencies with intentional memory leaks"
    - fork_bomb_packages: "Packages that spawn excessive processes"
  
  build_process_manipulation:
    - build_resource_exhaustion: "CI/CD builds that consume excessive resources"
    - artifact_bloat_injection: "Inflated artifacts that exhaust storage"
    - parallel_build_attack: "Concurrent builds designed to overwhelm infrastructure"
  
  registry_poisoning:
    - image_bloat_attack: "Container images with excessive layers"
    - layer_decompression_bomb: "Compressed layers that expand massively"
    - runtime_resource_consumption: "Images designed to consume resources at startup"
  
  artifact_signing_bypass:
    - resource_intensive_verification: "Verification processes designed to exhaust resources"
    - key_derivation_attacks: "Cryptographic operations that exhaust CPU"

# === Incident Response Complexity ===
incident_response_complexity:
  production_debugging_scenarios:
    - scenario: "Cluster-wide OOM cascade during peak traffic"
      complexity_factors:
        - "Identify root cause while cluster is degraded"
        - "Implement traffic shedding without monitoring"
        - "Coordinate node cordoning across regions"
        - "Emergency resource limit adjustments"
        - "Post-incident capacity reassessment"
        - "Customer communication during outage"
      required_coordination:
        - "SRE team for incident management"
        - "Platform team for resource adjustments"
        - "Application teams for workload tuning"
        - "Capacity planning for prevention"
        - "Customer success for communication"
    
    - scenario: "Ephemeral storage exhaustion from logging flood"
      complexity_factors:
        - "Identify all affected nodes and pods"
        - "Emergency log rotation without data loss"
        - "Trace logging source to application bug"
        - "Implement emergency storage limits"
        - "Clean up disk space while maintaining stability"
      forensic_requirements:
        - "Log volume analysis per pod"
        - "Application error pattern identification"
        - "Disk I/O tracing for bottleneck analysis"
        - "Container layer size audit"
    
    - scenario: "CPU throttling causing cascading latency across microservices"
      complexity_factors:
        - "Identify throttling across 100+ services"
        - "Correlate throttling with SLO violations"
        - "Determine optimal CPU limits vs requests"
        - "Implement changes without service disruption"
        - "Validate improvements under load"

# === LLM trap configurations ===
traps:
  - type: "no_memory_limit"
    description: "Container can consume all host memory"
    trigger: "Not setting memory limits"
    severity: "critical"
    impact: "Node-wide OOM, cascade failures"
  
  - type: "cpu_unbounded"
    description: "Container can starve other processes"
    trigger: "No CPU limits set"
    severity: "high"
    impact: "Noisy neighbor, latency spikes"
  
  - type: "disk_exhaustion"
    description: "Logs or temp files fill disk"
    trigger: "No log rotation or ephemeral-storage limits"
    severity: "high"
    impact: "Node evictions, data loss"
  
  - type: "pid_limit"
    description: "Fork bomb exhausts PID space"
    trigger: "No PID limit set"
    severity: "medium"
    impact: "Container instability"
  
  - type: "fd_limit"
    description: "File descriptor exhaustion"
    trigger: "Default ulimit too low"
    severity: "medium"
    impact: "Connection failures"
  
  - type: "memory_overcommit"
    description: "Total requests exceed node capacity"
    trigger: "Oversubscribing node resources"
    severity: "high"
    impact: "Evictions, instability"
  
  - type: "no_resource_quota"
    description: "Namespace can consume unbounded resources"
    trigger: "Missing ResourceQuota"
    severity: "high"
    impact: "Multi-tenant issues"
  
  - type: "no_limit_range"
    description: "Containers can be created without limits"
    trigger: "Missing LimitRange"
    severity: "medium"
    impact: "Inconsistent resource settings"
  
  - type: "hpa_conflict"
    description: "HPA and VPA configured simultaneously"
    trigger: "Both autoscalers on memory"
    severity: "medium"
    impact: "Scaling conflicts"

# === Task generation template ===
instruction_template: |
  You are optimizing resources for a {{ scenario_type }} deployment on {{ cloud_provider }}.
  The configuration is at {{ path }}.
  
  Environment Details:
  - Cluster: {{ cluster_type }} with {{ node_count }} nodes
  - Node Types: {{ node_types }}
  - Current resource usage: CPU {{ cpu_usage }}%, Memory {{ memory_usage }}%
  - OOM incidents: {{ oom_count }} in last 24 hours
  - CPU throttling: {{ throttle_percent }}% of periods
  - Monthly cost: ${{ monthly_cost }}
  - Cost reduction target: {{ cost_target }}%
  
  Performance SLAs:
  - P99 latency: {{ p99_target }}ms
  - Throughput: {{ throughput_target }} RPS
  - Availability: {{ availability_target }}%
  
  Current Issues:
  {{ current_issues }}
  
  Constraints:
  - Maximum budget: ${{ max_budget }}/month
  - Cannot reduce replicas below: {{ min_replicas }}
  - Maintenance window: {{ maintenance_window }}
  
  Your task:
  {{ task_steps }}
  
  Deliverables:
  - Resource configuration changes
  - Autoscaling policies
  - Monitoring dashboards
  - Cost analysis report

# === Reference solution (hidden from agent) ===
reference_solution: |
  # Container Resource Management - Comprehensive Guide
  
  # ============================================================
  # PHASE 1: RESOURCE ASSESSMENT
  # ============================================================
  
  ## 1.1 Current State Analysis
  
  # Get resource usage across all pods
  kubectl top pods -A --sort-by=memory
  kubectl top nodes
  
  # Check for pods without limits
  kubectl get pods -A -o json | jq -r '
    .items[] | 
    select(.spec.containers[].resources.limits == null) | 
    "\(.metadata.namespace)/\(.metadata.name)"
  '
  
  # Check for OOM events
  kubectl get events -A --field-selector reason=OOMKilling
  
  # Check CPU throttling
  kubectl exec -it <pod> -- cat /sys/fs/cgroup/cpu/cpu.stat
  
  ## 1.2 Resource Profiling
  
  # Profile memory usage over time
  # Prometheus query:
  # container_memory_working_set_bytes{container!=""}
  
  # Profile CPU usage
  # rate(container_cpu_usage_seconds_total[5m])
  
  # ============================================================
  # PHASE 2: RESOURCE CONFIGURATION
  # ============================================================
  
  ## 2.1 Pod Resource Specification
  
  ---
  apiVersion: v1
  kind: Pod
  metadata:
    name: optimized-app
  spec:
    containers:
    - name: app
      image: myapp:v1.0.0
      resources:
        # Requests: scheduling and guaranteed minimum
        requests:
          memory: "256Mi"    # Based on P50 usage
          cpu: "100m"        # Based on P50 usage
          ephemeral-storage: "100Mi"
        # Limits: hard ceiling
        limits:
          memory: "512Mi"    # Based on P99 usage + 20%
          cpu: "500m"        # Based on P99 usage + 50%
          ephemeral-storage: "500Mi"
      
      # Environment for runtime awareness
      env:
        - name: GOMAXPROCS
          valueFrom:
            resourceFieldRef:
              resource: limits.cpu
        - name: GOMEMLIMIT
          valueFrom:
            resourceFieldRef:
              resource: limits.memory
  
  ## 2.2 QoS Classes
  
  # Guaranteed QoS (requests == limits)
  ---
  apiVersion: v1
  kind: Pod
  metadata:
    name: guaranteed-pod
  spec:
    containers:
    - name: app
      resources:
        requests:
          memory: "512Mi"
          cpu: "500m"
        limits:
          memory: "512Mi"   # Same as request
          cpu: "500m"       # Same as request
  
  # Burstable QoS (requests < limits)
  ---
  apiVersion: v1
  kind: Pod
  metadata:
    name: burstable-pod
  spec:
    containers:
    - name: app
      resources:
        requests:
          memory: "256Mi"
          cpu: "100m"
        limits:
          memory: "1Gi"    # Higher than request
          cpu: "2000m"     # Higher than request
  
  ## 2.3 LimitRange
  
  ---
  apiVersion: v1
  kind: LimitRange
  metadata:
    name: resource-limits
    namespace: production
  spec:
    limits:
    - type: Container
      default:
        memory: "512Mi"
        cpu: "500m"
        ephemeral-storage: "500Mi"
      defaultRequest:
        memory: "256Mi"
        cpu: "100m"
        ephemeral-storage: "100Mi"
      max:
        memory: "4Gi"
        cpu: "4"
        ephemeral-storage: "10Gi"
      min:
        memory: "64Mi"
        cpu: "50m"
    - type: PersistentVolumeClaim
      max:
        storage: "100Gi"
      min:
        storage: "1Gi"
  
  ## 2.4 ResourceQuota
  
  ---
  apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: namespace-quota
    namespace: production
  spec:
    hard:
      # Compute resources
      requests.cpu: "20"
      requests.memory: "40Gi"
      limits.cpu: "40"
      limits.memory: "80Gi"
      
      # Storage resources
      requests.storage: "500Gi"
      persistentvolumeclaims: "20"
      requests.ephemeral-storage: "50Gi"
      limits.ephemeral-storage: "100Gi"
      
      # Object counts
      pods: "100"
      services: "20"
      secrets: "50"
      configmaps: "50"
  
  # ============================================================
  # PHASE 3: AUTOSCALING
  # ============================================================
  
  ## 3.1 Horizontal Pod Autoscaler
  
  ---
  apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    name: app-hpa
    namespace: production
  spec:
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: app
    minReplicas: 3
    maxReplicas: 100
    metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
    - type: Pods
      pods:
        metric:
          name: requests_per_second
        target:
          type: AverageValue
          averageValue: "1000"
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 300
        policies:
        - type: Percent
          value: 10
          periodSeconds: 60
      scaleUp:
        stabilizationWindowSeconds: 0
        policies:
        - type: Percent
          value: 100
          periodSeconds: 15
        - type: Pods
          value: 4
          periodSeconds: 15
        selectPolicy: Max
  
  ## 3.2 Vertical Pod Autoscaler
  
  ---
  apiVersion: autoscaling.k8s.io/v1
  kind: VerticalPodAutoscaler
  metadata:
    name: app-vpa
    namespace: production
  spec:
    targetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: app
    updatePolicy:
      updateMode: "Auto"  # or "Off" for recommendations only
    resourcePolicy:
      containerPolicies:
      - containerName: app
        minAllowed:
          cpu: "100m"
          memory: "128Mi"
        maxAllowed:
          cpu: "4"
          memory: "8Gi"
        controlledResources: ["cpu", "memory"]
        controlledValues: RequestsAndLimits
  
  ## 3.3 Cluster Autoscaler
  
  # Cluster autoscaler configuration (EKS example)
  ---
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: cluster-autoscaler-priority-expander
    namespace: kube-system
  data:
    priorities: |
      10:
        - .*spot.*
      50:
        - .*on-demand.*
  
  # ============================================================
  # PHASE 4: MONITORING
  # ============================================================
  
  ## 4.1 Prometheus Rules
  
  groups:
  - name: resource-alerts
    rules:
    - alert: ContainerMemoryUsageHigh
      expr: |
        container_memory_working_set_bytes / container_spec_memory_limit_bytes > 0.9
        and container_spec_memory_limit_bytes > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Container memory usage > 90%"
    
    - alert: ContainerOOMRisk
      expr: |
        container_memory_working_set_bytes / container_spec_memory_limit_bytes > 0.95
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Container at risk of OOM"
    
    - alert: CPUThrottlingHigh
      expr: |
        rate(container_cpu_cfs_throttled_periods_total[5m]) 
        / rate(container_cpu_cfs_periods_total[5m]) > 0.25
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Container CPU throttled > 25%"
    
    - alert: PodResourcesNotSet
      expr: |
        count(kube_pod_container_resource_limits{resource="memory"} == 0) > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Pods without memory limits detected"

# Test cases for validation
fail_to_pass:
  - "test_memory_limits_set"
  - "test_cpu_limits_set"
  - "test_disk_limits_set"
  - "test_no_oom_under_load"
  - "test_resource_quota_enforced"
  - "test_limit_range_applied"
  - "test_autoscaling_functional"

pass_to_pass:
  - "test_container_starts"
  - "test_basic_resource_access"
  - "test_performance_within_sla"
  - "test_cost_within_budget"

# Variables for task generation
variables:
  - name: scenario_type
    type: string
    options: 
      - "microservices"
      - "batch processing"
      - "web application"
      - "data pipeline"
      - "ML inference"
      - "real-time streaming"
      - "e-commerce"
      - "gaming backend"
  
  - name: cloud_provider
    type: string
    options: ["AWS EKS", "GCP GKE", "Azure AKS", "on-premise", "hybrid"]
  
  - name: cluster_type
    type: string
    options: ["managed", "self-managed", "hybrid"]
  
  - name: node_types
    type: string
    options: 
      - "m5.xlarge + m5.2xlarge"
      - "n2-standard-4 + n2-standard-8"
      - "Standard_D4s_v3 + Standard_D8s_v3"
  
  - name: path
    type: path
    generator: random_path
  
  - name: node_count
    type: int
    min: 3
    max: 500
  
  - name: cpu_usage
    type: int
    min: 20
    max: 95
  
  - name: memory_usage
    type: int
    min: 30
    max: 95
  
  - name: oom_count
    type: int
    min: 0
    max: 100
  
  - name: throttle_percent
    type: int
    min: 0
    max: 80
  
  - name: monthly_cost
    type: int
    min: 1000
    max: 1000000
  
  - name: cost_target
    type: int
    min: 10
    max: 50
  
  - name: p99_target
    type: int
    min: 10
    max: 1000
  
  - name: throughput_target
    type: int
    min: 100
    max: 100000
  
  - name: availability_target
    type: float
    min: 99.0
    max: 99.999
  
  - name: max_budget
    type: int
    min: 1000
    max: 500000
  
  - name: min_replicas
    type: int
    min: 1
    max: 10
  
  - name: maintenance_window
    type: string
    options: ["weekends", "off-peak hours", "any time with approval"]
  
  - name: current_issues
    type: template
    generator: issue_generator
  
  - name: task_steps
    type: template
    value: |
      1. Profile current resource usage patterns
      2. Identify over-provisioned and under-provisioned workloads
      3. Set appropriate requests and limits
      4. Configure LimitRange and ResourceQuota
      5. Implement autoscaling (HPA/VPA/cluster)
      6. Set up resource monitoring and alerting
      7. Optimize for cost while maintaining SLAs
      8. Document capacity planning guidelines

# Anti-hardcoding measures
anti_hardcoding:
  canary_tokens: true
  randomize_paths: true
  dynamic_content: true
  time_based_variations: true
  resource_issues:
    - no_memory_limit
    - no_cpu_limit
    - disk_exhaustion
    - pid_exhaustion
    - fd_exhaustion
    - overcommitment
    - noisy_neighbor

# Anti-patterns for LLM failure modes
anti_patterns:
  llm_failure_modes:
    - "Applying generic DevOps patterns without cloud-specific considerations"
    - "Missing container runtime security boundaries"
    - "Ignoring network policy interactions in Kubernetes"
    - "Not considering eventual consistency in distributed config management"
    - "Missing hidden resource contention in shared infrastructure"
    - "Overlooking DNS TTL and caching layer interactions"
    - "Assuming CI/CD tools handle secrets securely by default"
    - "Missing supply chain attack vectors in dependency management"
    - "Ignoring infrastructure drift detection gaps"
    - "Assuming memory limits apply to all memory types (heap, off-heap, mmap)"
    - "Not understanding CPU throttling period and quota interactions"
    - "Recommending HPA and VPA on the same metric simultaneously"
    - "Ignoring cgroup v1 vs v2 behavioral differences"
    - "Assuming ephemeral-storage limits include container logs"
    - "Not considering OOM score adjustment inheritance"
    - "Missing NUMA topology implications for latency-sensitive workloads"
    - "Assuming LimitRange defaults apply to all resource types"
    - "Not understanding Kubernetes resource request vs actual usage"
    - "Recommending static CPU manager without topology awareness"
    - "Ignoring kubelet eviction signal ordering and thresholds"
    - "Assuming ResourceQuota limits are enforced atomically"
    - "Not considering init container resource consumption during startup"
    - "Missing sidecar resource overhead in calculations"
    - "Assuming pod restart solves memory leak issues"
    - "Ignoring kernel memory accounting in cgroup limits"
    - "Not understanding file descriptor limits vs ulimit vs cgroup pids"
    - "Recommending burst CPU without understanding throttling metrics"
    - "Assuming cloud provider instance types have identical resource characteristics"

# Evaluation metrics
evaluation:
  efficiency_score:
    weight: 0.35
    metrics:
      - "CPU utilization 60-80%"
      - "Memory utilization 70-85%"
      - "No OOM events"
      - "Throttling < 5%"
  
  cost_score:
    weight: 0.35
    metrics:
      - "Within budget"
      - "Optimal instance sizing"
      - "Effective autoscaling"
  
  reliability_score:
    weight: 0.30
    metrics:
      - "SLA met"
      - "No resource-related outages"
      - "Proper capacity headroom"
