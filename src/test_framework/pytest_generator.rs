// Pytest test suite generator
// Generates Python pytest files for task verification

use std::collections::HashMap;

/// Expected output specification for a file or artifact
#[derive(Debug, Clone)]
pub struct ExpectedOutput {
    /// Path to the expected output file
    pub path: String,
    /// Expected file format (e.g., "json", "txt", "csv")
    pub format: String,
    /// Content validation rules
    pub content_checks: Vec<ContentCheck>,
    /// Whether the file must exist
    pub required: bool,
}

/// Content validation check
#[derive(Debug, Clone)]
pub struct ContentCheck {
    /// Type of check: "contains", "equals", "matches_regex", "json_path"
    pub check_type: String,
    /// Value to check for
    pub value: String,
    /// Description of what this check validates
    pub description: String,
}

/// Process validation configuration
#[derive(Debug, Clone)]
pub struct ProcessValidationConfig {
    /// Required commands that must be executed
    pub required_commands: Vec<String>,
    /// Files that must be created during process
    pub required_file_operations: Vec<String>,
    /// Time limit in seconds
    pub time_limit_seconds: u64,
    /// Memory limit in MB
    pub memory_limit_mb: u64,
}

impl Default for ProcessValidationConfig {
    fn default() -> Self {
        Self {
            required_commands: Vec::new(),
            required_file_operations: Vec::new(),
            time_limit_seconds: 300,
            memory_limit_mb: 1024,
        }
    }
}

/// Generates pytest test suites for task verification
pub struct PytestGenerator {
    task_id: String,
    category: String,
    subcategory: String,
    expected_outputs: HashMap<String, ExpectedOutput>,
    process_validation: ProcessValidationConfig,
    canary_id: String,
}

impl PytestGenerator {
    /// Create a new pytest generator
    pub fn new(
        task_id: String,
        category: String,
        subcategory: String,
        expected_outputs: HashMap<String, ExpectedOutput>,
        process_validation: ProcessValidationConfig,
        canary_id: String,
    ) -> Self {
        Self {
            task_id,
            category,
            subcategory,
            expected_outputs,
            process_validation,
            canary_id,
        }
    }

    /// Generate the complete pytest test file
    pub fn generate_test_file(&self) -> String {
        let mut lines = Vec::new();

        // File header
        lines.push(format!(
            r#""""
Auto-generated pytest test suite for task: {}
Category: {} / {}
Canary ID: {}

DO NOT MODIFY - This file is generated by swe_forge
""""#,
            self.task_id, self.category, self.subcategory, self.canary_id
        ));

        lines.push(String::new());

        // Imports
        lines.push("import json".to_string());
        lines.push("import os".to_string());
        lines.push("import re".to_string());
        lines.push("import time".to_string());
        lines.push("from pathlib import Path".to_string());
        lines.push("from typing import Any, Dict, List, Optional".to_string());
        lines.push(String::new());
        lines.push("import pytest".to_string());
        lines.push(String::new());

        // Constants
        lines.push(self.generate_constants());
        lines.push(String::new());

        // Output tests
        lines.push(self.generate_output_tests());
        lines.push(String::new());

        // State tests
        lines.push(self.generate_state_tests());
        lines.push(String::new());

        // Process tests
        lines.push(self.generate_process_tests());
        lines.push(String::new());

        // Reward calculator
        lines.push(self.generate_reward_calculator());

        lines.join("\n")
    }

    /// Generate conftest.py with fixtures
    pub fn generate_conftest(&self) -> String {
        format!(
            r#""""
Pytest configuration and fixtures for task: {}
""""

import json
import os
import time
from pathlib import Path
from typing import Any, Dict

import pytest


@pytest.fixture(scope="session")
def task_id() -> str:
    """Return the task ID for this test suite."""
    return "{}"


@pytest.fixture(scope="session")
def canary_id() -> str:
    """Return the canary ID for anti-hardcoding verification."""
    return "{}"


@pytest.fixture(scope="session")
def workspace_dir() -> Path:
    """Return the workspace directory path."""
    workspace = os.environ.get("DATAFORGE_WORKSPACE", "/workspace")
    return Path(workspace)


@pytest.fixture(scope="session")
def output_dir(workspace_dir: Path) -> Path:
    """Return the output directory path."""
    return workspace_dir / "output"


@pytest.fixture(scope="session")
def start_time() -> float:
    """Record the test session start time."""
    return time.time()


@pytest.fixture(scope="function")
def test_results() -> Dict[str, Any]:
    """Fixture to collect test results."""
    return {{
        "passed": [],
        "failed": [],
        "skipped": [],
    }}


def pytest_configure(config):
    """Configure pytest with custom markers."""
    config.addinivalue_line("markers", "output: marks tests as output verification")
    config.addinivalue_line("markers", "state: marks tests as state verification")
    config.addinivalue_line("markers", "process: marks tests as process verification")
    config.addinivalue_line("markers", "required: marks tests as required for pass")


def pytest_collection_modifyitems(config, items):
    """Modify test collection to add markers based on class names."""
    for item in items:
        if "OutputVerification" in item.nodeid:
            item.add_marker(pytest.mark.output)
        elif "StateVerification" in item.nodeid:
            item.add_marker(pytest.mark.state)
        elif "ProcessVerification" in item.nodeid:
            item.add_marker(pytest.mark.process)
"#,
            self.task_id, self.task_id, self.canary_id
        )
    }

    /// Generate constants section
    fn generate_constants(&self) -> String {
        let mut lines = Vec::new();

        lines.push("# Task Constants".to_string());
        lines.push(format!("TASK_ID = \"{}\"", self.task_id));
        lines.push(format!("CATEGORY = \"{}\"", self.category));
        lines.push(format!("SUBCATEGORY = \"{}\"", self.subcategory));
        lines.push(format!("CANARY_ID = \"{}\"", self.canary_id));
        lines.push(String::new());

        // Expected outputs dictionary
        lines.push("# Expected Outputs".to_string());
        lines.push("EXPECTED_OUTPUTS = {".to_string());

        for (name, output) in &self.expected_outputs {
            lines.push(format!("    \"{}\": {{", name));
            lines.push(format!("        \"path\": \"{}\",", output.path));
            lines.push(format!("        \"format\": \"{}\",", output.format));
            lines.push(format!("        \"required\": {},", output.required));
            lines.push("        \"content_checks\": [".to_string());

            for check in &output.content_checks {
                lines.push("            {".to_string());
                lines.push(format!(
                    "                \"check_type\": \"{}\",",
                    check.check_type
                ));
                lines.push(format!(
                    "                \"value\": \"{}\",",
                    Self::escape_python_string(&check.value)
                ));
                lines.push(format!(
                    "                \"description\": \"{}\",",
                    Self::escape_python_string(&check.description)
                ));
                lines.push("            },".to_string());
            }

            lines.push("        ],".to_string());
            lines.push("    },".to_string());
        }

        lines.push("}".to_string());
        lines.push(String::new());

        // Process validation config
        lines.push("# Process Validation".to_string());
        lines.push("PROCESS_VALIDATION = {".to_string());
        lines.push("    \"required_commands\": [".to_string());
        for cmd in &self.process_validation.required_commands {
            lines.push(format!("        \"{}\",", Self::escape_python_string(cmd)));
        }
        lines.push("    ],".to_string());
        lines.push("    \"required_file_operations\": [".to_string());
        for op in &self.process_validation.required_file_operations {
            lines.push(format!("        \"{}\",", Self::escape_python_string(op)));
        }
        lines.push("    ],".to_string());
        lines.push(format!(
            "    \"time_limit_seconds\": {},",
            self.process_validation.time_limit_seconds
        ));
        lines.push(format!(
            "    \"memory_limit_mb\": {},",
            self.process_validation.memory_limit_mb
        ));
        lines.push("}".to_string());

        lines.join("\n")
    }

    /// Generate output verification tests
    #[allow(clippy::vec_init_then_push)]
    fn generate_output_tests(&self) -> String {
        let mut lines = Vec::new();

        lines.push("class TestOutputVerification:".to_string());
        lines.push("    \"\"\"Tests for output file verification.\"\"\"".to_string());
        lines.push(String::new());

        // Test for file existence
        lines
            .push("    def test_required_files_exist(self, output_dir: Path) -> None:".to_string());
        lines.push("        \"\"\"Verify all required output files exist.\"\"\"".to_string());
        lines.push("        missing_files = []".to_string());
        lines.push("        for name, spec in EXPECTED_OUTPUTS.items():".to_string());
        lines.push("            if spec[\"required\"]:".to_string());
        lines.push("                file_path = output_dir / spec[\"path\"]".to_string());
        lines.push("                if not file_path.exists():".to_string());
        lines.push("                    missing_files.append(spec[\"path\"])".to_string());
        lines.push(
            "        assert not missing_files, f\"Missing required files: {missing_files}\""
                .to_string(),
        );
        lines.push(String::new());

        // Test for file content
        lines
            .push("    def test_output_content_valid(self, output_dir: Path) -> None:".to_string());
        lines.push(
            "        \"\"\"Verify output file contents match expectations.\"\"\"".to_string(),
        );
        lines.push("        errors = []".to_string());
        lines.push("        for name, spec in EXPECTED_OUTPUTS.items():".to_string());
        lines.push("            file_path = output_dir / spec[\"path\"]".to_string());
        lines.push("            if not file_path.exists():".to_string());
        lines.push("                if spec[\"required\"]:".to_string());
        lines.push(
            "                    errors.append(f\"Required file missing: {spec['path']}\")"
                .to_string(),
        );
        lines.push("                continue".to_string());
        lines.push(String::new());
        lines.push("            content = file_path.read_text()".to_string());
        lines.push("            for check in spec[\"content_checks\"]:".to_string());
        lines.push(
            "                if not self._validate_content(content, check, spec[\"format\"]):"
                .to_string(),
        );
        lines.push(
            "                    errors.append(f\"{name}: {check['description']}\")".to_string(),
        );
        lines.push(
            "        assert not errors, f\"Content validation errors: {errors}\"".to_string(),
        );
        lines.push(String::new());

        // Test for format validity
        lines.push("    def test_output_format_valid(self, output_dir: Path) -> None:".to_string());
        lines.push("        \"\"\"Verify output files have correct format.\"\"\"".to_string());
        lines.push("        errors = []".to_string());
        lines.push("        for name, spec in EXPECTED_OUTPUTS.items():".to_string());
        lines.push("            file_path = output_dir / spec[\"path\"]".to_string());
        lines.push("            if not file_path.exists():".to_string());
        lines.push("                continue".to_string());
        lines.push(String::new());
        lines.push("            if spec[\"format\"] == \"json\":".to_string());
        lines.push("                try:".to_string());
        lines.push("                    json.loads(file_path.read_text())".to_string());
        lines.push("                except json.JSONDecodeError as e:".to_string());
        lines
            .push("                    errors.append(f\"{name}: Invalid JSON - {e}\")".to_string());
        lines
            .push("        assert not errors, f\"Format validation errors: {errors}\"".to_string());
        lines.push(String::new());

        // Test for canary ID presence (anti-hardcoding)
        lines.push("    def test_canary_id_present(self, output_dir: Path) -> None:".to_string());
        lines.push(
            "        \"\"\"Verify canary ID is correctly embedded in outputs.\"\"\"".to_string(),
        );
        lines.push("        found_canary = False".to_string());
        lines.push("        for name, spec in EXPECTED_OUTPUTS.items():".to_string());
        lines.push("            file_path = output_dir / spec[\"path\"]".to_string());
        lines.push("            if file_path.exists():".to_string());
        lines.push("                content = file_path.read_text()".to_string());
        lines.push("                if CANARY_ID in content:".to_string());
        lines.push("                    found_canary = True".to_string());
        lines.push("                    break".to_string());
        lines.push(
            "        assert found_canary, f\"Canary ID {CANARY_ID} not found in any output\""
                .to_string(),
        );
        lines.push(String::new());

        // Helper method
        lines.push("    def _validate_content(".to_string());
        lines.push("        self,".to_string());
        lines.push("        content: str,".to_string());
        lines.push("        check: Dict[str, str],".to_string());
        lines.push("        file_format: str,".to_string());
        lines.push("    ) -> bool:".to_string());
        lines.push(
            "        \"\"\"Validate content against a check specification.\"\"\"".to_string(),
        );
        lines.push("        check_type = check[\"check_type\"]".to_string());
        lines.push("        value = check[\"value\"]".to_string());
        lines.push(String::new());
        lines.push("        if check_type == \"contains\":".to_string());
        lines.push("            return value in content".to_string());
        lines.push("        elif check_type == \"equals\":".to_string());
        lines.push("            return content.strip() == value.strip()".to_string());
        lines.push("        elif check_type == \"matches_regex\":".to_string());
        lines.push("            return bool(re.search(value, content))".to_string());
        lines.push("        elif check_type == \"json_path\":".to_string());
        lines.push("            if file_format != \"json\":".to_string());
        lines.push("                return False".to_string());
        lines.push("            try:".to_string());
        lines.push("                data = json.loads(content)".to_string());
        lines.push("                return self._check_json_path(data, value)".to_string());
        lines.push("            except json.JSONDecodeError:".to_string());
        lines.push("                return False".to_string());
        lines.push("        return False".to_string());
        lines.push(String::new());

        lines.push("    def _check_json_path(self, data: Any, path: str) -> bool:".to_string());
        lines.push("        \"\"\"Check if a JSON path exists and has a value.\"\"\"".to_string());
        lines.push("        parts = path.split(\".\")".to_string());
        lines.push("        current = data".to_string());
        lines.push("        for part in parts:".to_string());
        lines.push("            if isinstance(current, dict) and part in current:".to_string());
        lines.push("                current = current[part]".to_string());
        lines.push("            elif isinstance(current, list):".to_string());
        lines.push("                try:".to_string());
        lines.push("                    idx = int(part)".to_string());
        lines.push("                    current = current[idx]".to_string());
        lines.push("                except (ValueError, IndexError):".to_string());
        lines.push("                    return False".to_string());
        lines.push("            else:".to_string());
        lines.push("                return False".to_string());
        lines.push("        return current is not None".to_string());

        lines.join("\n")
    }

    /// Generate state verification tests
    #[allow(clippy::vec_init_then_push)]
    fn generate_state_tests(&self) -> String {
        let mut lines = Vec::new();

        lines.push("class TestStateVerification:".to_string());
        lines.push("    \"\"\"Tests for workspace state verification.\"\"\"".to_string());
        lines.push(String::new());

        // Test workspace structure
        lines.push(
            "    def test_workspace_structure(self, workspace_dir: Path) -> None:".to_string(),
        );
        lines.push("        \"\"\"Verify the workspace has expected structure.\"\"\"".to_string());
        lines.push(
            "        assert workspace_dir.exists(), f\"Workspace does not exist: {workspace_dir}\""
                .to_string(),
        );
        lines.push("        output_dir = workspace_dir / \"output\"".to_string());
        lines.push(
            "        assert output_dir.exists(), \"Output directory does not exist\"".to_string(),
        );
        lines.push(String::new());

        // Test no unexpected modifications
        lines.push(
            "    def test_no_unexpected_modifications(self, workspace_dir: Path) -> None:"
                .to_string(),
        );
        lines.push(
            "        \"\"\"Verify no unexpected files were modified outside allowed areas.\"\"\""
                .to_string(),
        );
        lines.push("        allowed_dirs = [\"output\", \"temp\", \".cache\"]".to_string());
        lines.push("        violations = []".to_string());
        lines.push(String::new());
        lines.push("        for item in workspace_dir.iterdir():".to_string());
        lines.push("            if item.is_file() and item.name.startswith(\".\"):".to_string());
        lines.push("                continue  # Allow hidden config files".to_string());
        lines.push("            if item.is_dir() and item.name not in allowed_dirs:".to_string());
        lines.push("                modified_files = list(item.rglob(\"*\"))".to_string());
        lines.push("                if modified_files:".to_string());
        lines.push(
            "                    violations.extend([str(f) for f in modified_files[:5]])"
                .to_string(),
        );
        lines.push(String::new());
        lines.push(
            "        # Note: This is a soft check - violations are logged but may not fail"
                .to_string(),
        );
        lines.push("        if violations:".to_string());
        lines.push(
            "            pytest.skip(f\"Potential unexpected modifications: {violations[:5]}\")"
                .to_string(),
        );
        lines.push(String::new());

        // Test environment state
        lines.push("    def test_environment_state(self) -> None:".to_string());
        lines
            .push("        \"\"\"Verify required environment variables are set.\"\"\"".to_string());
        lines.push("        required_vars = [\"DATAFORGE_WORKSPACE\"]".to_string());
        lines.push(
            "        missing = [var for var in required_vars if not os.environ.get(var)]"
                .to_string(),
        );
        lines.push("        if missing:".to_string());
        lines.push(
            "            pytest.skip(f\"Optional environment variables not set: {missing}\")"
                .to_string(),
        );

        lines.join("\n")
    }

    /// Generate process verification tests
    #[allow(clippy::vec_init_then_push)]
    fn generate_process_tests(&self) -> String {
        let mut lines = Vec::new();

        lines.push("class TestProcessVerification:".to_string());
        lines.push("    \"\"\"Tests for process execution verification.\"\"\"".to_string());
        lines.push(String::new());

        // Test execution time
        lines.push("    def test_execution_time(self, start_time: float) -> None:".to_string());
        lines.push("        \"\"\"Verify execution completed within time limit.\"\"\"".to_string());
        lines.push("        elapsed = time.time() - start_time".to_string());
        lines.push("        limit = PROCESS_VALIDATION[\"time_limit_seconds\"]".to_string());
        lines.push("        assert elapsed <= limit, f\"Execution time {elapsed:.1f}s exceeded limit {limit}s\"".to_string());
        lines.push(String::new());

        // Test required commands
        lines.push("    def test_required_commands_executed(".to_string());
        lines.push("        self,".to_string());
        lines.push("        workspace_dir: Path,".to_string());
        lines.push("    ) -> None:".to_string());
        lines.push("        \"\"\"Verify required commands were executed.\"\"\"".to_string());
        lines.push("        required = PROCESS_VALIDATION[\"required_commands\"]".to_string());
        lines.push("        if not required:".to_string());
        lines.push("            pytest.skip(\"No required commands specified\")".to_string());
        lines.push(String::new());
        lines.push("        # Check command history if available".to_string());
        lines.push("        history_file = workspace_dir / \".command_history\"".to_string());
        lines.push("        if not history_file.exists():".to_string());
        lines.push("            pytest.skip(\"Command history not available\")".to_string());
        lines.push(String::new());
        lines.push("        history = history_file.read_text()".to_string());
        lines.push("        missing_commands = []".to_string());
        lines.push("        for cmd in required:".to_string());
        lines.push("            if cmd not in history:".to_string());
        lines.push("                missing_commands.append(cmd)".to_string());
        lines.push(String::new());
        lines.push("        assert not missing_commands, f\"Required commands not executed: {missing_commands}\"".to_string());
        lines.push(String::new());

        // Test file operations
        lines.push("    def test_required_file_operations(".to_string());
        lines.push("        self,".to_string());
        lines.push("        workspace_dir: Path,".to_string());
        lines.push("    ) -> None:".to_string());
        lines.push(
            "        \"\"\"Verify required file operations were performed.\"\"\"".to_string(),
        );
        lines.push(
            "        required = PROCESS_VALIDATION[\"required_file_operations\"]".to_string(),
        );
        lines.push("        if not required:".to_string());
        lines
            .push("            pytest.skip(\"No required file operations specified\")".to_string());
        lines.push(String::new());
        lines.push("        missing_operations = []".to_string());
        lines.push("        for op in required:".to_string());
        lines.push(
            "            # Check if the file exists as evidence of the operation".to_string(),
        );
        lines.push("            file_path = workspace_dir / op".to_string());
        lines.push("            if not file_path.exists():".to_string());
        lines.push("                missing_operations.append(op)".to_string());
        lines.push(String::new());
        lines.push("        assert not missing_operations, f\"Required file operations not completed: {missing_operations}\"".to_string());

        lines.join("\n")
    }

    /// Generate reward calculation function
    #[allow(clippy::vec_init_then_push)]
    fn generate_reward_calculator(&self) -> String {
        let mut lines = Vec::new();

        lines.push("def calculate_reward(".to_string());
        lines.push("    output_passed: int,".to_string());
        lines.push("    output_total: int,".to_string());
        lines.push("    state_passed: int,".to_string());
        lines.push("    state_total: int,".to_string());
        lines.push("    process_passed: int,".to_string());
        lines.push("    process_total: int,".to_string());
        lines.push("    execution_time: float,".to_string());
        lines.push(") -> Dict[str, Any]:".to_string());
        lines.push("    \"\"\"".to_string());
        lines.push("    Calculate the reward score for the task completion.".to_string());
        lines.push(String::new());
        lines.push("    Returns a dictionary with:".to_string());
        lines.push("        - score: Total score achieved".to_string());
        lines.push("        - max_score: Maximum possible score".to_string());
        lines.push("        - percentage: Score as percentage".to_string());
        lines.push("        - passed: Whether the task passed minimum threshold".to_string());
        lines.push("        - breakdown: Detailed score breakdown by category".to_string());
        lines.push("    \"\"\"".to_string());
        lines.push("    # Weights for each verification category".to_string());
        lines.push("    OUTPUT_WEIGHT = 0.5".to_string());
        lines.push("    STATE_WEIGHT = 0.25".to_string());
        lines.push("    PROCESS_WEIGHT = 0.25".to_string());
        lines.push(String::new());
        lines.push("    # Calculate category scores".to_string());
        lines.push("    output_score = (output_passed / output_total * OUTPUT_WEIGHT) if output_total > 0 else 0".to_string());
        lines.push("    state_score = (state_passed / state_total * STATE_WEIGHT) if state_total > 0 else 0".to_string());
        lines.push("    process_score = (process_passed / process_total * PROCESS_WEIGHT) if process_total > 0 else 0".to_string());
        lines.push(String::new());
        lines.push("    total_score = output_score + state_score + process_score".to_string());
        lines.push("    max_score = OUTPUT_WEIGHT + STATE_WEIGHT + PROCESS_WEIGHT".to_string());
        lines.push(
            "    percentage = (total_score / max_score) * 100 if max_score > 0 else 0".to_string(),
        );
        lines.push(String::new());
        lines.push("    # Passing threshold is 70%".to_string());
        lines.push("    PASS_THRESHOLD = 70.0".to_string());
        lines.push("    passed = percentage >= PASS_THRESHOLD".to_string());
        lines.push(String::new());
        lines.push("    return {".to_string());
        lines.push("        \"task_id\": TASK_ID,".to_string());
        lines.push("        \"canary_id\": CANARY_ID,".to_string());
        lines.push("        \"score\": round(total_score, 4),".to_string());
        lines.push("        \"max_score\": round(max_score, 4),".to_string());
        lines.push("        \"percentage\": round(percentage, 2),".to_string());
        lines.push("        \"passed\": passed,".to_string());
        lines.push("        \"execution_time_seconds\": round(execution_time, 2),".to_string());
        lines.push("        \"breakdown\": {".to_string());
        lines.push("            \"output_verification\": {".to_string());
        lines.push("                \"score\": round(output_score, 4),".to_string());
        lines.push("                \"max_score\": OUTPUT_WEIGHT,".to_string());
        lines.push("                \"tests_passed\": output_passed,".to_string());
        lines.push("                \"tests_total\": output_total,".to_string());
        lines.push("            },".to_string());
        lines.push("            \"state_verification\": {".to_string());
        lines.push("                \"score\": round(state_score, 4),".to_string());
        lines.push("                \"max_score\": STATE_WEIGHT,".to_string());
        lines.push("                \"tests_passed\": state_passed,".to_string());
        lines.push("                \"tests_total\": state_total,".to_string());
        lines.push("            },".to_string());
        lines.push("            \"process_verification\": {".to_string());
        lines.push("                \"score\": round(process_score, 4),".to_string());
        lines.push("                \"max_score\": PROCESS_WEIGHT,".to_string());
        lines.push("                \"tests_passed\": process_passed,".to_string());
        lines.push("                \"tests_total\": process_total,".to_string());
        lines.push("            },".to_string());
        lines.push("        },".to_string());
        lines.push("    }".to_string());

        lines.join("\n")
    }

    /// Escape a string for use in Python code
    fn escape_python_string(s: &str) -> String {
        s.replace('\\', "\\\\")
            .replace('"', "\\\"")
            .replace('\n', "\\n")
            .replace('\r', "\\r")
            .replace('\t', "\\t")
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_pytest_generator_creates_valid_output() {
        let mut expected_outputs = HashMap::new();
        expected_outputs.insert(
            "result".to_string(),
            ExpectedOutput {
                path: "result.json".to_string(),
                format: "json".to_string(),
                content_checks: vec![ContentCheck {
                    check_type: "json_path".to_string(),
                    value: "status".to_string(),
                    description: "Status field exists".to_string(),
                }],
                required: true,
            },
        );

        let generator = PytestGenerator::new(
            "test-task-001".to_string(),
            "code_generation".to_string(),
            "python".to_string(),
            expected_outputs,
            ProcessValidationConfig::default(),
            "canary-abc123".to_string(),
        );

        let test_file = generator.generate_test_file();
        assert!(test_file.contains("import pytest"));
        assert!(test_file.contains("TASK_ID = \"test-task-001\""));
        assert!(test_file.contains("class TestOutputVerification"));
    }

    #[test]
    fn test_conftest_generation() {
        let generator = PytestGenerator::new(
            "test-task-002".to_string(),
            "debugging".to_string(),
            "rust".to_string(),
            HashMap::new(),
            ProcessValidationConfig::default(),
            "canary-xyz789".to_string(),
        );

        let conftest = generator.generate_conftest();
        assert!(conftest.contains("@pytest.fixture"));
        assert!(conftest.contains("canary-xyz789"));
    }

    #[test]
    fn test_escape_python_string() {
        assert_eq!(
            PytestGenerator::escape_python_string("hello\nworld"),
            "hello\\nworld"
        );
        assert_eq!(
            PytestGenerator::escape_python_string("say \"hello\""),
            "say \\\"hello\\\""
        );
    }
}
