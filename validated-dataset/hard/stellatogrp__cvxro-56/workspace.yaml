id: stellatogrp/cvxro-56
repo: stellatogrp/cvxro
base_commit: a42aecafc084c1e2fb7a836ee27aac0afa927d4c
merge_commit: 4547285aeefd2c8f460e2fd05618ab79ca05b9bf
language: python
difficulty_score: 3
created_at: 2026-02-17T11:22:51.608101601Z
patch: "diff --git a/cvxro/torch_expression_generator.py b/cvxro/torch_expression_generator.py\nindex 95dcde6..73c89a6 100644\n--- a/cvxro/torch_expression_generator.py\n+++ b/cvxro/torch_expression_generator.py\n@@ -160,7 +160,7 @@ def generate_torch_expressions(problem, eval_exp: Expression | None = None):\n             constraints = problem.ordered_uncertain_no_max_constraints[max_id]\n         else:\n             # Create a constraint from all the constraints of this max_id\n-            args = [cp.reshape(constraint.args[0],(1,)) for \\\n+            args = [cp.reshape(constraint.args[0], (1,), order=\"F\") for\n                     constraint in problem.constraints_by_type[max_id]]\n             constraints = [cp.NonNeg(-cp.maximum(*args))]\n         for constraint in constraints:  # NOT problem.constraints: these are the new constraints\ndiff --git a/cvxro/uncertain_canon/utils.py b/cvxro/uncertain_canon/utils.py\nindex 6fec83e..bf66529 100644\n--- a/cvxro/uncertain_canon/utils.py\n+++ b/cvxro/uncertain_canon/utils.py\n@@ -6,7 +6,7 @@\n from cvxpy.expressions.expression import Expression\n from cvxpy.reductions.inverse_data import InverseData\n from cvxpy.reductions.solution import Solution\n-from scipy.sparse import csc_matrix, lil_matrix\n+from scipy.sparse import csc_matrix\n from scipy.sparse._coo import coo_matrix\n \n UNCERTAIN_NO_MAX_ID = -1 #Use this ID for all uncertain constraints without max\n@@ -25,31 +25,25 @@ def standard_invert(solution: Solution, inverse_data: InverseData) -> Solution:\n \n     return Solution(solution.status, solution.opt_val, pvars, dvars, solution.attr)\n \n-def reshape_tensor(T_Ab: coo_matrix, n_var: int) -> np.ndarray:\n+def reshape_tensor(T_Ab: coo_matrix, n_var: int) -> csc_matrix:\n     \"\"\"\n     This function reshapes T_Ab so T_Ab@param_vec gives the constraints row by row instead of\n-    column by column. At the moment, it returns a dense matrix instead of a sparse one.\n-    \"\"\"\n-    def _calc_source_row(target_row: int, num_constraints: int, n_var: int) -> int:\n-        \"\"\"\n-        This is a helper function that calculates the index of the source row of T_Ab for the\n-        reshaped target row.\n-        \"\"\"\n-        constraint_num = 0 if n_var == 0 else target_row%n_var\n-        var_num = target_row if n_var == 0 else target_row//n_var\n-        source_row = constraint_num*num_constraints+var_num\n-        return source_row\n-\n+    column by column.\n \n+    Uses a permutation-index approach: builds a permutation array and uses sparse\n+    advanced indexing to reorder all rows at once, avoiding a Python-level row-by-row loop.\n+    \"\"\"\n     T_Ab = csc_matrix(T_Ab)\n-    n_var_full = n_var+1 #Includes the free paramter\n+    n_var_full = n_var + 1  # Includes the free parameter\n     num_rows = T_Ab.shape[0]\n-    num_constraints = num_rows//n_var_full\n-    T_Ab_res = lil_matrix(T_Ab.shape) #TODO: This changes csc to csr, might be inefficient\n-    for target_row in range(num_rows): #Counter for populating the new row of T_Ab_res\n-        source_row = _calc_source_row(target_row, num_constraints, n_var_full)\n-        T_Ab_res[target_row, :] = T_Ab[source_row, :]\n-    return T_Ab_res\n+    num_constraints = num_rows // n_var_full\n+\n+    target_rows = np.arange(num_rows)\n+    constraint_nums = target_rows % n_var_full\n+    var_nums = target_rows // n_var_full\n+    perm = constraint_nums * num_constraints + var_nums\n+\n+    return T_Ab[perm, :]\n \n def calc_num_constraints(constraints: list[Constraint]) -> int:\n     \"\"\"\ndiff --git a/profiling/README.md b/profiling/README.md\nnew file mode 100644\nindex 0000000..76f219e\n--- /dev/null\n+++ b/profiling/README.md\n@@ -0,0 +1,86 @@\n+# CVXRO Profiling Suite\n+\n+Profiling scripts for the CVXRO core (non-training) pipeline. These scripts identify where time and memory are spent in the canonicalization and solve pipeline.\n+\n+## Quick Start\n+\n+```bash\n+# Run all profiling scripts\n+bash profiling/run_profiling.sh\n+\n+# Run a specific script\n+bash profiling/run_profiling.sh hotspots\n+bash profiling/run_profiling.sh scaling\n+bash profiling/run_profiling.sh memory\n+bash profiling/run_profiling.sh canonicalization\n+\n+# Or run individually\n+uv run python profiling/profile_hotspots.py\n+uv run python profiling/profile_scaling.py\n+uv run python profiling/profile_memory.py\n+uv run python profiling/profile_canonicalization.py\n+```\n+\n+## Scripts\n+\n+### `profile_canonicalization.py` — Function-level cProfile\n+\n+Uses `cProfile` to get a top-down view of where time is spent across the full `solve()` pipeline. Tests 7 problem configurations:\n+\n+- Small/medium/large Ellipsoidal problems (n=5/50/200)\n+- Multi-uncertainty (Ellipsoidal + Box)\n+- Different set types (Box, Budget, Polyhedral)\n+\n+Outputs sorted cumulative/tottime tables. Saves `.prof` files in `results/` for further analysis with `pstats` or `snakeviz`.\n+\n+### `profile_memory.py` — Memory Profiling\n+\n+Uses `tracemalloc` to measure per-stage memory allocations:\n+\n+- Snapshots before/after: canonicalization, torch expression generation, uncertainty removal, solve\n+- Top memory allocators by file:line\n+- Compares across problem sizes (n=5, 50, 200)\n+\n+### `profile_scaling.py` — Scaling Analysis\n+\n+Measures how canonicalization time scales with:\n+\n+- Problem dimension `n` (5, 10, 20, 50, 100, 200)\n+- Number of constraints `m` (2, 5, 10, 20, 50)\n+- Number of uncertain parameters (1, 2, 3)\n+\n+Outputs scaling ratio tables. Generates matplotlib plots if available.\n+\n+### `profile_hotspots.py` — Targeted Micro-benchmarks\n+\n+Isolates and benchmarks specific suspected bottlenecks:\n+\n+1. **`reshape_tensor()`**: Benchmarks the optimized permutation-index approach against the old lil_matrix row-by-row loop (kept as a reference baseline)\n+2. **`get_problem_data()`**: Measures CVXPY internal canonicalization cost as a fraction of total\n+3. **`generate_torch_expressions()`**: Quantifies the overhead and potential speedup from deferring this call\n+4. **Variable creation**: Cost of creating `cp.Variable(shape)` in nested loops\n+5. **`has_unc_param()`**: Parameter iteration cost on repeated calls\n+6. **Full solve breakdown**: Time per stage as percentage of total `solve()`\n+\n+### `run_profiling.sh` — Runner Script\n+\n+Runs all profiling scripts in sequence (hotspots first as the quickest, then scaling, memory, and cProfile) and saves consolidated output to `results/`.\n+\n+## Output\n+\n+Results are saved to `profiling/results/`:\n+- `.prof` files — loadable with `pstats` or `snakeviz`\n+- `.log` files — full console output from each run\n+- `.png` files — scaling plots (if matplotlib available)\n+\n+## Interpreting Results\n+\n+- **Cumulative time**: Total time spent in a function including all sub-calls\n+- **Total time**: Time spent in a function excluding sub-calls\n+- **Scaling ratios**: How much slower a larger problem is compared to the previous size\n+\n+Key things to look for:\n+- What fraction of `solve()` is CVXRO overhead vs CVXPY solver?\n+- How much time does `generate_torch_expressions()` add (it's not needed for solve)?\n+- Is `reshape_tensor()` still fast after future changes? (Was a major bottleneck before optimization.)\n+- Where are the memory hotspots?\ndiff --git a/profiling/profile_canonicalization.py b/profiling/profile_canonicalization.py\nnew file mode 100644\nindex 0000000..64465b7\n--- /dev/null\n+++ b/profiling/profile_canonicalization.py\n@@ -0,0 +1,150 @@\n+\"\"\"\n+Function-level cProfile of the CVXRO solve() pipeline.\n+\n+Tests multiple problem configurations and outputs sorted timing tables.\n+Saves .prof files for further analysis with pstats/snakeviz.\n+\"\"\"\n+\n+import cProfile\n+import os\n+import pstats\n+import sys\n+import time\n+\n+import cvxpy as cp\n+import numpy as np\n+\n+import cvxro\n+\n+OUTPUT_DIR = os.path.join(os.path.dirname(__file__), \"results\")\n+\n+\n+def make_problem_simple(n, num_constraints, uncertainty_set):\n+    \"\"\"Create a simple robust optimization problem.\"\"\"\n+    x = cp.Variable(n)\n+    u = cvxro.UncertainParameter(n, uncertainty_set=uncertainty_set)\n+\n+    objective = cp.Minimize(cp.sum(x))\n+    constraints = [x >= -10, x <= 10]\n+    for i in range(num_constraints):\n+        coeff = np.random.randn(n)\n+        constraints.append(coeff @ x + u @ np.ones(n) <= 5 + i)\n+\n+    return cvxro.RobustProblem(objective, constraints)\n+\n+\n+def make_problem_multi_unc(n, num_constraints, sets):\n+    \"\"\"Create a problem with multiple uncertain parameters.\"\"\"\n+    x = cp.Variable(n)\n+    u_params = [cvxro.UncertainParameter(n, uncertainty_set=s) for s in sets]\n+\n+    objective = cp.Minimize(cp.sum(x))\n+    constraints = [x >= -10, x <= 10]\n+    for i in range(num_constraints):\n+        coeff = np.random.randn(n)\n+        expr = coeff @ x\n+        for u in u_params:\n+            expr = expr + u @ np.ones(n)\n+        constraints.append(expr <= 5 + i)\n+\n+    return cvxro.RobustProblem(objective, constraints)\n+\n+\n+def profile_config(name, problem_factory, num_runs=3):\n+    \"\"\"Profile a problem configuration and print results.\"\"\"\n+    print(f\"\\n{'='*70}\")\n+    print(f\"Configuration: {name}\")\n+    print(f\"{'='*70}\")\n+\n+    # Warmup run (first solve triggers lazy imports and solver initialization)\n+    prob = problem_factory()\n+    prob.solve(solver=\"CLARABEL\")\n+    del prob\n+\n+    # Timed runs\n+    times = []\n+    for i in range(num_runs):\n+        prob = problem_factory()\n+        start = time.perf_counter()\n+        prob.solve(solver=\"CLARABEL\")\n+        elapsed = time.perf_counter() - start\n+        times.append(elapsed)\n+        del prob\n+\n+    print(f\"Wall time: {np.mean(times):.4f}s +/- {np.std(times):.4f}s \"\n+          f\"(min={min(times):.4f}s, max={max(times):.4f}s)\")\n+\n+    # cProfile run\n+    prob = problem_factory()\n+    profiler = cProfile.Profile()\n+    profiler.enable()\n+    prob.solve(solver=\"CLARABEL\")\n+    profiler.disable()\n+\n+    # Print top functions by cumulative time\n+    print(\"\\n--- Top 30 by cumulative time ---\")\n+    stats = pstats.Stats(profiler, stream=sys.stdout)\n+    stats.strip_dirs()\n+    stats.sort_stats(\"cumulative\")\n+    stats.print_stats(30)\n+\n+    print(\"\\n--- Top 30 by total time ---\")\n+    stats.sort_stats(\"tottime\")\n+    stats.print_stats(30)\n+\n+    # Save .prof file\n+    os.makedirs(OUTPUT_DIR, exist_ok=True)\n+    safe_name = name.replace(\" \", \"_\").replace(\",\", \"\").replace(\"=\", \"\")\n+    prof_path = os.path.join(OUTPUT_DIR, f\"{safe_name}.prof\")\n+    profiler.dump_stats(prof_path)\n+    print(f\"Saved profile to {prof_path}\")\n+\n+    return times\n+\n+\n+def main():\n+    np.random.seed(42)\n+\n+    configs = [\n+        (\"small_n5_m3_ellipsoidal\", lambda: make_problem_simple(\n+            n=5, num_constraints=3, uncertainty_set=cvxro.Ellipsoidal(rho=2.0))),\n+\n+        (\"medium_n50_m10_ellipsoidal\", lambda: make_problem_simple(\n+            n=50, num_constraints=10, uncertainty_set=cvxro.Ellipsoidal(rho=2.0))),\n+\n+        (\"large_n200_m20_ellipsoidal\", lambda: make_problem_simple(\n+            n=200, num_constraints=20, uncertainty_set=cvxro.Ellipsoidal(rho=2.0))),\n+\n+        (\"multi_unc_n20_ellipsoidal_box\", lambda: make_problem_multi_unc(\n+            n=20, num_constraints=5,\n+            sets=[cvxro.Ellipsoidal(rho=2.0), cvxro.Box(rho=1.5)])),\n+\n+        (\"n20_m5_box\", lambda: make_problem_simple(\n+            n=20, num_constraints=5, uncertainty_set=cvxro.Box(rho=1.5))),\n+\n+        (\"n20_m5_budget\", lambda: make_problem_simple(\n+            n=20, num_constraints=5, uncertainty_set=cvxro.Budget(rho1=1.0, rho2=1.0))),\n+\n+        (\"n20_m5_polyhedral\", lambda: make_problem_simple(\n+            n=20, num_constraints=5,\n+            uncertainty_set=cvxro.Polyhedral(\n+                lhs=np.eye(20), rhs=np.ones(20)))),\n+    ]\n+\n+    all_results = {}\n+    for name, factory in configs:\n+        times = profile_config(name, factory)\n+        all_results[name] = times\n+\n+    # Summary table\n+    print(f\"\\n{'='*70}\")\n+    print(\"SUMMARY\")\n+    print(f\"{'='*70}\")\n+    print(f\"{'Config':<40} {'Mean (s)':>10} {'Std (s)':>10} {'Min (s)':>10}\")\n+    print(\"-\" * 70)\n+    for name, times in all_results.items():\n+        print(f\"{name:<40} {np.mean(times):>10.4f} {np.std(times):>10.4f} {min(times):>10.4f}\")\n+\n+\n+if __name__ == \"__main__\":\n+    main()\ndiff --git a/profiling/profile_hotspots.py b/profiling/profile_hotspots.py\nnew file mode 100644\nindex 0000000..379173c\n--- /dev/null\n+++ b/profiling/profile_hotspots.py\n@@ -0,0 +1,352 @@\n+\"\"\"\n+Targeted micro-benchmarks for suspected bottlenecks in the CVXRO pipeline.\n+\n+Isolates and benchmarks specific operations:\n+1. reshape_tensor() — lil_matrix row-by-row copy loop\n+2. get_problem_data() — CVXPY internal canonicalization\n+3. _gen_constraints() loop — CVXPY expression construction\n+4. generate_torch_expressions() — overhead vs rest of pipeline\n+5. Variable creation in remove_uncertain_terms()\n+6. has_unc_param() repeated calls\n+\"\"\"\n+\n+import statistics\n+import time\n+\n+import cvxpy as cp\n+import numpy as np\n+from cvxpy.problems.objective import Maximize\n+from scipy.sparse import csc_matrix, lil_matrix\n+from scipy.sparse import random as sp_random\n+\n+import cvxro\n+from cvxro.uncertain_canon.remove_uncertain_maximum import RemoveSumOfMaxOfUncertain\n+from cvxro.uncertain_canon.remove_uncertainty import RemoveUncertainty\n+from cvxro.uncertain_canon.uncertain_canonicalization import UncertainCanonicalization\n+from cvxro.uncertain_canon.utils import reshape_tensor\n+from cvxro.utils import gen_and_apply_chain, has_unc_param\n+\n+\n+def make_problem(n, num_constraints=5):\n+    \"\"\"Create a robust optimization problem.\"\"\"\n+    x = cp.Variable(n)\n+    u = cvxro.UncertainParameter(n, uncertainty_set=cvxro.Ellipsoidal(rho=2.0))\n+\n+    objective = cp.Minimize(cp.sum(x))\n+    constraints = [x >= -10, x <= 10]\n+    for i in range(num_constraints):\n+        coeff = np.random.randn(n)\n+        constraints.append(coeff @ x + u @ np.ones(n) <= 5 + i)\n+\n+    return cvxro.RobustProblem(objective, constraints)\n+\n+\n+def benchmark(func, num_runs=10, label=\"\"):\n+    \"\"\"Run a function multiple times and report timing statistics.\"\"\"\n+    times = []\n+    for _ in range(num_runs):\n+        start = time.perf_counter()\n+        func()\n+        elapsed = time.perf_counter() - start\n+        times.append(elapsed)\n+\n+    mean = statistics.mean(times)\n+    std = statistics.stdev(times) if len(times) > 1 else 0\n+    median = statistics.median(times)\n+    print(f\"  {label:<55} mean={mean:.6f}s  std={std:.6f}s  \"\n+          f\"median={median:.6f}s  (n={num_runs})\")\n+    return mean\n+\n+\n+def benchmark_reshape_tensor():\n+    \"\"\"Benchmark 1: reshape_tensor() — optimized vs old approach.\n+\n+    The old lil_matrix row-by-row loop is kept here as a reference baseline\n+    to verify the optimization remains effective and to detect regressions.\n+    \"\"\"\n+    print(\"\\n\" + \"=\" * 70)\n+    print(\"BENCHMARK 1: reshape_tensor()\")\n+    print(\"Compares optimized (current) vs old lil_matrix loop (reference)\")\n+    print(\"=\" * 70)\n+\n+    def reshape_tensor_old(T_Ab, n_var):\n+        \"\"\"Old approach: lil_matrix row-by-row copy (pre-optimization baseline).\"\"\"\n+        T_Ab = csc_matrix(T_Ab)\n+        n_var_full = n_var + 1\n+        num_rows = T_Ab.shape[0]\n+        num_constraints = num_rows // n_var_full\n+        T_Ab_res = lil_matrix(T_Ab.shape)\n+        for target_row in range(num_rows):\n+            constraint_num = 0 if n_var_full == 0 else target_row % n_var_full\n+            var_num = target_row if n_var_full == 0 else target_row // n_var_full\n+            source_row = constraint_num * num_constraints + var_num\n+            T_Ab_res[target_row, :] = T_Ab[source_row, :]\n+        return T_Ab_res\n+\n+    for size_label, n_var, density in [\n+        (\"small (n=5)\", 5, 0.3),\n+        (\"medium (n=50)\", 50, 0.1),\n+        (\"large (n=200)\", 200, 0.05),\n+    ]:\n+        print(f\"\\n  --- {size_label} ---\")\n+        n_var_full = n_var + 1\n+        n_constraints = max(3, n_var // 10)\n+        num_rows = n_var_full * n_constraints\n+        num_cols = n_var * 2  # arbitrary parameter count\n+\n+        T_Ab = sp_random(num_rows, num_cols, density=density, format=\"coo\")\n+\n+        benchmark(\n+            lambda T=T_Ab, nv=n_var: reshape_tensor_old(T, nv),\n+            num_runs=50, label=\"Old (lil_matrix loop)\")\n+\n+        benchmark(\n+            lambda T=T_Ab, nv=n_var: reshape_tensor(T, nv),\n+            num_runs=50, label=\"Current (permutation index)\")\n+\n+\n+def benchmark_get_problem_data():\n+    \"\"\"Benchmark 2: get_problem_data() — CVXPY internal canonicalization.\"\"\"\n+    print(\"\\n\" + \"=\" * 70)\n+    print(\"BENCHMARK 2: get_problem_data() — CVXPY internal cost\")\n+    print(\"Fraction of total canonicalization time spent in CVXPY internals\")\n+    print(\"=\" * 70)\n+\n+    for n, m in [(5, 3), (50, 10), (200, 20)]:\n+        print(f\"\\n  --- n={n}, m={m} ---\")\n+        np.random.seed(42)\n+\n+        # Time get_problem_data alone\n+        def time_get_problem_data():\n+            prob = make_problem(n, m)\n+            # Apply the reductions up to canonicalization\n+            reductions = [RemoveSumOfMaxOfUncertain(), UncertainCanonicalization()]\n+            chain, problem_canon, inv_data = gen_and_apply_chain(prob, reductions)\n+            # Now time get_problem_data on the canonical problem\n+            # We actually need a fresh problem to call get_problem_data\n+            return prob\n+\n+        # Create the epigraph problem that UncertainCanonicalization uses internally\n+        prob = make_problem(n, m)\n+        epigraph_obj_var = cp.Variable()\n+        epi_cons = prob.objective.expr <= epigraph_obj_var\n+        new_constraints = [epi_cons] + prob.constraints\n+        epigraph_problem = cvxro.RobustProblem(\n+            cp.Minimize(epigraph_obj_var), new_constraints\n+        )\n+\n+        # Time just get_problem_data\n+        benchmark(\n+            lambda: epigraph_problem.get_problem_data(solver=\"CLARABEL\"),\n+            num_runs=20, label=\"get_problem_data() alone\")\n+\n+        # Time full canonicalization (for comparison)\n+        def full_canon():\n+            p = make_problem(n, m)\n+            p._solver = \"CLARABEL\"\n+            reductions = [RemoveSumOfMaxOfUncertain(), UncertainCanonicalization()]\n+            gen_and_apply_chain(p, reductions)\n+\n+        benchmark(\n+            full_canon, num_runs=20,\n+            label=\"Full canonicalization (includes get_problem_data)\")\n+\n+\n+def benchmark_generate_torch_expressions():\n+    \"\"\"Benchmark 4: generate_torch_expressions() cost vs rest of pipeline.\"\"\"\n+    print(\"\\n\" + \"=\" * 70)\n+    print(\"BENCHMARK 4: generate_torch_expressions() overhead\")\n+    print(\"Compares remove_uncertainty with and without torch expression generation\")\n+    print(\"=\" * 70)\n+\n+    for n, m in [(5, 3), (20, 5), (50, 10), (200, 20)]:\n+        print(f\"\\n  --- n={n}, m={m} ---\")\n+        np.random.seed(42)\n+\n+        from cvxro.torch_expression_generator import generate_torch_expressions\n+        from cvxro.uncertain_canon.flip_objective import FlipObjective\n+\n+        def full_remove_uncertainty():\n+            prob = make_problem(n, m)\n+            prob.remove_uncertainty(solver=\"CLARABEL\")\n+\n+        def without_torch_expressions():\n+            \"\"\"remove_uncertainty minus generate_torch_expressions.\"\"\"\n+            prob = make_problem(n, m)\n+            prob._solver = \"CLARABEL\"\n+            reductions_canon = []\n+            if isinstance(prob.objective, Maximize):\n+                reductions_canon += [FlipObjective()]\n+            reductions_canon += [RemoveSumOfMaxOfUncertain(), UncertainCanonicalization()]\n+            chain_canon, problem_canon, inv_data = gen_and_apply_chain(prob, reductions_canon)\n+            # Skip generate_torch_expressions!\n+            chain_no_unc, problem_no_unc, inv_data_no_unc = gen_and_apply_chain(\n+                problem_canon, reductions=[RemoveUncertainty()]\n+            )\n+\n+        def only_torch_expressions():\n+            \"\"\"Just generate_torch_expressions on an already-canonicalized problem.\"\"\"\n+            prob = make_problem(n, m)\n+            prob._solver = \"CLARABEL\"\n+            reductions_canon = [RemoveSumOfMaxOfUncertain(), UncertainCanonicalization()]\n+            _, problem_canon, _ = gen_and_apply_chain(prob, reductions_canon)\n+            generate_torch_expressions(problem_canon)\n+\n+        t_full = benchmark(full_remove_uncertainty, num_runs=10,\n+                           label=\"Full remove_uncertainty()\")\n+        t_without = benchmark(without_torch_expressions, num_runs=10,\n+                              label=\"Without generate_torch_expressions()\")\n+        t_torch_only = benchmark(only_torch_expressions, num_runs=10,\n+                                 label=\"Only generate_torch_expressions()\")\n+\n+        if t_full > 0:\n+            pct = (t_torch_only / t_full) * 100\n+            print(f\"  --> generate_torch_expressions is ~{pct:.1f}% of total \"\n+                  f\"remove_uncertainty time\")\n+            print(f\"  --> Potential speedup from deferral: {t_full / t_without:.2f}x\")\n+\n+\n+def benchmark_variable_creation():\n+    \"\"\"Benchmark 5: Variable creation in remove_uncertain_terms().\"\"\"\n+    print(\"\\n\" + \"=\" * 70)\n+    print(\"BENCHMARK 5: Variable(shape) creation cost\")\n+    print(\"Measures the cost of creating CVXPY Variable objects\")\n+    print(\"=\" * 70)\n+\n+    for shape in [1, 5, 20, 50, 200]:\n+        def create_vars(shape=shape, k_num=3, u_num=2):\n+            for _ in range(u_num):\n+                for _ in range(k_num):\n+                    cp.Variable(shape)\n+                    cp.Variable(shape)  # supp_cons\n+\n+        benchmark(create_vars, num_runs=100,\n+                  label=f\"shape={shape}, k_num=3, u_num=2 ({12} vars)\")\n+\n+\n+def benchmark_has_unc_param():\n+    \"\"\"Benchmark 6: has_unc_param() repeated calls.\"\"\"\n+    print(\"\\n\" + \"=\" * 70)\n+    print(\"BENCHMARK 6: has_unc_param() — parameter iteration cost\")\n+    print(\"Measures cost of calling has_unc_param on the same expressions\")\n+    print(\"=\" * 70)\n+\n+    for n, m in [(5, 3), (20, 10), (50, 20)]:\n+        np.random.seed(42)\n+        prob = make_problem(n, m)\n+\n+        constraints = prob.constraints\n+\n+        def call_has_unc_param(constraints=constraints):\n+            for c in constraints:\n+                has_unc_param(c)\n+\n+        benchmark(call_has_unc_param, num_runs=100,\n+                  label=f\"n={n}, m={m}: {len(constraints)} constraints\")\n+\n+        # Also measure parameters() call separately\n+        expr = constraints[-1]  # An uncertain constraint\n+\n+        def call_parameters(expr=expr):\n+            expr.parameters()\n+\n+        benchmark(call_parameters, num_runs=100,\n+                  label=\"  -> expr.parameters() on one constraint\")\n+\n+\n+def benchmark_full_solve_breakdown():\n+    \"\"\"Comprehensive breakdown: what fraction of solve() is each stage.\"\"\"\n+    print(\"\\n\" + \"=\" * 70)\n+    print(\"FULL SOLVE BREAKDOWN\")\n+    print(\"Time per stage as a fraction of total solve()\")\n+    print(\"=\" * 70)\n+\n+    for n, m in [(5, 3), (50, 10), (200, 20)]:\n+        print(f\"\\n  --- n={n}, m={m} ---\")\n+        np.random.seed(42)\n+\n+        from cvxro.torch_expression_generator import generate_torch_expressions\n+\n+        # Total solve time\n+        def full_solve():\n+            prob = make_problem(n, m)\n+            prob.solve(solver=\"CLARABEL\")\n+\n+        t_total = benchmark(full_solve, num_runs=5, label=\"Total solve()\")\n+\n+        # Stage 1: RemoveSumOfMaxOfUncertain + UncertainCanonicalization\n+        def stage_canon():\n+            prob = make_problem(n, m)\n+            prob._solver = \"CLARABEL\"\n+            reductions = [RemoveSumOfMaxOfUncertain(), UncertainCanonicalization()]\n+            gen_and_apply_chain(prob, reductions)\n+\n+        t_canon = benchmark(stage_canon, num_runs=5, label=\"Stage 1: Canonicalization\")\n+\n+        # Stage 2: generate_torch_expressions\n+        prob = make_problem(n, m)\n+        prob._solver = \"CLARABEL\"\n+        reductions = [RemoveSumOfMaxOfUncertain(), UncertainCanonicalization()]\n+        _, problem_canon, _ = gen_and_apply_chain(prob, reductions)\n+\n+        def stage_torch(pc=problem_canon):\n+            generate_torch_expressions(pc)\n+\n+        # Note: can only run once since it modifies problem_canon in place\n+        start = time.perf_counter()\n+        generate_torch_expressions(problem_canon)\n+        t_torch = time.perf_counter() - start\n+        print(f\"  {'Stage 2: generate_torch_expressions':<55} \"\n+              f\"time={t_torch:.6f}s  (single run, modifies in-place)\")\n+\n+        # Stage 3: RemoveUncertainty\n+        def stage_remove():\n+            prob = make_problem(n, m)\n+            prob._solver = \"CLARABEL\"\n+            reductions = [RemoveSumOfMaxOfUncertain(), UncertainCanonicalization()]\n+            _, pc, _ = gen_and_apply_chain(prob, reductions)\n+            generate_torch_expressions(pc)\n+            gen_and_apply_chain(pc, reductions=[RemoveUncertainty()])\n+\n+        # We want just the RemoveUncertainty step, but it needs the preceding stages.\n+        # So we measure the total and subtract.\n+        t_remove_total = benchmark(stage_remove, num_runs=5,\n+                                   label=\"Stages 1+2+3 (through RemoveUncertainty)\")\n+\n+        # Stage 4: Solve the deterministic problem\n+        prob = make_problem(n, m)\n+        prob.remove_uncertainty(solver=\"CLARABEL\")\n+\n+        def stage_solve():\n+            p = prob.problem_no_unc\n+            for x in p.parameters():\n+                if x.value is None:\n+                    x.value = np.zeros(x.shape)\n+            p.solve(solver=\"CLARABEL\")\n+\n+        t_solve = benchmark(stage_solve, num_runs=5, label=\"Stage 4: CVXPY solve (deterministic)\")\n+\n+        if t_total > 0:\n+            # Percentages are approximate: each stage is timed independently with\n+            # fresh problem construction, so run-to-run variance means they may not\n+            # sum to exactly 100%. The max(0, ...) on RemoveUncertainty handles\n+            # cases where timing noise makes the subtraction negative.\n+            print(\"\\n  Approximate breakdown (% of total):\")\n+            print(f\"    Canonicalization:              ~{t_canon/t_total*100:.1f}%\")\n+            print(f\"    generate_torch_expressions:    ~{t_torch/t_total*100:.1f}%\")\n+            print(f\"    RemoveUncertainty (estimated): ~\"\n+                  f\"{max(0, (t_remove_total-t_canon-t_torch)/t_total*100):.1f}%\")\n+            print(f\"    CVXPY solve:                   ~{t_solve/t_total*100:.1f}%\")\n+\n+\n+def main():\n+    benchmark_reshape_tensor()\n+    benchmark_get_problem_data()\n+    benchmark_generate_torch_expressions()\n+    benchmark_variable_creation()\n+    benchmark_has_unc_param()\n+    benchmark_full_solve_breakdown()\n+\n+\n+if __name__ == \"__main__\":\n+    main()\ndiff --git a/profiling/profile_memory.py b/profiling/profile_memory.py\nnew file mode 100644\nindex 0000000..4b985be\n--- /dev/null\n+++ b/profiling/profile_memory.py\n@@ -0,0 +1,167 @@\n+\"\"\"\n+Memory profiling of the CVXRO canonicalization pipeline using tracemalloc.\n+\n+Measures peak memory and per-stage allocations across problem sizes.\n+\"\"\"\n+\n+import tracemalloc\n+\n+import cvxpy as cp\n+import numpy as np\n+from cvxpy.problems.objective import Maximize\n+\n+import cvxro\n+from cvxro.uncertain_canon.remove_uncertain_maximum import RemoveSumOfMaxOfUncertain\n+from cvxro.uncertain_canon.remove_uncertainty import RemoveUncertainty\n+from cvxro.uncertain_canon.uncertain_canonicalization import UncertainCanonicalization\n+from cvxro.utils import gen_and_apply_chain\n+\n+\n+def make_problem(n, num_constraints=5):\n+    \"\"\"Create a robust optimization problem of given size.\"\"\"\n+    x = cp.Variable(n)\n+    u = cvxro.UncertainParameter(n, uncertainty_set=cvxro.Ellipsoidal(rho=2.0))\n+\n+    objective = cp.Minimize(cp.sum(x))\n+    constraints = [x >= -10, x <= 10]\n+    for i in range(num_constraints):\n+        coeff = np.random.randn(n)\n+        constraints.append(coeff @ x + u @ np.ones(n) <= 5 + i)\n+\n+    return cvxro.RobustProblem(objective, constraints)\n+\n+\n+def format_bytes(size):\n+    \"\"\"Format bytes into human-readable string.\"\"\"\n+    for unit in [\"B\", \"KB\", \"MB\", \"GB\"]:\n+        if abs(size) < 1024:\n+            return f\"{size:.1f} {unit}\"\n+        size /= 1024\n+    return f\"{size:.1f} TB\"\n+\n+\n+def top_allocators(snapshot, limit=10):\n+    \"\"\"Print top memory allocators from a tracemalloc snapshot.\"\"\"\n+    stats = snapshot.statistics(\"lineno\")\n+    print(f\"  Top {limit} allocators:\")\n+    for stat in stats[:limit]:\n+        print(f\"    {stat}\")\n+\n+\n+def profile_stages(n, num_constraints=5):\n+    \"\"\"Profile memory usage per stage of the canonicalization pipeline.\"\"\"\n+    print(f\"\\n{'='*70}\")\n+    print(f\"Memory profile: n={n}, constraints={num_constraints}\")\n+    print(f\"{'='*70}\")\n+\n+    np.random.seed(42)\n+    prob = make_problem(n, num_constraints)\n+    prob._solver = \"CLARABEL\"\n+\n+    tracemalloc.start()\n+\n+    # Stage 0: baseline\n+    snap_baseline = tracemalloc.take_snapshot()\n+    mem_baseline = tracemalloc.get_traced_memory()\n+\n+    # Stage 1a: RemoveSumOfMaxOfUncertain + UncertainCanonicalization\n+    from cvxro.uncertain_canon.flip_objective import FlipObjective\n+\n+    reductions_canon = []\n+    if isinstance(prob.objective, Maximize):\n+        reductions_canon += [FlipObjective()]\n+    reductions_canon += [RemoveSumOfMaxOfUncertain(), UncertainCanonicalization()]\n+    chain_canon, problem_canon, inverse_data_canon = gen_and_apply_chain(\n+        problem=prob, reductions=reductions_canon\n+    )\n+\n+    snap_after_canon = tracemalloc.take_snapshot()\n+    mem_after_canon = tracemalloc.get_traced_memory()\n+\n+    # Stage 2: generate_torch_expressions\n+    from cvxro.torch_expression_generator import generate_torch_expressions\n+\n+    generate_torch_expressions(problem_canon)\n+\n+    snap_after_torch = tracemalloc.take_snapshot()\n+    mem_after_torch = tracemalloc.get_traced_memory()\n+\n+    # Stage 3: RemoveUncertainty\n+    chain_no_unc, problem_no_unc, inverse_data_no_unc = gen_and_apply_chain(\n+        problem_canon, reductions=[RemoveUncertainty()]\n+    )\n+\n+    snap_after_remove = tracemalloc.take_snapshot()\n+    mem_after_remove = tracemalloc.get_traced_memory()\n+\n+    # Stage 4: Solve\n+    for x in problem_no_unc.parameters():\n+        if x.value is None:\n+            x.value = x.data[0] if hasattr(x, \"data\") and x.data is not None else np.zeros(x.shape)\n+    problem_no_unc.solve(solver=\"CLARABEL\")\n+\n+    snap_after_solve = tracemalloc.take_snapshot()\n+    mem_after_solve = tracemalloc.get_traced_memory()\n+\n+    peak_mem = tracemalloc.get_traced_memory()[1]\n+    tracemalloc.stop()\n+\n+    # Report\n+    stages = [\n+        (\"Baseline\", mem_baseline),\n+        (\"After canonicalization (1a+1b)\", mem_after_canon),\n+        (\"After generate_torch_expressions\", mem_after_torch),\n+        (\"After RemoveUncertainty\", mem_after_remove),\n+        (\"After solve\", mem_after_solve),\n+    ]\n+\n+    print(f\"\\n  {'Stage':<45} {'Current':>12} {'Peak':>12}\")\n+    print(f\"  {'-'*69}\")\n+    for name, (current, peak) in stages:\n+        print(f\"  {name:<45} {format_bytes(current):>12} {format_bytes(peak):>12}\")\n+\n+    print(f\"\\n  Overall peak memory: {format_bytes(peak_mem)}\")\n+\n+    # Deltas\n+    print(\"\\n  Memory deltas between stages:\")\n+    snapshots = [\n+        (\"Canonicalization\", snap_baseline, snap_after_canon),\n+        (\"generate_torch_expressions\", snap_after_canon, snap_after_torch),\n+        (\"RemoveUncertainty\", snap_after_torch, snap_after_remove),\n+        (\"Solve\", snap_after_remove, snap_after_solve),\n+    ]\n+    for name, before, after in snapshots:\n+        delta_stats = after.compare_to(before, \"lineno\")\n+        total_delta = sum(s.size_diff for s in delta_stats if s.size_diff > 0)\n+        print(f\"\\n  --- {name}: +{format_bytes(total_delta)} ---\")\n+        for stat in delta_stats[:5]:\n+            if stat.size_diff > 0:\n+                print(f\"    {stat}\")\n+\n+    return peak_mem\n+\n+\n+def main():\n+    sizes = [5, 50, 200]\n+\n+    print(\"CVXRO Memory Profiling\")\n+    print(\"=\" * 70)\n+\n+    results = {}\n+    for n in sizes:\n+        peak = profile_stages(n, num_constraints=max(3, n // 10))\n+        results[n] = peak\n+\n+    # Summary\n+    print(f\"\\n{'='*70}\")\n+    print(\"SUMMARY: Peak memory by problem size\")\n+    print(f\"{'='*70}\")\n+    print(f\"{'n':<10} {'Constraints':<15} {'Peak Memory':>15}\")\n+    print(\"-\" * 40)\n+    for n in sizes:\n+        m = max(3, n // 10)\n+        print(f\"{n:<10} {m:<15} {format_bytes(results[n]):>15}\")\n+\n+\n+if __name__ == \"__main__\":\n+    main()\ndiff --git a/profiling/profile_scaling.py b/profiling/profile_scaling.py\nnew file mode 100644\nindex 0000000..abe8fab\n--- /dev/null\n+++ b/profiling/profile_scaling.py\n@@ -0,0 +1,215 @@\n+\"\"\"\n+Scaling analysis of the CVXRO canonicalization pipeline.\n+\n+Measures how solve() time scales with:\n+- Problem dimension n\n+- Number of constraints m\n+- Number of uncertain parameters\n+\n+Outputs tables and optionally generates matplotlib plots.\n+\"\"\"\n+\n+import time\n+\n+import cvxpy as cp\n+import numpy as np\n+\n+import cvxro\n+\n+\n+def make_problem(n, num_constraints, num_uncertain=1, set_type=\"ellipsoidal\"):\n+    \"\"\"Create a robust optimization problem.\"\"\"\n+    x = cp.Variable(n)\n+\n+    if set_type == \"ellipsoidal\":\n+        sets = [cvxro.Ellipsoidal(rho=2.0) for _ in range(num_uncertain)]\n+    elif set_type == \"box\":\n+        sets = [cvxro.Box(rho=1.5) for _ in range(num_uncertain)]\n+    else:\n+        sets = [cvxro.Ellipsoidal(rho=2.0) for _ in range(num_uncertain)]\n+\n+    u_params = [cvxro.UncertainParameter(n, uncertainty_set=s) for s in sets]\n+\n+    objective = cp.Minimize(cp.sum(x))\n+    constraints = [x >= -10, x <= 10]\n+    for i in range(num_constraints):\n+        coeff = np.random.randn(n)\n+        expr = coeff @ x\n+        for u in u_params:\n+            expr = expr + u @ np.ones(n)\n+        constraints.append(expr <= 5 + i)\n+\n+    return cvxro.RobustProblem(objective, constraints)\n+\n+\n+def time_solve(problem_factory, num_runs=3, warmup=True):\n+    \"\"\"Time the solve() method over multiple runs.\"\"\"\n+    if warmup:\n+        prob = problem_factory()\n+        prob.solve(solver=\"CLARABEL\")\n+        del prob\n+\n+    times = []\n+    for _ in range(num_runs):\n+        prob = problem_factory()\n+        start = time.perf_counter()\n+        prob.solve(solver=\"CLARABEL\")\n+        elapsed = time.perf_counter() - start\n+        times.append(elapsed)\n+        del prob\n+\n+    return np.mean(times), np.std(times), np.min(times)\n+\n+\n+def scaling_dimension():\n+    \"\"\"Measure scaling with problem dimension n.\"\"\"\n+    print(\"\\n\" + \"=\" * 70)\n+    print(\"SCALING WITH DIMENSION (n)\")\n+    print(\"Fixed: 5 constraints, 1 uncertain param (Ellipsoidal)\")\n+    print(\"=\" * 70)\n+\n+    dims = [5, 10, 20, 50, 100, 200]\n+    results = []\n+\n+    print(f\"\\n{'n':>6} {'Mean (s)':>10} {'Std (s)':>10} {'Min (s)':>10} {'Ratio':>8}\")\n+    print(\"-\" * 50)\n+\n+    prev_mean = None\n+    for n in dims:\n+        np.random.seed(42)\n+\n+        def factory(n=n):\n+            return make_problem(n, num_constraints=5, num_uncertain=1)\n+\n+        mean, std, mn = time_solve(factory)\n+        ratio = mean / prev_mean if prev_mean else 1.0\n+        print(f\"{n:>6} {mean:>10.4f} {std:>10.4f} {mn:>10.4f} {ratio:>8.2f}x\")\n+        results.append((n, mean, std, mn))\n+        prev_mean = mean\n+\n+    return dims, results\n+\n+\n+def scaling_constraints():\n+    \"\"\"Measure scaling with number of constraints m.\"\"\"\n+    print(\"\\n\" + \"=\" * 70)\n+    print(\"SCALING WITH CONSTRAINTS (m)\")\n+    print(\"Fixed: n=20, 1 uncertain param (Ellipsoidal)\")\n+    print(\"=\" * 70)\n+\n+    constraints = [2, 5, 10, 20, 50]\n+    results = []\n+\n+    print(f\"\\n{'m':>6} {'Mean (s)':>10} {'Std (s)':>10} {'Min (s)':>10} {'Ratio':>8}\")\n+    print(\"-\" * 50)\n+\n+    prev_mean = None\n+    for m in constraints:\n+        np.random.seed(42)\n+\n+        def factory(m=m):\n+            return make_problem(20, num_constraints=m, num_uncertain=1)\n+\n+        mean, std, mn = time_solve(factory)\n+        ratio = mean / prev_mean if prev_mean else 1.0\n+        print(f\"{m:>6} {mean:>10.4f} {std:>10.4f} {mn:>10.4f} {ratio:>8.2f}x\")\n+        results.append((m, mean, std, mn))\n+        prev_mean = mean\n+\n+    return constraints, results\n+\n+\n+def scaling_uncertain_params():\n+    \"\"\"Measure scaling with number of uncertain parameters.\"\"\"\n+    print(\"\\n\" + \"=\" * 70)\n+    print(\"SCALING WITH UNCERTAIN PARAMETERS\")\n+    print(\"Fixed: n=20, 5 constraints (Ellipsoidal)\")\n+    print(\"=\" * 70)\n+\n+    num_params = [1, 2, 3]\n+    results = []\n+\n+    print(f\"\\n{'#params':>8} {'Mean (s)':>10} {'Std (s)':>10} {'Min (s)':>10} {'Ratio':>8}\")\n+    print(\"-\" * 50)\n+\n+    prev_mean = None\n+    for p in num_params:\n+        np.random.seed(42)\n+\n+        def factory(p=p):\n+            return make_problem(20, num_constraints=5, num_uncertain=p)\n+\n+        mean, std, mn = time_solve(factory)\n+        ratio = mean / prev_mean if prev_mean else 1.0\n+        print(f\"{p:>8} {mean:>10.4f} {std:>10.4f} {mn:>10.4f} {ratio:>8.2f}x\")\n+        results.append((p, mean, std, mn))\n+        prev_mean = mean\n+\n+    return num_params, results\n+\n+\n+def try_plot(dim_data, cons_data, param_data):\n+    \"\"\"Attempt to generate matplotlib plots. Skips silently if unavailable.\"\"\"\n+    try:\n+        import matplotlib\n+        matplotlib.use(\"Agg\")\n+        import matplotlib.pyplot as plt\n+    except ImportError:\n+        print(\"\\nmatplotlib not available, skipping plots.\")\n+        return\n+\n+    import os\n+    output_dir = os.path.join(os.path.dirname(__file__), \"results\")\n+    os.makedirs(output_dir, exist_ok=True)\n+\n+    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n+\n+    # Dimension scaling\n+    dims, dim_results = dim_data\n+    means = [r[1] for r in dim_results]\n+    stds = [r[2] for r in dim_results]\n+    axes[0].errorbar(dims, means, yerr=stds, marker=\"o\", capsize=3)\n+    axes[0].set_xlabel(\"Problem dimension (n)\")\n+    axes[0].set_ylabel(\"Time (s)\")\n+    axes[0].set_title(\"Scaling with dimension\")\n+    axes[0].set_xscale(\"log\")\n+    axes[0].set_yscale(\"log\")\n+    axes[0].grid(True, alpha=0.3)\n+\n+    # Constraint scaling\n+    constraints, cons_results = cons_data\n+    means = [r[1] for r in cons_results]\n+    stds = [r[2] for r in cons_results]\n+    axes[1].errorbar(constraints, means, yerr=stds, marker=\"s\", capsize=3)\n+    axes[1].set_xlabel(\"Number of constraints (m)\")\n+    axes[1].set_ylabel(\"Time (s)\")\n+    axes[1].set_title(\"Scaling with constraints\")\n+    axes[1].set_xscale(\"log\")\n+    axes[1].set_yscale(\"log\")\n+    axes[1].grid(True, alpha=0.3)\n+\n+    # Uncertain parameter scaling\n+    params, param_results = param_data\n+    means = [r[1] for r in param_results]\n+    stds = [r[2] for r in param_results]\n+    axes[2].errorbar(params, means, yerr=stds, marker=\"^\", capsize=3)\n+    axes[2].set_xlabel(\"Number of uncertain params\")\n+    axes[2].set_ylabel(\"Time (s)\")\n+    axes[2].set_title(\"Scaling with uncertain params\")\n+    axes[2].grid(True, alpha=0.3)\n+\n+    plt.tight_layout()\n+    plot_path = os.path.join(output_dir, \"scaling_curves.png\")\n+    plt.savefig(plot_path, dpi=150)\n+    print(f\"\\nPlots saved to {plot_path}\")\n+\n+\n+def main():\n+    dim_data = scaling_dimension()\n+    cons_data = scaling_constraints()\n+    param_data = scaling_uncertain_params()\n+    try_plot(dim_data, cons_data, param_data)\n+\n+\n+if __name__ == \"__main__\":\n+    main()\ndiff --git a/profiling/run_profiling.sh b/profiling/run_profiling.sh\nnew file mode 100755\nindex 0000000..f3b3424\n--- /dev/null\n+++ b/profiling/run_profiling.sh\n@@ -0,0 +1,76 @@\n+#!/bin/bash\n+# Run all CVXRO profiling scripts and consolidate output.\n+#\n+# Usage:\n+#   bash profiling/run_profiling.sh           # Run all\n+#   bash profiling/run_profiling.sh hotspots  # Run specific script\n+\n+set -e\n+\n+SCRIPT_DIR=\"$(cd \"$(dirname \"$0\")\" && pwd)\"\n+PROJECT_DIR=\"$(dirname \"$SCRIPT_DIR\")\"\n+RESULTS_DIR=\"$SCRIPT_DIR/results\"\n+LOG_FILE=\"$RESULTS_DIR/profiling_$(date +%Y%m%d_%H%M%S).log\"\n+\n+mkdir -p \"$RESULTS_DIR\"\n+\n+cd \"$PROJECT_DIR\"\n+\n+run_script() {\n+    local name=\"$1\"\n+    local script=\"$2\"\n+    echo \"\"\n+    echo \"==================================================================\"\n+    echo \"  Running: $name\"\n+    echo \"  Script:  $script\"\n+    echo \"  Time:    $(date)\"\n+    echo \"==================================================================\"\n+    echo \"\"\n+    uv run python \"$script\"\n+}\n+\n+# If a specific script is requested\n+if [ -n \"$1\" ]; then\n+    case \"$1\" in\n+        canon*)\n+            run_script \"Canonicalization Profile\" \"$SCRIPT_DIR/profile_canonicalization.py\" 2>&1 | tee -a \"$LOG_FILE\"\n+            ;;\n+        memory|mem*)\n+            run_script \"Memory Profile\" \"$SCRIPT_DIR/profile_memory.py\" 2>&1 | tee -a \"$LOG_FILE\"\n+            ;;\n+        scal*)\n+            run_script \"Scaling Analysis\" \"$SCRIPT_DIR/profile_scaling.py\" 2>&1 | tee -a \"$LOG_FILE\"\n+            ;;\n+        hot*)\n+            run_script \"Hotspot Micro-benchmarks\" \"$SCRIPT_DIR/profile_hotspots.py\" 2>&1 | tee -a \"$LOG_FILE\"\n+            ;;\n+        *)\n+            echo \"Unknown script: $1\"\n+            echo \"Options: canonicalization, memory, scaling, hotspots\"\n+            exit 1\n+            ;;\n+    esac\n+    echo \"\"\n+    echo \"Log saved to: $LOG_FILE\"\n+    exit 0\n+fi\n+\n+# Run all scripts: hotspots first (quickest, most targeted), then scaling,\n+# memory, and finally cProfile (most detailed, slowest).\n+echo \"CVXRO Profiling Suite\"\n+echo \"Log: $LOG_FILE\"\n+echo \"\"\n+\n+{\n+    run_script \"Hotspot Micro-benchmarks\" \"$SCRIPT_DIR/profile_hotspots.py\"\n+    run_script \"Scaling Analysis\" \"$SCRIPT_DIR/profile_scaling.py\"\n+    run_script \"Memory Profile\" \"$SCRIPT_DIR/profile_memory.py\"\n+    run_script \"Canonicalization Profile\" \"$SCRIPT_DIR/profile_canonicalization.py\"\n+} 2>&1 | tee \"$LOG_FILE\"\n+\n+echo \"\"\n+echo \"==================================================================\"\n+echo \"  All profiling complete\"\n+echo \"  Log: $LOG_FILE\"\n+echo \"  Results: $RESULTS_DIR/\"\n+echo \"==================================================================\"\ndiff --git a/tests/core/test_ellipsoidal_uncertainty.py b/tests/core/test_ellipsoidal_uncertainty.py\nindex 747eba6..a8440c8 100755\n--- a/tests/core/test_ellipsoidal_uncertainty.py\n+++ b/tests/core/test_ellipsoidal_uncertainty.py\n@@ -14,10 +14,9 @@\n from cvxpy.expressions.expression import Expression\n from cvxpy.reductions.reduction import Reduction\n from numpy import ndarray\n-from scipy.sparse import csc_matrix, csr_matrix\n-from scipy.sparse._coo import coo_matrix\n \n from cvxro.robust_problem import RobustProblem\n+from cvxro.uncertain_canon.utils import reshape_tensor as tensor_reshaper\n from cvxro.uncertain_parameter import UncertainParameter\n from cvxro.uncertainty_sets.ellipsoidal import Ellipsoidal\n \n@@ -177,34 +176,6 @@ def _finalize_expressions_uncertain(T_Ab, n_var):\n     return A_rec_certain, A_rec_uncertain, b_rec\n \n \n-def tensor_reshaper(T_Ab: coo_matrix, n_var: int) -> np.ndarray:\n-    \"\"\"\n-    This function reshapes T_Ab so T_Ab@param_vec gives the constraints row by row instead of\n-    column by column. At the moment, it returns a dense matrix instead of a sparse one.\n-    \"\"\"\n-\n-    def _calc_source_row(target_row: int, num_constraints: int) -> int:\n-        \"\"\"\n-        This is a helper function that calculates the index of the source row of T_Ab for the\n-        reshaped target row.\n-        \"\"\"\n-        constraint_num = target_row % (num_constraints - 1)\n-        var_num = target_row // (num_constraints - 1)\n-        source_row = constraint_num * num_constraints + var_num\n-        return source_row\n-\n-    T_Ab = csc_matrix(T_Ab)\n-    n_var_full = n_var + 1  # Includes the free paramter\n-    num_rows = T_Ab.shape[0]\n-    num_constraints = num_rows // n_var_full\n-    T_Ab_res = csr_matrix(T_Ab.shape)\n-    target_row = 0  # Counter for populating the new row of T_Ab_res\n-    for target_row in range(num_rows):\n-        source_row = _calc_source_row(target_row, num_constraints)\n-        T_Ab_res[target_row, :] = T_Ab[source_row, :]\n-    return T_Ab_res\n-\n-\n def calc_num_constraints(constraints: list[Constraint]) -> int:\n     \"\"\"\n     This function calculates the number of constraints from a list of constraints.\n"
test_patch: ''
fail_to_pass:
- python -m unittest tests.test_reshape_tensor
pass_to_pass:
- python -m compileall cvxro
install_config:
  install: pip install -e .
  python: '3.11'
  test_cmd: pytest
meta:
  added_lines: '1063'
  difficulty: hard
  files_changed: '9'
  pr_title: Optimize reshape_tensor (35-280x speedup) and add profiling suite
  removed_lines: '52'
  source: gh-archive-pr
  test_files: '[{"path":"tests/test_reshape_tensor.py","content":"import importlib.util\nimport unittest\nfrom pathlib import Path\n\nimport numpy as np\nfrom scipy.sparse import coo_matrix, csc_matrix\n\n\ndef load_utils_module():\n    utils_path = Path(__file__).resolve().parents[1] / \"cvxro\" / \"uncertain_canon\" / \"utils.py\"\n    spec = importlib.util.spec_from_file_location(\"cvxro_uncertain_utils\", utils_path)\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n    return module\n\n\nclass TestReshapeTensor(unittest.TestCase):\n    def setUp(self):\n        self.utils = load_utils_module()\n\n    def test_reshape_tensor_permutation_and_type(self):\n        n_var = 2\n        num_constraints = 3\n        n_var_full = n_var + 1\n        num_rows = num_constraints * n_var_full\n        # Build a dense matrix with distinct row patterns\n        dense = np.arange(num_rows * 4, dtype=float).reshape(num_rows, 4)\n        T_Ab = coo_matrix(dense)\n\n        reshaped = self.utils.reshape_tensor(T_Ab, n_var)\n\n        target_rows = np.arange(num_rows)\n        constraint_nums = target_rows % n_var_full\n        var_nums = target_rows // n_var_full\n        perm = constraint_nums * num_constraints + var_nums\n        expected = csc_matrix(dense)[perm, :]\n\n        self.assertIsInstance(reshaped, csc_matrix)\n        self.assertEqual(reshaped.shape, expected.shape)\n        self.assertTrue(np.allclose(reshaped.toarray(), expected.toarray()))\n\n    def test_reshape_tensor_zero_variables_identity(self):\n        n_var = 0\n        num_constraints = 4\n        num_rows = num_constraints * (n_var + 1)\n        dense = np.array([\n            [1.0, 0.0, -2.0],\n            [0.0, 3.5, 4.0],\n            [5.0, -6.0, 0.0],\n            [7.0, 8.0, 9.0],\n        ])\n        self.assertEqual(dense.shape[0], num_rows)\n        T_Ab = coo_matrix(dense)\n\n        reshaped = self.utils.reshape_tensor(T_Ab, n_var)\n\n        self.assertIsInstance(reshaped, csc_matrix)\n        self.assertEqual(reshaped.nnz, csc_matrix(dense).nnz)\n        self.assertTrue(np.allclose(reshaped.toarray(), dense))\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"}]'
  test_generation: agentic-docker
prompt: |-
  stellatogrp/cvxro (#56): Optimize reshape_tensor (35-280x speedup) and add profiling suite

  Improve performance of tensor reshaping to remove a major bottleneck in the canonicalization pipeline. Ensure reshaping scales efficiently at larger problem sizes and no longer dominates solve time. Add a profiling suite with scripts to benchmark runtime, memory usage, scaling behavior, and hotspots across multiple configurations, plus a runner to execute the profiling workflows.
original_pr_body: "stellatogrp/cvxro (#56): Optimize reshape_tensor (35-280x speedup) and add profiling suite\n\n## Summary (to be merged after #55 after setting target branch to `develop`)\r\n\r\n- **Fix `reshape_tensor()` bottleneck**: Replace the Python-level row-by-row `lil_matrix` copy loop with vectorized NumPy permutation-index + sparse advanced indexing. This was the dominant bottleneck in the canonicalization pipeline, accounting for 60-76% of total `solve()` time at larger problem sizes.\r\n\r\n  | Size | Before | After | Speedup |\r\n  |------|--------|-------|---------|\r\n  | n=5 | 1.08ms | 0.031ms | **35x** |\r\n  | n=50 | 15.3ms | 0.054ms | **283x** |\r\n  | n=200 | 381ms | 1.9ms | **200x** |\r\n\r\n- **Add `profiling/` suite** with reusable scripts for ongoing performance analysis:\r\n  - `profile_canonicalization.py` — cProfile across 7 problem configurations\r\n  - `profile_memory.py` — tracemalloc per-stage memory analysis\r\n  - `profile_scaling.py` — scaling curves with dimension/constraints/params\r\n  - `profile_hotspots.py` — targeted micro-benchmarks of suspected bottlenecks\r\n  - `run_profiling.sh` — runner script\r\n\r\n## Test plan\r\n\r\n- [x] All 91 core + integration tests pass\r\n- [x] Run `bash profiling/run_profiling.sh` to verify profiling scripts"
quality_score: 0.78
quality_passed: true
docker_passed: false
workspace_path: null
status: ready
